{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lores\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\lores\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\lores\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\lores\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\lores\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Import per il resto delle celle\n",
    "import re\n",
    "import emoji\n",
    "import regex\n",
    "import nltk\n",
    "import json\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from collections import Counter\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "import pymongo\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import psycopg2\n",
    "%matplotlib inline\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "emoticons = [':‑)',':)',':-]',':]',':->',':>','8-)','8)',':-}',':}',':o)',':c)',':^)','=]','=)',':‑D',':D','8‑D','8D','=D','=3','B^D','c:',\n",
    "'C:','x‑D','xD','X‑D','XD',':-))',':‑(',':(',':‑c',':c',':‑<',':<',':‑[',':[',':-||','>:[',':{',':@',':('',;(',\":'‑(\",\":'(\",':=(',\":'‑)\",\":')\",\n",
    "':\"D',\"D‑':\",'D:<','D:','D8','D;','D=','DX',':‑O',':O',':‑o',':o',':-0','8‑0','>:O','=O','=o','=0',':-3',':3','=3','x3','X3','>:3',':-*',\n",
    "':*',':×',';‑)',';)','*-)','*)',';‑]',';]',';^)',';>',':‑,',';D',';3',':‑P',':P','X‑P','XP','x‑p','xp',':‑p',':p',':‑Þ',':Þ',':‑þ',':þ',':‑b',':b',\n",
    "'d:','=p','>:P',':-/',':/',':‑.',\">:/\",'=/',':L','=L',':S',':‑|',':|',':$','://)','://3',':‑X',':X',':‑#',':#',':‑&',':&','O:‑)','O:)','0:‑3'\n",
    ",'0:3','0:‑)','0:)','0;^)','>:‑)','>:)','}:‑)','}:)','3:‑)','3:)','>;‑)','>;)','>:3',';3','|;‑)','|‑O','B-)',':‑J','#‑)','%‑)','%)',':‑###..'\n",
    ",':###..','<:‑|',\"',:-|\",\"',:-l\",':E','8-X','8=X','x-3','x=3','~:>','v.v','._.','._.;','QQ  ','qq','Qq','X_X','x_x','+_+','X_x','x_X'\n",
    "'<_<','>_>','<.<','>.>','O_O','o_o','O-O','o‑o','O_o','o_O','>.<','>_<','^5','o/\\o','>_>^ ^<_<','V.v.V','V=(° °)=V','(^^^)','(::[]::)',\n",
    "'(o)(o)','( • )( • )','[̲̅$̲̅(̲̅1̲̅)̲̅$̲̅]','[̲̅$̲̅(̲̅5̲̅)̲̅$̲̅]','[̲̅$̲̅(̲̅10)̲̅$̲̅]','[̲̅$̲̅(̲̅100)̲̅$̲̅]','( ͡° ͜ʖ ͡°)','ヽ༼ຈل͜ຈ༽ﾉ','(๑ˇεˇ๑)','(◕‿◕✿)','( ༎ຶ ۝ ༎ຶ )','(=ʘᆽʘ=)∫','ʕ •ᴥ•ʔ',\n",
    "'(>_<)','(>_<)>','(>w<)',\"(';')\",'(^_^;)','(-_-;)','(~_~;)','(・.・;)','(・_・;)','^^;','(・・;)','^_^;','(#^.^#)'\n",
    ",'(^^;)','(⁄ ⁄•⁄ω⁄•⁄ ⁄)','(-.-)y-°°°','(^.^)y-.o○','(-_-)zzz','(^_-)-☆','(^_-)','((+_+))','(+o+)','(°°)','(°-°) (°.°)',\n",
    "'(°_°)','(°_°>)','(°レ°)','(o|o)','<(｀^´)>','^_^','(°o°)','(^_^)/','(^O^)／','(^o^)／','(^^)/','(≧∇≦)/','(/◕ヮ◕)/','(^o^)丿','∩(·ω·)∩',\n",
    "'(·ω·)','^ω^','_(._.)_','_(_^_)_','<(_ _)>','<m(__)m>','m(__)m','m(_ _)m','(凸ಠ益ಠ)凸',\"('_')\",'(/_;)','(T_T)','(;_;)','(;_;','(;_:)','(;O;)'\n",
    ",'(:_;)','(ToT)','(Ｔ▽Ｔ)',';_;',';-;',';n;',';;','Q.Q','T.T','TnT','QQ','Q_Q',\n",
    "'(ー_ー)!!','(-.-)','(-_-)','(一一)','(；一_一)','(=_=)','(=^・^=)','(=^・・^=)','=^_^=','(..)','(._.)',\n",
    "'.o○','○o.','_旦~~','( ^^)','_U~~','( ^^)','☆ミ','☆彡','>°)))彡','(Q))',\n",
    "'><ヨヨ (°))<<','>°))))彡','<°)))彡','>°))彡','<+','))><<','<*))','>=<','<コ:彡',\n",
    "'Ｃ:.ミ','~>°)～～～','～°·_·°～','(°°)～','●～*','￣|○',':3ミ','^m^','(・・?','(?_?)','>^_^<','<^!^>',\n",
    "'^/^','（*^_^*','§^.^§','(^<^)','(^.^)','(^ム^)','(^·^)','(^.^)','(^_^.)','(^_^)','(^^)','(^J^)',\n",
    "'(*^.^*)','^_^','(#^.^#)','（^—^）','(^^)/~~~','(^_^)/~','(;_;)/~~~','(^.^)/~~~',\n",
    "'(-_-)/~~~','($··)/~~~','(@^^)/~~~','(T_T)/~~~','(V)o￥o(V)','＼(~o~)／','＼(^o^)／','＼(-o-)／','ヽ(^。^)ノ',\n",
    "'ヽ(^o^)丿','(*^0^*)','(*_*)','(*_*;','(+_+) (@_@)','(@_@。','(＠_＠;)',\n",
    " '＼(◎o◎)／！','(*^^)v','(^^)v','(^_^)v','（’-’*) (＾ｖ＾)','(＾▽＾)','(・∀・)','(´∀`)','(⌒▽⌒）',\n",
    " '＼(^o^)／','\\(^o^)/','(~o~)','(~_~)','(^^ゞ','ˊ＿>ˋ','(p_-)', '(-_q)','((d[-_-]b))','(-\"-)','(ーー゛)',\n",
    " '(^_^メ)', '(-_-メ)','(~_~メ)','(－－〆)','(・へ・)','(｀´)'\n",
    " , '<`～´>','<`ヘ´>','(ーー;)','(^0_0^)','(＾ｖ＾)', '(＾ｕ＾)','(＾◇＾)','( ^)o(^ )','(^O^)','(^o^)','(^○^)',')^o^(',\n",
    " '(*^▽^*)','(￣ー￣)','(￣□￣;)', '°o°','°O°',':O','o_O','o_0', 'o.O','(o.o)','oO','(°◇°)','（ ﾟ Дﾟ)','(*￣m￣)',\n",
    " 'ヽ(´ー｀)┌','¯\\_(ツ)_/¯','¯\\(°_o)/¯','(´･ω･`)', '(‘A`)','(づ￣ ³￣)づ','(*^3^)/~☆','(︶｡︶✽)','(-_-) zzz'\n",
    " , 'uwu','UwU','OWO','OwO','～°·_·°～', '(°°)～',':3ミ','(´･ω･`)','(`･ω･´)','(｀-´)>', '（　´_ゝ`）'] \n",
    "\n",
    "puntuaction = ['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '.',\n",
    "           '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', \n",
    "           '`', '{', '|', '}', '~', '»', '«', '“', '”', \"-\", \"...\",\"..\",\".,\",\",.\"]\n",
    "\n",
    "\n",
    "mongoClient = pymongo.MongoClient(\"mongodb://admin:admin@localhost:27017/\")\n",
    "mydb = mongoClient[\"maadb\"]\n",
    "\n",
    "collectionTwitter = mydb[\"Twitter\"] #Questa é per la collezione dei emoji emoticon hashtag per ogni emozione\n",
    "collectionAnger = mydb[\"CollectionAnger\"]\n",
    "collectionAnticipation = mydb[\"CollectionAnticipation\"]\n",
    "collectionDisgust = mydb[\"CollectionDisgust\"]\n",
    "collectionFear = mydb[\"CollectionFear\"]\n",
    "collectionHope = mydb[\"CollectionHope\"]\n",
    "collectionAJoy = mydb[\"CollectionJoy\"]\n",
    "collectionSadness = mydb[\"CollectionSadness\"]\n",
    "collectionSurprise = mydb[\"CollectionSurprise\"]\n",
    "collectionTrust = mydb[\"CollectionTrust\"]\n",
    "\n",
    "angerCollections = mydb[\"anger\"] #Collezioni per solo i twitt di anger\n",
    "anticipationCollections = mydb[\"anticipation\"]\n",
    "disgustCollections = mydb[\"disgust\"]\n",
    "fearCollections = mydb[\"fear\"]\n",
    "joyCollections = mydb[\"joy\"]\n",
    "sadnessCollections = mydb[\"sadness\"]\n",
    "surpriseCollections = mydb[\"surprise\"]\n",
    "trustCollections = mydb[\"trust\"]\n",
    "resourcesCollection = mydb[\"Resources\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trattamento dataset Anger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lista di righe, ogni riga una tweet anger\n",
    "file = open(\"Dataset/dataset_dt_anger_60k.txt\", 'r',encoding=\"utf8\")\n",
    "anger = []\n",
    "hashtagAnger = []\n",
    "emoticonAnger = []\n",
    "emojiAnger = []\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    #Eliminazione URL e USERNAME\n",
    "    riga = re.sub(r'URL', '',riga)\n",
    "    riga = re.sub(r'USERNAME', '',riga)\n",
    "\n",
    "    #array di hashtag singoli\n",
    "    hashTag = (re.findall('#[a-zA-Z0-9]+', riga))\n",
    "\n",
    "     #generiamo l'array di hashtag senza hashtag\n",
    "    for h in hashTag:\n",
    "        hashtagAnger.append(h.replace(\"#\",\"\"))\n",
    "\n",
    "    #Rimuoviamo glin hashtag dalla riga\n",
    "    riga = riga.replace('#',\"\")\n",
    "\n",
    "    #Salvataggio frase\n",
    "    anger.append(riga)\n",
    "\n",
    "\n",
    "file.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(anger)):\n",
    "    #Controllo se l'emoticons c'è e in caso la rimuovo\n",
    "    for singleEmoticon in emoticons:\n",
    "        emoticonsFind = re.findall(re.escape(singleEmoticon),anger[i])\n",
    "        if(len(emoticonsFind) != 0):\n",
    "            emoticonAnger.extend(emoticonsFind)\n",
    "            anger[i] = anger[i].replace(singleEmoticon,\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(anger)):\n",
    "    #preleviamo le emoji e le rimuoviamo dal testo\n",
    "    data = regex.findall(r'\\X', anger[i])\n",
    "    for word in data:\n",
    "        if any(char in emoji.UNICODE_EMOJI['en'] for char in word):\n",
    "            emojiAnger.append(word)\n",
    "            anger[i] = anger[i].replace(word, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processamento slang \n",
    "with open(\"Slangs/slangs_before.json\") as f:\n",
    "    slangsBefore = json.load(f)\n",
    "    for i in range(len(anger)):\n",
    "        for word in anger[i].split():\n",
    "            indexOfWord = anger[i].find(word)\n",
    "            if slangsBefore.get(word) is None:\n",
    "                if slangsBefore.get(word.lower()) is None:\n",
    "                    if slangsBefore.get(word.upper()) is None:\n",
    "                        continue\n",
    "                    else:\n",
    "                        anger[i] = anger[i][:indexOfWord] + slangsBefore.get(word.upper()) + anger[i][indexOfWord+len(word):]\n",
    "                else:\n",
    "                    anger[i] = anger[i][:indexOfWord] + slangsBefore.get(word.lower()) + anger[i][indexOfWord+len(word):]\n",
    "            else:\n",
    "                anger[i] = anger[i][:indexOfWord] + slangsBefore.get(word) + anger[i][indexOfWord+len(word):]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizzazione di tutte le frasi\n",
    "angerTokenized = []\n",
    "for linea in anger:\n",
    "    result = word_tokenize(linea)\n",
    "    angerTokenized.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Esecuzione della POS tagging per ogni riga della matrice\n",
    "for i in range(len(angerTokenized)):\n",
    "    angerTokenized[i] = nltk.pos_tag(angerTokenized[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trattamento punteggiatura\n",
    "for line in angerTokenized:\n",
    "    newLine = line\n",
    "    listToRemove = []\n",
    "    for i in range(len(line)):\n",
    "        if line[i][0] in puntuaction:\n",
    "            listToRemove.append(line[i])\n",
    "    for element in listToRemove:\n",
    "        newLine.remove(element)\n",
    "    line = newLine\n",
    "\n",
    "linePuntuaction = re.compile(\"[\" + re.escape(\"\".join(puntuaction)) + \"]\")\n",
    "for line in angerTokenized:\n",
    "    for i in range(len(line)):\n",
    "        tupla = [(re.sub(linePuntuaction, \"\", line[i][0])),line[i][1]]\n",
    "        line[i] = tupla\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming\n",
    "#ps = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for line in angerTokenized:\n",
    "    for i in range(len(line)):\n",
    "        tupla = [lemmatizer.lemmatize(line[i][0]),line[i][1]]\n",
    "        line[i] = tupla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trattamnento Stop word dopo Stemming\n",
    "stop_words = set(stopwords.words('english'))\n",
    "angerTokStoped = []\n",
    "for line in angerTokenized:\n",
    "    filtred_word = []\n",
    "    for row in line:\n",
    "        if  row[0] not in stop_words:\n",
    "            filtred_word.append(row) \n",
    "    angerTokStoped.append(filtred_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#liste singole di parole e token\n",
    "angerListWord = []\n",
    "angerListTok = []\n",
    "for line in angerTokStoped:\n",
    "    for row in line:\n",
    "        if row[0] != '':\n",
    "            angerListWord.append(row[0])\n",
    "        angerListTok.append(row[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lavorazione risorse lessicali\n",
    "resourceAngerEmoSN = []\n",
    "resourceAngerNRC = []\n",
    "resourceAngerSentisense = []\n",
    "\n",
    "#Lista di righe, ogni riga una tweet anger\n",
    "file = open(\"Risorse lessicali\\Anger\\EmoSN_anger.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        resourceAngerEmoSN.append(riga)\n",
    "\n",
    "file.close()\n",
    "\n",
    "file = open(\"Risorse lessicali\\\\Anger\\\\NRC_anger.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        resourceAngerNRC.append(riga)\n",
    "\n",
    "file.close()\n",
    "\n",
    "file = open(\"Risorse lessicali\\Anger\\sentisense_anger.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        resourceAngerSentisense.append(riga)\n",
    "\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.InsertOneResult at 0x1b86032b4c0>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Caricare le risorse lessicali \n",
    "\n",
    "AngerEmoSN = {\n",
    "    '_id': 'EmoSN_anger',\n",
    "    'sentiment': 'anger', \n",
    "    'wordList': resourceAngerEmoSN\n",
    "}\n",
    "\n",
    "AngerNRC = {\n",
    "    '_id': 'NRC_anger',\n",
    "    'sentiment': 'anger',  \n",
    "    'wordList': resourceAngerNRC\n",
    "}\n",
    "\n",
    "AngerSentisense = {\n",
    "    '_id': 'sentisense_anger',\n",
    "    'sentiment': 'anger',\n",
    "    'wordList': resourceAngerSentisense\n",
    "}\n",
    "\n",
    "resourcesCollection.insert_one(AngerEmoSN)\n",
    "resourcesCollection.insert_one(AngerNRC)\n",
    "resourcesCollection.insert_one(AngerSentisense)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conteggio delle frequenze delle parole\n",
    "countWordAnger = Counter(angerListWord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione database e collezioni\n",
    "# Inserimento singoli documenti contententi i tweet anger\n",
    "wordsString = {}\n",
    "\n",
    "for i in range(len(angerListWord)):\n",
    "    wordsString = {\n",
    "        \"lemma\": angerListWord[i],\n",
    "        \"POS\": angerListTok[i]\n",
    "    }\n",
    "    angerTwit={\"word\":wordsString}\n",
    "    angerCollections.insert_one(angerTwit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inserimento collezione Twitter con hashtags, emoticons ed emoji \n",
    "angerInsert = {\n",
    "    \"sentiment\":\"anger\",\n",
    "    \"hashtags\": hashtagAnger,\n",
    "    \"emoticons\": emoticonAnger,\n",
    "    \"emoji\": emojiAnger\n",
    "}\n",
    "\n",
    "#inserimento\n",
    "x = collectionAnger.insert_one(angerInsert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raggruppa i documenti di input in base all'espressione _id specificata e per ciascun raggruppamento distinto genera un documento\n",
    "pipeline = [\n",
    "    {\"$group\": {\"_id\": \"$word.lemma\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"word\": \"$_id\",\n",
    "      \"freq\": \"$count\"\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"freq\": -1\n",
    "    }},\n",
    "    {\"$out\": \"anger\"}\n",
    "]\n",
    "\n",
    "#post processing per renderlo valido \n",
    "angerCollections.aggregate(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creazione Word Clouds per le parole\n",
    "data = {}\n",
    "for document in angerCollections.find().limit(50):\n",
    "    data[document['word']] = document['freq']\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2)\n",
    "wc.fit_words(data)\n",
    "wc.to_file('Word Clouds/angerWords.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conteggio degli hashtags\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$hashtags\"},\n",
    "    {\"$group\": {\"_id\": \"$hashtags\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"word\": \"$_id\",\n",
    "      \"count\": 1\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"count\": -1\n",
    "    }}\n",
    "]\n",
    "\n",
    "hashtagsCount = collectionAnger.aggregate(pipeline)\n",
    "\n",
    "data = {}\n",
    "for document in hashtagsCount:\n",
    "    data[document['word']] = document['count']\n",
    "\n",
    "collectionAnger.update_one(\n",
    "  {\"sentiment\": \"anger\"},\n",
    "  {\"$set\": {\"hashtags\": data}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creazione Word Clouds per gli hashtags\n",
    "\n",
    "hashtags = collectionAnger.distinct(\"hashtags\")\n",
    "\n",
    "hashtagsLimited = {}\n",
    "for (word,count) in [x for x in hashtags[0].items()][:50]:\n",
    "    hashtagsLimited[word] = count\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2)\n",
    "wc.fit_words(hashtagsLimited)\n",
    "wc.to_file('Word Clouds/angerHashtags.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conteggio delle emoji\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$emoji\"},\n",
    "    {\"$group\": {\"_id\": \"$emoji\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"emoji\": \"$_id\",\n",
    "      \"count\": 1\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"count\": -1\n",
    "    }}\n",
    "]\n",
    "\n",
    "emojiCount = collectionAnger.aggregate(pipeline)\n",
    "\n",
    "data = {}\n",
    "for document in emojiCount:\n",
    "    data[document['emoji']] = document['count']\n",
    "\n",
    "collectionAnger.update_one(\n",
    "  {\"sentiment\": \"anger\"},\n",
    "  {\"$set\": {\"emoji\": data}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creazione Word Clouds per le emoji\n",
    "\n",
    "emojis = collectionAnger.distinct(\"emoji\")\n",
    "\n",
    "emojisLimited = {}\n",
    "for (word,count) in [x for x in emojis[0].items()][:50]:\n",
    "    emojisLimited[word] = count\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2, font_path='Font Emoji\\Symbola.otf')\n",
    "wc.fit_words(emojisLimited)\n",
    "wc.to_file('Word Clouds/angerEmojis.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conteggio degli emoticons\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$emoticons\"},\n",
    "    {\"$group\": {\"_id\": \"$emoticons\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"emoticons\": \"$_id\",\n",
    "      \"count\": 1\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"count\": -1\n",
    "    }}\n",
    "]\n",
    "\n",
    "emoticonsCount = collectionAnger.aggregate(pipeline)\n",
    "\n",
    "data = {}\n",
    "for document in emoticonsCount:\n",
    "    data[document['emoticons']] = document['count']\n",
    "\n",
    "collectionAnger.update_one(\n",
    "  {\"sentiment\": \"anger\"},\n",
    "  {\"$set\": {\"emoticons\": data}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creazione Word Clouds per le emoticons\n",
    "\n",
    "emoticons = collectionAnger.distinct(\"emoticons\")\n",
    "\n",
    "emoticonsLimited = {}\n",
    "for (word,count) in [x for x in emoticons[0].items()][:50]:\n",
    "    emoticonsLimited[word] = count\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2)\n",
    "wc.fit_words(emoticonsLimited)\n",
    "wc.to_file('Word Clouds/angerEmoticons.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline di aggregazione per inserire la risorsa lessicale a cui appartiene la parola\n",
    "\n",
    "pipeline = [\n",
    "    {\"$lookup\": {\n",
    "        \"from\": \"Resources\",\n",
    "        \"localField\": \"word\",\n",
    "        \"foreignField\": \"wordList\",\n",
    "        \"pipeline\": [\n",
    "            {\"$match\": {\"$expr\": {\"$regexMatch\": {\"input\": \"$_id\", \"regex\": \"anger\"}}}}\n",
    "        ],\n",
    "        \"as\": \"LexResources\"\n",
    "    }},\n",
    "    {\"$project\": {\n",
    "        \"resources.wordList\": 0,\n",
    "        \"resources.totNumberWords\": 0,\n",
    "        \"resources.sentiment\": 0,\n",
    "        \"LexResources.sentiment\": 0,\n",
    "        \"LexResources.totNumberWords\": 0,\n",
    "        \"LexResources.wordList\": 0\n",
    "    }},\n",
    "    {\"$out\": \"anger\"}\n",
    "]\n",
    "\n",
    "angerCollections.aggregate(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline di aggregazione per calcolare le percentuali richieste\n",
    "\n",
    "pipelineCount = [\n",
    "    {\"$group\": {\"_id\": \"null\", \"count\": { \"$sum\": \"$freq\" }}},\n",
    "    {\"$project\": {\n",
    "        \"_id\": 0,\n",
    "        \"count\": \"$count\"\n",
    "    }}\n",
    "]\n",
    "\n",
    "results = angerCollections.aggregate(pipelineCount)\n",
    "\n",
    "countWords = 0\n",
    "for data in results:\n",
    "    countWords = data['count']\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$LexResources\"},\n",
    "    {\"$lookup\": {\n",
    "        \"from\": \"Resources\",\n",
    "        \"localField\": \"LexResources._id\",\n",
    "        \"foreignField\": \"_id\",\n",
    "        \"as\": \"RefResource\"\n",
    "    }},\n",
    "    {\"$group\": {\"_id\": { \"lex\": \"$LexResources._id\", \"RefResource\": \"$RefResource\" }, \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "        \"_id\": \"$_id.lex\",\n",
    "        \"wordList\": \"$_id.RefResource.wordList\",\n",
    "        \"commonWords\": \"$count\"\n",
    "    }},\n",
    "    {\"$unwind\": \"$wordList\"},\n",
    "    {\"$project\": {\n",
    "        \"_id\": 1,\n",
    "        \"commonWords\": 1,\n",
    "        \"totWordsLex\": {\"$size\": \"$wordList\"}\n",
    "    }},\n",
    "    {\"$addFields\": {\"totalWordsTwitter\": countWords}},\n",
    "    {\"$project\": {\n",
    "        \"perc_presence_lex_res\": { \"$round\": [{\"$divide\": [ \"$commonWords\", \"$totWordsLex\" ] }, 6] },\n",
    "        \"perc_presence_twitter\": { \"$round\": [{\"$divide\": [ \"$commonWords\", \"$totalWordsTwitter\" ]}, 6] }\n",
    "    }}\n",
    "]\n",
    "\n",
    "results = angerCollections.aggregate(pipeline)\n",
    "\n",
    "# Genero i dati per l'istogramma\n",
    "istogramAnger_lexRes = []\n",
    "istogramAnger_twitter = []\n",
    "\n",
    "for data in results:\n",
    "    print(data)\n",
    "    log =math.log(data['perc_presence_lex_res'],10)\n",
    "    istogramAnger_lexRes.append([data['_id'], data['perc_presence_lex_res']])\n",
    "    log =math.log(data['perc_presence_twitter'],10)\n",
    "    istogramAnger_twitter.append([data['_id'], data['perc_presence_twitter']])\n",
    " \n",
    "# Create the pandas DataFrame\n",
    "dfAnger_lexRes = pd.DataFrame(istogramAnger_lexRes, columns = ['Risorsa', 'Percentuale'])\n",
    "dfAnger_twitter = pd.DataFrame(istogramAnger_twitter,columns = ['Risorsa', 'Percentuale'])      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione istogramma per presenza parole nelle risorse lessicali\n",
    "\n",
    "dfAnger_lexRes.plot.bar(x='Risorsa')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione istogramma per presenza parole nei twitter\n",
    "\n",
    "dfAnger_twitter.plot.bar(x='Risorsa')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connessione, creazione cursore e database\n",
    "\n",
    "conn = psycopg2.connect(database = \"maadb\", user = \"postgres\", password = \"admin\", host = \"localhost\", port = \"5432\")\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione tabella risorse lessicali anger e inserimento\n",
    "cur.execute('''CREATE TABLE RISORSEANGER\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    NOME             TEXT       NOT NULL,\n",
    "                    PAROLA           TEXT       NOT NULL\n",
    "                );\n",
    "            '''\n",
    ")\n",
    "nome = \"EmoSN_anger\"\n",
    "for word in resourceAngerEmoSN:\n",
    "    cur.execute(\"INSERT INTO RISORSEANGER (NOME, PAROLA) VALUES(%s, %s)\",(nome,word))\n",
    "\n",
    "nome = \"NRC_anger\"\n",
    "for word in resourceAngerNRC:\n",
    "    cur.execute(\"INSERT INTO RISORSEANGER (NOME, PAROLA) VALUES(%s, %s)\",(nome,word))\n",
    "\n",
    "nome = \"sentisense_anger\"\n",
    "for word in resourceAngerSentisense:\n",
    "    cur.execute(\"INSERT INTO RISORSEANGER (NOME, PAROLA) VALUES(%s, %s)\",(nome,word))\n",
    "\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione tabelle per le parole, emoji, emoticon e hashtag\n",
    "\n",
    "cur.execute('''CREATE TABLE PAROLEANGER\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    PAROLA           TEXT       NOT NULL, \n",
    "                    EmoSN_anger      BOOL       NOT NULL DEFAULT FALSE,\n",
    "                    NRC_anger      BOOL       NOT NULL DEFAULT FALSE,\n",
    "                    sentisense_anger      BOOL       NOT NULL DEFAULT FALSE,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ")   \n",
    "\n",
    "cur.execute('''CREATE TABLE EMOJIANGER\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    EMOJI       TEXT       NOT NULL,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ") \n",
    "\n",
    "cur.execute('''CREATE TABLE EMOTICONSANGER\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    EMOTICONS       TEXT       NOT NULL,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ") \n",
    "\n",
    "cur.execute('''CREATE TABLE HASHTAGANGER\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    HASHTAG     TEXT       NOT NULL,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ") \n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inserimento hashtag, emoticon ed emoji\n",
    "\n",
    "for hashTag in hashtagAnger:\n",
    "    cur.execute(\"INSERT INTO HASHTAGANGER (HASHTAG,FREQUENCY) VALUES(%s,%s)\",(hashTag,0))\n",
    "\n",
    "for emoticons in emoticonAnger:\n",
    "    cur.execute(\"INSERT INTO EMOTICONSANGER (EMOTICONS,FREQUENCY) VALUES(%s,%s)\",(emoticons,0))\n",
    "\n",
    "for emojis in emojiAnger:\n",
    "    cur.execute(\"INSERT INTO EMOJIANGER (EMOJI,FREQUENCY) VALUES(%s,%s)\",(emojis,0))\n",
    "\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inserimento parole in db\n",
    "for word in angerListWord:\n",
    "    cur.execute(\"INSERT INTO PAROLEANGER (PAROLA,EmoSN_anger,NRC_anger,sentisense_anger,FREQUENCY) VALUES(%s,%s,%s,%s,%s)\",(word,False,False,False,0))\n",
    "\n",
    "conn.commit()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze hashtag\n",
    "cur.execute(\"update hashtaganger as ha1 set frequency = (select count(*) from hashtaganger ha2 where ha1.hashtag = ha2.hashtag);\")\n",
    "cur.execute(\"delete from hashtaganger as ha1 where exists (Select 1 From hashtaganger as ha2 where ha1.hashtag = ha2.hashtag and ha1.frequency = ha2.frequency and ha2.ctid > ha1.ctid);\")\n",
    "\n",
    "conn.commit()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze emoji\n",
    "cur.execute(\"update EMOJIANGER as ha1 set frequency = (select count(*) from EMOJIANGER ha2 where ha1.emoji = ha2.emoji);\")\n",
    "cur.execute(\"delete from EMOJIANGER as ha1 where exists (Select 1 From EMOJIANGER as ha2 where ha1.emoji = ha2.emoji and ha1.frequency = ha2.frequency and ha2.ctid > ha1.ctid);\")\n",
    "\n",
    "conn.commit()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze emoticons\n",
    "cur.execute(\"update EMOTICONSANGER as ha1 set frequency = (select count(*) from EMOTICONSANGER ha2 where ha1.emoticons = ha2.emoticons);\")\n",
    "cur.execute(\"delete from EMOTICONSANGER as ha1 where exists (Select 1 From EMOTICONSANGER as ha2 where ha1.emoticons = ha2.emoticons and ha1.frequency = ha2.frequency and ha2.ctid > ha1.ctid);\")\n",
    "\n",
    "conn.commit()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze parole (ci mette più di 100 minuti)\n",
    "\"\"\"\n",
    "cur.execute(\"update PAROLEANGER as ha1 set frequency = (select count(*) from PAROLEANGER ha2 where ha1.parola = ha2.parola);\")\n",
    "cur.execute(\"delete from PAROLEANGER as ha1 where exists (Select 1 From PAROLEANGER as ha2 where ha1.parola = ha2.parola and ha1.frequency = ha2.frequency and ha2.ctid > ha1.ctid);\")\n",
    "\n",
    "conn.commit()  \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze parole\n",
    "cur.execute(\"select count(*),parola from paroleanger group by parola\")\n",
    "result = []\n",
    "for word in cur.fetchall():\n",
    "    result.append(word) \n",
    "\n",
    "cur.execute(\"truncate table paroleanger;\")\n",
    "cur.execute(\"ALTER SEQUENCE paroleanger_id_seq RESTART WITH 1;\")\n",
    "\n",
    "for couple in result:\n",
    "    cur.execute(\"INSERT INTO PAROLEANGER (PAROLA,EmoSN_anger,NRC_anger,sentisense_anger,FREQUENCY) VALUES(%s,%s,%s,%s,%s)\",(couple[1],False,False,False,couple[0]))\n",
    "\n",
    "conn.commit()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggiornamento presenza parola nelle risorse lessicali\n",
    "cur.execute(\"UPDATE paroleanger SET EmoSN_anger = TRUE WHERE parola IN (select p.parola from paroleanger as p LEFT OUTER JOIN risorseanger as r ON p.parola = r.parola WHERE r.nome = 'EmoSN_anger')\")\n",
    "cur.execute(\"UPDATE paroleanger SET NRC_ANGER = TRUE WHERE parola IN (select p.parola from paroleanger as p LEFT OUTER JOIN risorseanger as r ON p.parola = r.parola WHERE r.nome = 'NRC_anger')\")\n",
    "cur.execute(\"UPDATE paroleanger SET sentisense_anger = TRUE WHERE parola IN (select p.parola from paroleanger as p LEFT OUTER JOIN risorseanger as r ON p.parola = r.parola WHERE r.nome = 'sentisense_anger')\")\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvataggio in un txt delle parole non presenti in nessuna risorsa lessicale\n",
    "\n",
    "wordsMongo = angerCollections.find({\"LexResources\": []}, {\"_id\": 0, \"freq\": 0, \"LexResources\": 0})\n",
    "words = []\n",
    "\n",
    "for word in wordsMongo:\n",
    "    words.append(word['word'])\n",
    "\n",
    "with open('Nuove Parole/anger.txt', 'w',encoding=\"utf8\") as f:\n",
    "    for word in words:\n",
    "        f.write(\"%s\\n\" % word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trattamento dataset Anticipation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lista di righe, ogni riga una tweet anticipation\n",
    "file = open(\"Dataset/dataset_dt_anticipation_60k.txt\", 'r',encoding=\"utf8\")\n",
    "anticipation = []\n",
    "hashtagAnticipation = []\n",
    "emoticonAnticipation = []\n",
    "emojiAnticipation = []\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "    riga = re.sub(r'URL', '',riga)\n",
    "    riga = re.sub(r'USERNAME', '',riga)\n",
    "\n",
    "    #array di hashtag singoli\n",
    "    hashTag = (re.findall('#[a-zA-Z0-9]+', riga))\n",
    "\n",
    "    #generiamo l'array di hashtag senza hashtag\n",
    "    for h in hashTag:\n",
    "        hashtagAnticipation.append(h.replace(\"#\",\"\"))\n",
    "\n",
    "    #Rimuoviamo glin hashtag dalla riga\n",
    "    riga = riga.replace('#',\"\")\n",
    "    \n",
    "    anticipation.append(riga)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(anticipation)):\n",
    "    #Controllo se l'emoticons c'è e in caso la rimuovo\n",
    "    for singleEmoticon in emoticons:\n",
    "        emoticonsFind = re.findall(re.escape(singleEmoticon),anticipation[i])\n",
    "        if(len(emoticonsFind) != 0):\n",
    "            emoticonAnticipation.extend(emoticonsFind)\n",
    "            anticipation[i] = anticipation[i].replace(singleEmoticon,\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(anticipation)):\n",
    "    #preleviamo le emoji e le rimuoviamo dal testo\n",
    "    data = regex.findall(r'\\X', anticipation[i])\n",
    "    for word in data:\n",
    "        if any(char in emoji.UNICODE_EMOJI['en'] for char in word):\n",
    "            emojiAnticipation.append(word)\n",
    "            anticipation[i] = anticipation[i].replace(word, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processamento slang prima di punteggiatura e lowercase\n",
    "with open(\"Slangs/slangs_before.json\") as f:\n",
    "    slangsBefore = json.load(f)\n",
    "    for i in range(len(anticipation)):\n",
    "        for word in anticipation[i].split():\n",
    "            indexOfWord = anticipation[i].find(word)\n",
    "            if slangsBefore.get(word) is None:\n",
    "                if slangsBefore.get(word.lower()) is None:\n",
    "                    if slangsBefore.get(word.upper()) is None:\n",
    "                        continue\n",
    "                    else:\n",
    "                        anticipation[i] = anticipation[i][:indexOfWord] + slangsBefore.get(word.upper()) + anticipation[i][indexOfWord+len(word):]\n",
    "                else:\n",
    "                    anticipation[i] = anticipation[i][:indexOfWord] + slangsBefore.get(word.lower()) + anticipation[i][indexOfWord+len(word):]\n",
    "            else:\n",
    "                anticipation[i] = anticipation[i][:indexOfWord] + slangsBefore.get(word) + anticipation[i][indexOfWord+len(word):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizzazione di tutte le frasi\n",
    "anticipationTokenized = []\n",
    "for linea in anticipation:\n",
    "    result = word_tokenize(linea)\n",
    "    anticipationTokenized.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Esecuzione della POS tagging per ogni riga della matrice\n",
    "for i in range(len(anticipationTokenized)):\n",
    "    anticipationTokenized[i] = nltk.pos_tag(anticipationTokenized[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trattamento punteggiatura\n",
    "for line in anticipationTokenized:\n",
    "    newLine = line\n",
    "    listToRemove = []\n",
    "    for i in range(len(line)):\n",
    "        if line[i][0] in puntuaction:\n",
    "            listToRemove.append(line[i])\n",
    "    for element in listToRemove:\n",
    "        newLine.remove(element)\n",
    "    line = newLine\n",
    "\n",
    "linePuntuaction = re.compile(\"[\" + re.escape(\"\".join(puntuaction)) + \"]\")\n",
    "for line in anticipationTokenized:\n",
    "    for i in range(len(line)):\n",
    "        tupla = [(re.sub(linePuntuaction, \"\", line[i][0])),line[i][1]]\n",
    "        line[i] = tupla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming\n",
    "#ps = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for line in anticipationTokenized:\n",
    "    for i in range(len(line)):\n",
    "        tupla = [lemmatizer.lemmatize(line[i][0]),line[i][1]]\n",
    "        line[i] = tupla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trattamnento Stop word dopo Stemming\n",
    "stop_words = set(stopwords.words('english'))\n",
    "anticipationTokStoped = []\n",
    "for line in anticipationTokenized:\n",
    "    filtred_word = []\n",
    "    for row in line:\n",
    "        if  row[0] not in stop_words:\n",
    "            filtred_word.append(row) \n",
    "    anticipationTokStoped.append(filtred_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lavorazione risorse lessicali\n",
    "resourceNRC_anticipation = []\n",
    "resourceSentisense_anticipation = []\n",
    "#Lista di righe, ogni riga una tweet anger\n",
    "file = open(\"Risorse lessicali\\\\Anticipation\\\\NRC_anticipation.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "            resourceNRC_anticipation.append(riga)\n",
    "\n",
    "file.close()\n",
    "\n",
    "file = open(\"Risorse lessicali\\Anticipation\\sentisense_anticipation.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "            resourceSentisense_anticipation.append(riga)\n",
    "\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Caricare le risorse lessicali \n",
    "\n",
    "\n",
    "AnticipationNRC = {\n",
    "    '_id': 'NRC_anticipation',\n",
    "    'sentiment': 'anticipation',  \n",
    "    'wordList': resourceNRC_anticipation\n",
    "}\n",
    "\n",
    "AnticipationSentisense = {\n",
    "    '_id': 'sentisense_anticipation',\n",
    "    'sentiment': 'anticipation',\n",
    "    'wordList': resourceSentisense_anticipation\n",
    "}\n",
    "\n",
    "resourcesCollection.insert_one(AnticipationNRC)\n",
    "resourcesCollection.insert_one(AnticipationSentisense)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anticipationListWord = []\n",
    "anticipationListTok = []\n",
    "for line in anticipationTokStoped:\n",
    "    for row in line:\n",
    "        if row[0] != '':\n",
    "            anticipationListWord.append(row[0])\n",
    "        anticipationListTok.append(row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conteggio delle frequenze delle parole\n",
    "countWordAnticipation = Counter(anticipationListWord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione database e collezioni\n",
    "# Inserimento singoli documenti contententi i tweet anger\n",
    "wordsString = {}\n",
    "\n",
    "for i in range(len(anticipationListWord)):\n",
    "    wordsString = {\n",
    "        \"lemma\": anticipationListWord[i],\n",
    "        \"POS\": anticipationListTok[i]\n",
    "    }\n",
    "    anticipationTwit={\"word\":wordsString}\n",
    "    anticipationCollections.insert_one(anticipationTwit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inserimento Documento\n",
    "anticipationInsert = {\n",
    "    \"sentiment\":\"anticipation\",\n",
    "    \"hashtags\": hashtagAnticipation,\n",
    "    \"emoticons\": emoticonAnticipation,\n",
    "    \"emoji\": emojiAnticipation\n",
    "}\n",
    "\n",
    "#inserimento\n",
    "x = collectionAnticipation.insert_one(anticipationInsert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# match --> { $match: { <query> } } se si soddisfa una query\n",
    "# project --> { $project: { <specification(s)> } } prende un documento che può specificare l'inclusione dei campi, \n",
    "# la soppressione del campo _id, l'aggiunta di nuovi campi e il ripristino dei valori dei campi esistenti\n",
    "# addfield --> { $addFields: { <newField>: <expression>, ... } } Specificare il nome di ogni campo da aggiungere e impostarne il valore su un'espressione di aggregazione\n",
    "# lookup --> {$lookup: {from , localField, foreignkey, as   } } Per eseguire una corrispondenza di uguaglianza tra un campo dei documenti di input\n",
    "# group --> {$groud: {_id:<expression> , <field> : {<accumulator1> : <expression1>}} } \n",
    "#   Raggruppa i documenti di input in base all'espressione _id specificata e per ciascun raggruppamento distinto genera un documento\n",
    "\n",
    "pipeline = [\n",
    "    {\"$group\": {\"_id\": \"$word.lemma\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"word\": \"$_id\",\n",
    "      \"freq\": \"$count\"\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"freq\": -1\n",
    "    }},\n",
    "    {\"$out\": \"anticipation\"}\n",
    "]\n",
    "\n",
    "\n",
    "wordCount = anticipationCollections.aggregate(pipeline)\n",
    "\"\"\"\n",
    "#Creazione Word Clouds per le parole\n",
    "data = {}\n",
    "for document in anticipationCollections.find().limit(50):\n",
    "    data[document['word']] = document['freq']\n",
    "\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2)\n",
    "wc.fit_words(data)\n",
    "wc.to_file('Word Clouds/anticipationWords.png')\n",
    "\"\"\"\n",
    "# Conteggio degli hashtags\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$hashtags\"},\n",
    "    {\"$group\": {\"_id\": \"$hashtags\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"word\": \"$_id\",\n",
    "      \"count\": 1\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"count\": -1\n",
    "    }}\n",
    "]\n",
    "\n",
    "hashtagsCount = collectionAnticipation.aggregate(pipeline)\n",
    "\n",
    "data = {}\n",
    "for document in hashtagsCount:\n",
    "    data[document['word']] = document['count']\n",
    "\n",
    "collectionAnticipation.update_one(\n",
    "  {\"sentiment\": \"anticipation\"},\n",
    "  {\"$set\": {\"hashtags\": data}}\n",
    ")\n",
    "\n",
    "#Creazione Word Clouds per gli hashtag\n",
    "\"\"\"\n",
    "hashtags = collectionAnticipation.distinct(\"hashtags\")\n",
    "\n",
    "hashtagsLimited = {}\n",
    "for (word,count) in [x for x in hashtags[0].items()][:50]:\n",
    "    hashtagsLimited[word] = count\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2)\n",
    "wc.fit_words(hashtagsLimited)\n",
    "wc.to_file('Word Clouds/anticipationHashtags.png')\n",
    "\"\"\"\n",
    "# Conteggio delle emoji\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$emoji\"},\n",
    "    {\"$group\": {\"_id\": \"$emoji\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"emoji\": \"$_id\",\n",
    "      \"count\": 1\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"count\": -1\n",
    "    }}\n",
    "]\n",
    "\n",
    "emojiCount = collectionAnticipation.aggregate(pipeline)\n",
    "\n",
    "data = {}\n",
    "for document in emojiCount:\n",
    "    data[document['emoji']] = document['count']\n",
    "\n",
    "collectionAnticipation.update_one(\n",
    "  {\"sentiment\": \"anticipation\"},\n",
    "  {\"$set\": {\"emoji\": data}}\n",
    ")\n",
    "\n",
    "#Creazione Word Clouds per le emoji\n",
    "\"\"\"\n",
    "emojis = collectionAnticipation.distinct(\"emoji\")\n",
    "\n",
    "emojisLimited = {}\n",
    "for (word,count) in [x for x in emojis[0].items()][:50]:\n",
    "    emojisLimited[word] = count\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2)\n",
    "wc.fit_words(emojisLimited)\n",
    "wc.to_file('Word Clouds/anticipationEmojis.png')\n",
    "\"\"\"\n",
    "# Conteggio degli emoticons\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$emoticons\"},\n",
    "    {\"$group\": {\"_id\": \"$emoticons\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"emoticons\": \"$_id\",\n",
    "      \"count\": 1\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"count\": -1\n",
    "    }}\n",
    "]\n",
    "\n",
    "emoticonsCount = collectionAnticipation.aggregate(pipeline)\n",
    "\n",
    "data = {}\n",
    "for document in emoticonsCount:\n",
    "    data[document['emoticons']] = document['count']\n",
    "\n",
    "collectionAnticipation.update_one(\n",
    "  {\"sentiment\": \"anticipation\"},\n",
    "  {\"$set\": {\"emoticons\": data}}\n",
    ")\n",
    "\"\"\"\n",
    "#Creazione Word Clouds per le emoticons\n",
    "emoticons = collectionAnticipation.distinct(\"emoticons\")\n",
    "\n",
    "emoticonsLimited = {}\n",
    "for (word,count) in [x for x in emoticons[0].items()][:50]:\n",
    "    emoticonsLimited[word] = count\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2)\n",
    "wc.fit_words(emoticonsLimited)\n",
    "wc.to_file('Word Clouds/anticipationEmoticons.png')\n",
    "\"\"\"\n",
    "#Pipeline di aggregazione per inserire la risorsa lessicale a cui appartiene la parola\n",
    "pipeline = [\n",
    "    {\"$lookup\": {\n",
    "        \"from\": \"Resources\",\n",
    "        \"localField\": \"word\",\n",
    "        \"foreignField\": \"wordList\",\n",
    "        \"pipeline\": [\n",
    "            {\"$match\": {\"$expr\": {\"$regexMatch\": {\"input\": \"$_id\", \"regex\": \"anticipation\"}}}}\n",
    "        ],\n",
    "        \"as\": \"LexResources\"\n",
    "    }},\n",
    "    {\"$project\": {\n",
    "        \"resources.wordList\": 0,\n",
    "        \"resources.totNumberWords\": 0,\n",
    "        \"resources.sentiment\": 0,\n",
    "        \"LexResources.sentiment\": 0,\n",
    "        \"LexResources.totNumberWords\": 0,\n",
    "        \"LexResources.wordList\": 0\n",
    "    }},\n",
    "    {\"$out\": \"anticipation\"}\n",
    "]\n",
    "\n",
    "anticipationCollections.aggregate(pipeline)\n",
    "\n",
    "#Pipeline di aggregazione per calcolare le percentuali richieste\n",
    "\n",
    "pipelineCount = [\n",
    "    {\"$group\": {\"_id\": \"null\", \"count\": { \"$sum\": \"$freq\" }}},\n",
    "    {\"$project\": {\n",
    "        \"_id\": 0,\n",
    "        \"count\": \"$count\"\n",
    "    }}\n",
    "]\n",
    "\n",
    "results = anticipationCollections.aggregate(pipelineCount)\n",
    "\n",
    "countWords = 0\n",
    "for data in results:\n",
    "    countWords = data['count']\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$LexResources\"},\n",
    "    {\"$lookup\": {\n",
    "        \"from\": \"Resources\",\n",
    "        \"localField\": \"LexResources._id\",\n",
    "        \"foreignField\": \"_id\",\n",
    "        \"as\": \"RefResource\"\n",
    "    }},\n",
    "    {\"$group\": {\"_id\": { \"lex\": \"$LexResources._id\", \"RefResource\": \"$RefResource\" }, \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "        \"_id\": \"$_id.lex\",\n",
    "        \"wordList\": \"$_id.RefResource.wordList\",\n",
    "        \"commonWords\": \"$count\"\n",
    "    }},\n",
    "    {\"$unwind\": \"$wordList\"},\n",
    "    {\"$project\": {\n",
    "        \"_id\": 1,\n",
    "        \"commonWords\": 1,\n",
    "        \"totWordsLex\": {\"$size\": \"$wordList\"}\n",
    "    }},\n",
    "    {\"$addFields\": {\"totalWordsTwitter\": countWords}},\n",
    "    {\"$project\": {\n",
    "        \"perc_presence_lex_res\": { \"$round\": [{\"$divide\": [ \"$commonWords\", \"$totWordsLex\" ] }, 6] },\n",
    "        \"perc_presence_twitter\": { \"$round\": [{\"$divide\": [ \"$commonWords\", \"$totalWordsTwitter\" ]}, 6] }\n",
    "    }}\n",
    "]\n",
    "\n",
    "results = anticipationCollections.aggregate(pipeline)\n",
    "\n",
    "# Genero i dati per l'istogramma\n",
    "istogramAnticipation_lexRes = []\n",
    "istogramAnticipation_twitter = []\n",
    "\n",
    "for data in results:\n",
    "    print(data)\n",
    "    log =math.log(data['perc_presence_lex_res'],10)\n",
    "    istogramAnticipation_lexRes.append([data['_id'], data['perc_presence_lex_res']])\n",
    "    log =math.log(data['perc_presence_twitter'],10)\n",
    "    istogramAnticipation_twitter.append([data['_id'], data['perc_presence_twitter']])\n",
    " \n",
    "# Create the pandas DataFrame\n",
    "dfAnticipation_lexRes = pd.DataFrame(istogramAnticipation_lexRes, columns = ['Risorsa', 'Percentuale'])\n",
    "dfAnticipation_twitter = pd.DataFrame(istogramAnticipation_twitter,columns = ['Risorsa', 'Percentuale'])      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione istogramma per presenza parole nei twitter\n",
    "\n",
    "dfAnticipation_lexRes.plot.bar(x='Risorsa')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione istogramma per presenza parole nei twitter\n",
    "\n",
    "dfAnticipation_twitter.plot.bar(x='Risorsa')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connessione, creazione cursore e database\n",
    "conn = psycopg2.connect(database = \"maadb\", user = \"postgres\", password = \"admin\", host = \"localhost\", port = \"5432\")\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione tabella risorse lessicali anticipation e inserimento\n",
    "cur.execute('''CREATE TABLE RISORSEANTICIPATION\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    NOME             TEXT       NOT NULL,\n",
    "                    PAROLA           TEXT       NOT NULL\n",
    "                );\n",
    "            '''\n",
    ")\n",
    "nome = \"NRC_anticipation\"\n",
    "for word in resourceNRC_anticipation:\n",
    "    cur.execute(\"INSERT INTO RISORSEANTICIPATION (NOME, PAROLA) VALUES(%s, %s)\",(nome,word))\n",
    "\n",
    "nome = \"sentisense_anticipation\"\n",
    "for word in resourceSentisense_anticipation:\n",
    "    cur.execute(\"INSERT INTO RISORSEANTICIPATION (NOME, PAROLA) VALUES(%s, %s)\",(nome,word))\n",
    "\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione tabelle per le parole, emoji, emoticon e hashtag\n",
    "\n",
    "cur.execute('''CREATE TABLE PAROLEANTICIPATION\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    PAROLA           TEXT       NOT NULL, \n",
    "                    NRC_anticipation      BOOL       NOT NULL DEFAULT FALSE,\n",
    "                    sentisense_anticipation      BOOL       NOT NULL DEFAULT FALSE,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ")   \n",
    "\n",
    "cur.execute('''CREATE TABLE EMOJIANTICIPATION\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    EMOJI       TEXT       NOT NULL,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ") \n",
    "\n",
    "cur.execute('''CREATE TABLE EMOTICONSANTICIPATION\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    EMOTICONS       TEXT       NOT NULL,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ") \n",
    "\n",
    "cur.execute('''CREATE TABLE HASHTAGANTICIPATION\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    HASHTAG     TEXT       NOT NULL,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ") \n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inserimento hashtag, emoticon ed emoji\n",
    "\n",
    "for hashTag in hashtagAnticipation:\n",
    "    cur.execute(\"INSERT INTO HASHTAGANTICIPATION (HASHTAG,FREQUENCY) VALUES(%s,%s)\",(hashTag,0))\n",
    "\n",
    "for emoticons in emoticonAnticipation:\n",
    "    cur.execute(\"INSERT INTO EMOTICONSANTICIPATION (EMOTICONS,FREQUENCY) VALUES(%s,%s)\",(emoticons,0))\n",
    "\n",
    "for emojis in emojiAnticipation:\n",
    "    cur.execute(\"INSERT INTO EMOJIANTICIPATION (EMOJI,FREQUENCY) VALUES(%s,%s)\",(emojis,0))\n",
    "\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inserimento parole in db\n",
    "for word in anticipationListWord:\n",
    "    cur.execute(\"INSERT INTO PAROLEANTICIPATION (PAROLA,NRC_anticipation,sentisense_anticipation,FREQUENCY) VALUES(%s,%s,%s,%s)\",(word,False,False,0))\n",
    "\n",
    "conn.commit()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze hashtag\n",
    "cur.execute(\"update HASHTAGANTICIPATION as ha1 set frequency = (select count(*) from HASHTAGANTICIPATION ha2 where ha1.hashtag = ha2.hashtag);\")\n",
    "cur.execute(\"delete from HASHTAGANTICIPATION as ha1 where exists (Select 1 From HASHTAGANTICIPATION as ha2 where ha1.hashtag = ha2.hashtag and ha1.frequency = ha2.frequency and ha2.ctid > ha1.ctid);\")\n",
    "\n",
    "conn.commit()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze emoji\n",
    "cur.execute(\"update EMOJIANTICIPATION as ha1 set frequency = (select count(*) from EMOJIANTICIPATION ha2 where ha1.emoji = ha2.emoji);\")\n",
    "cur.execute(\"delete from EMOJIANTICIPATION as ha1 where exists (Select 1 From EMOJIANTICIPATION as ha2 where ha1.emoji = ha2.emoji and ha1.frequency = ha2.frequency and ha2.ctid > ha1.ctid);\")\n",
    "\n",
    "conn.commit()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze emoticons\n",
    "cur.execute(\"update EMOTICONSANTICIPATION as ha1 set frequency = (select count(*) from EMOTICONSANTICIPATION ha2 where ha1.emoticons = ha2.emoticons);\")\n",
    "cur.execute(\"delete from EMOTICONSANTICIPATION as ha1 where exists (Select 1 From EMOTICONSANTICIPATION as ha2 where ha1.emoticons = ha2.emoticons and ha1.frequency = ha2.frequency and ha2.ctid > ha1.ctid);\")\n",
    "\n",
    "conn.commit()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze parole\n",
    "cur.execute(\"select count(*),parola from PAROLEANTICIPATION group by parola\")\n",
    "result = []\n",
    "for word in cur.fetchall():\n",
    "    result.append(word) \n",
    "\n",
    "cur.execute(\"truncate table PAROLEANTICIPATION;\")\n",
    "#cur.execute(\"ALTER SEQUENCE PAROLEANTICIPATION RESTART WITH 1;\")\n",
    "\n",
    "for couple in result:\n",
    "    cur.execute(\"INSERT INTO PAROLEANTICIPATION (PAROLA,NRC_anticipation,sentisense_anticipation,FREQUENCY) VALUES(%s,%s,%s,%s)\",(couple[1],False,False,couple[0]))\n",
    "\n",
    "conn.commit()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggiornamento presenza parola nelle risorse lessicali\n",
    "cur.execute(\"UPDATE PAROLEANTICIPATION SET NRC_anticipation = TRUE WHERE parola IN (select p.parola from PAROLEANTICIPATION as p LEFT OUTER JOIN RISORSEANTICIPATION as r ON p.parola = r.parola WHERE r.nome = 'NRC_anticipation')\")\n",
    "cur.execute(\"UPDATE PAROLEANTICIPATION SET sentisense_anticipation = TRUE WHERE parola IN (select p.parola from PAROLEANTICIPATION as p LEFT OUTER JOIN RISORSEANTICIPATION as r ON p.parola = r.parola WHERE r.nome = 'sentisense_anticipation')\")\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvataggio in un txt delle parole non presenti in nessuna risorsa lessicale\n",
    "wordsMongo = anticipationCollections.find({\"LexResources\": []}, {\"_id\": 0, \"freq\": 0, \"LexResources\": 0})\n",
    "words = []\n",
    "\n",
    "for word in wordsMongo:\n",
    "    words.append(word['word'])\n",
    "\n",
    "with open('Nuove Parole/anticipation.txt', 'w',encoding=\"utf8\") as f:\n",
    "    for word in words:\n",
    "        f.write(\"%s\\n\" % word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trattamento dataset Disgust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lista di righe, ogni riga una tweet disgust\n",
    "file = open(\"Dataset/dataset_dt_disgust_60k.txt\", 'r',encoding=\"utf8\")\n",
    "disgust = []\n",
    "hashtagDisgust = []\n",
    "emoticonDisgust = []\n",
    "emojiDisgust = []\n",
    "\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "    riga = re.sub(r'URL', '',riga)\n",
    "    riga = re.sub(r'USERNAME', '',riga)\n",
    "\n",
    "    #array di hashtag singoli\n",
    "    hashTag = (re.findall('#[a-zA-Z0-9]+', riga))\n",
    "    \n",
    "    #generiamo l'array di hashtag senza hashtag\n",
    "    for h in hashTag:\n",
    "        hashtagDisgust.append(h.replace(\"#\",\"\"))\n",
    "\n",
    "    #Rimuoviamo glin hashtag dalla riga\n",
    "    riga = riga.replace('#',\"\")\n",
    "\n",
    "    disgust.append(riga)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(disgust)):\n",
    "    #Controllo se l'emoticons c'è e in caso la rimuovo\n",
    "    for singleEmoticon in emoticons:\n",
    "        emoticonsFind = re.findall(re.escape(singleEmoticon),disgust[i])\n",
    "        if(len(emoticonsFind) != 0):\n",
    "            emoticonDisgust.extend(emoticonsFind)\n",
    "            disgust[i] = disgust[i].replace(singleEmoticon,\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(disgust)):\n",
    "    #preleviamo le emoji e le rimuoviamo dal testo\n",
    "    data = regex.findall(r'\\X', disgust[i])\n",
    "    for word in data:\n",
    "        if any(char in emoji.UNICODE_EMOJI['en'] for char in word):\n",
    "            emojiDisgust.append(word)\n",
    "            disgust[i] = disgust[i].replace(word, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processamento slang prima di punteggiatura e lowercase\n",
    "with open(\"Slangs/slangs_before.json\") as f:\n",
    "    slangsBefore = json.load(f)\n",
    "    for i in range(len(disgust)):\n",
    "        for word in disgust[i].split():\n",
    "            indexOfWord = disgust[i].find(word)\n",
    "            if slangsBefore.get(word) is None:\n",
    "                if slangsBefore.get(word.lower()) is None:\n",
    "                    if slangsBefore.get(word.upper()) is None:\n",
    "                        continue\n",
    "                    else:\n",
    "                        disgust[i] = disgust[i][:indexOfWord] + slangsBefore.get(word.upper()) + disgust[i][indexOfWord+len(word):]\n",
    "                else:\n",
    "                    disgust[i] = disgust[i][:indexOfWord] + slangsBefore.get(word.lower()) + disgust[i][indexOfWord+len(word):]\n",
    "            else:\n",
    "                disgust[i] = disgust[i][:indexOfWord] + slangsBefore.get(word) + disgust[i][indexOfWord+len(word):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizzazione di tutte le frasi\n",
    "disgustTokenized = []\n",
    "for linea in disgust:\n",
    "    result = word_tokenize(linea)\n",
    "    disgustTokenized.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Esecuzione della POS tagging per ogni riga della matrice\n",
    "for i in range(len(disgustTokenized)):\n",
    "    disgustTokenized[i] = nltk.pos_tag(disgustTokenized[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trattamento punteggiatura\n",
    "for line in disgustTokenized:\n",
    "    newLine = line\n",
    "    listToRemove = []\n",
    "    for i in range(len(line)):\n",
    "        if line[i][0] in puntuaction:\n",
    "            listToRemove.append(line[i])\n",
    "    for element in listToRemove:\n",
    "        newLine.remove(element)\n",
    "    line = newLine\n",
    "\n",
    "linePuntuaction = re.compile(\"[\" + re.escape(\"\".join(puntuaction)) + \"]\")\n",
    "for line in disgustTokenized:\n",
    "    for i in range(len(line)):\n",
    "        tupla = [(re.sub(linePuntuaction, \"\", line[i][0])),line[i][1]]\n",
    "        line[i] = tupla\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming\n",
    "#ps = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for line in disgustTokenized:\n",
    "    for i in range(len(line)):\n",
    "        tupla = [lemmatizer.lemmatize(line[i][0]),line[i][1]]\n",
    "        line[i] = tupla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trattamnento Stop word dopo Stemming\n",
    "stop_words = set(stopwords.words('english'))\n",
    "disgustTokStoped = []\n",
    "for line in disgustTokenized:\n",
    "    filtred_word = []\n",
    "    for row in line:\n",
    "        if  row[0] not in stop_words:\n",
    "            filtred_word.append(row) \n",
    "    disgustTokStoped.append(filtred_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disgustListWord = []\n",
    "disgustListTok = []\n",
    "for line in disgustTokStoped:\n",
    "    for row in line:\n",
    "        disgustListWord.append(row[0])\n",
    "        disgustListTok.append(row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lavorazione risorse lessicali\n",
    "resourceDisgustSentisense_disgust = []\n",
    "resourceDisgustNRC_disgust = []\n",
    "resourceDisgustSentisense_hate = []\n",
    "#Lista di righe, ogni riga una tweet disgust\n",
    "file = open(\"Risorse lessicali\\Disgust-Hate\\sentisense_disgust.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        resourceDisgustSentisense_disgust.append(riga)\n",
    "\n",
    "file.close()\n",
    "\n",
    "file = open(\"C:Risorse lessicali\\\\Disgust-Hate\\\\NRC_disgust.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        resourceDisgustNRC_disgust.append(riga)\n",
    "\n",
    "file.close()\n",
    "\n",
    "file = open(\"Risorse lessicali\\Disgust-Hate\\sentisense_hate.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        resourceDisgustSentisense_hate.append(riga)\n",
    "\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Caricare le risorse lessicali \n",
    "\n",
    "DisgustSentisense = {\n",
    "    '_id': 'Sentisense_disgust',\n",
    "    'sentiment': 'disgust', \n",
    "    'wordList': resourceDisgustSentisense_disgust\n",
    "}\n",
    "\n",
    "DisgustNRC_disgust = {\n",
    "    '_id': 'NRC_disgust',\n",
    "    'sentiment': 'disgust',  \n",
    "    'wordList': resourceDisgustNRC_disgust\n",
    "}\n",
    "\n",
    "DisgustSentisense_hate = {\n",
    "    '_id': 'Sentisense_hatesense',\n",
    "    'sentiment': 'disgust',\n",
    "    'wordList': resourceDisgustSentisense_hate\n",
    "}\n",
    "\n",
    "resourcesCollection.insert_one(DisgustSentisense)\n",
    "resourcesCollection.insert_one(DisgustNRC_disgust)\n",
    "resourcesCollection.insert_one(DisgustSentisense_hate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conteggio delle frequenze delle parole\n",
    "countWordDisgust = Counter(disgustListWord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione database e collezioni\n",
    "# Inserimento singoli documenti contententi i tweet disgust\n",
    "wordsString = {}\n",
    "\n",
    "for i in range(len(disgustListWord)):\n",
    "    wordsString = {\n",
    "        \"lemma\": disgustListWord[i],\n",
    "        \"POS\": disgustListTok[i]\n",
    "    }\n",
    "    disgustTwit={\"word\":wordsString}\n",
    "    disgustCollections.insert_one(disgustTwit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inserimento collezione Twitter con hashtags, emoticons ed emoji \n",
    "disgustInsert = {\n",
    "    \"sentiment\":\"disgust\",\n",
    "    \"hashtags\": hashtagDisgust,\n",
    "    \"emoticons\": emoticonDisgust,\n",
    "    \"emoji\": emojiDisgust\n",
    "}\n",
    "\n",
    "#inserimento\n",
    "collectionDisgust.insert_one(disgustInsert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raggruppa i documenti di input in base all'espressione _id specificata e per ciascun raggruppamento distinto genera un documento\n",
    "pipeline = [\n",
    "    {\"$group\": {\"_id\": \"$word.lemma\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"word\": \"$_id\",\n",
    "      \"freq\": \"$count\"\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"freq\": -1\n",
    "    }},\n",
    "    {\"$out\": \"disgust\"}\n",
    "]\n",
    "\n",
    "#post processing per renderlo valido \n",
    "disgustCollections.aggregate(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creazione Word Clouds per le parole\n",
    "data = {}\n",
    "for document in disgustCollections.find().limit(50):\n",
    "    data[document['word']] = document['freq']\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2)\n",
    "wc.fit_words(data)\n",
    "wc.to_file('Word Clouds/disgustWords.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conteggio degli hashtags\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$hashtags\"},\n",
    "    {\"$group\": {\"_id\": \"$hashtags\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"word\": \"$_id\",\n",
    "      \"count\": 1\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"count\": -1\n",
    "    }}\n",
    "]\n",
    "\n",
    "hashtagsCount = collectionDisgust.aggregate(pipeline)\n",
    "\n",
    "data = {}\n",
    "for document in hashtagsCount:\n",
    "    data[document['word']] = document['count']\n",
    "\n",
    "collectionDisgust.update_one(\n",
    "  {\"sentiment\": \"disgust\"},\n",
    "  {\"$set\": {\"hashtags\": data}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creazione Word Clouds per gli hashtags\n",
    "\n",
    "hashtags = collectionDisgust.distinct(\"hashtags\")\n",
    "\n",
    "hashtagsLimited = {}\n",
    "for (word,count) in [x for x in hashtags[0].items()][:50]:\n",
    "    hashtagsLimited[word] = count\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2)\n",
    "wc.fit_words(hashtagsLimited)\n",
    "wc.to_file('Word Clouds/disgustHashtags.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conteggio delle emoji\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$emoji\"},\n",
    "    {\"$group\": {\"_id\": \"$emoji\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"emoji\": \"$_id\",\n",
    "      \"count\": 1\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"count\": -1\n",
    "    }}\n",
    "]\n",
    "\n",
    "emojiCount = collectionDisgust.aggregate(pipeline)\n",
    "\n",
    "data = {}\n",
    "for document in emojiCount:\n",
    "    data[document['emoji']] = document['count']\n",
    "\n",
    "collectionDisgust.update_one(\n",
    "  {\"sentiment\": \"disgust\"},\n",
    "  {\"$set\": {\"emoji\": data}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creazione Word Clouds per le emoji\n",
    "\n",
    "emojis = collectionDisgust.distinct(\"emoji\")\n",
    "\n",
    "emojisLimited = {}\n",
    "for (word,count) in [x for x in emojis[0].items()][:50]:\n",
    "    emojisLimited[word] = count\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2, font_path='Font Emoji\\Symbola.otf')\n",
    "wc.fit_words(emojisLimited)\n",
    "wc.to_file('Word Clouds/disgustEmojis.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conteggio degli emoticons\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$emoticons\"},\n",
    "    {\"$group\": {\"_id\": \"$emoticons\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"emoticons\": \"$_id\",\n",
    "      \"count\": 1\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"count\": -1\n",
    "    }}\n",
    "]\n",
    "\n",
    "emoticonsCount = collectionDisgust.aggregate(pipeline)\n",
    "\n",
    "data = {}\n",
    "for document in emoticonsCount:\n",
    "  data = document['emoticons']\n",
    "\n",
    "collectionDisgust.update_one(\n",
    "  {\"sentiment\": \"disgust\"},\n",
    "  {\"$set\": {\"emoticons\": data}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creazione Word Clouds per le emoticons\n",
    "\n",
    "emoticons = collectionDisgust.distinct(\"emoticons\")\n",
    "\n",
    "emoticonsLimited = {}\n",
    "for (word,count) in [x for x in emoticons[0].items()][:50]:\n",
    "    emoticonsLimited[word] = count\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2)\n",
    "wc.fit_words(emoticonsLimited)\n",
    "wc.to_file('Word Clouds/disgustEmoticons.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline di aggregazione per inserire la risorsa lessicale a cui appartiene la parola\n",
    "\n",
    "pipeline = [\n",
    "    {\"$lookup\": {\n",
    "        \"from\": \"Resources\",\n",
    "        \"localField\": \"word\",\n",
    "        \"foreignField\": \"wordList\",\n",
    "        \"pipeline\": [\n",
    "            {\"$match\": {\"$expr\": {\"$regexMatch\": {\"input\": \"$_id\", \"regex\": \"disgust\"}}}}\n",
    "        ],\n",
    "        \"as\": \"LexResources\"\n",
    "    }},\n",
    "    {\"$project\": {\n",
    "        \"resources.wordList\": 0,\n",
    "        \"resources.totNumberWords\": 0,\n",
    "        \"resources.sentiment\": 0,\n",
    "        \"LexResources.sentiment\": 0,\n",
    "        \"LexResources.totNumberWords\": 0,\n",
    "        \"LexResources.wordList\": 0\n",
    "    }},\n",
    "    {\"$out\": \"disgust\"}\n",
    "]\n",
    "\n",
    "disgustCollections.aggregate(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline di aggregazione per calcolare le percentuali richieste\n",
    "\n",
    "pipelineCount = [\n",
    "    {\"$group\": {\"_id\": \"null\", \"count\": { \"$sum\": \"$freq\" }}},\n",
    "    {\"$project\": {\n",
    "        \"_id\": 0,\n",
    "        \"count\": \"$count\"\n",
    "    }}\n",
    "]\n",
    "\n",
    "results = disgustCollections.aggregate(pipelineCount)\n",
    "\n",
    "countWords = 0\n",
    "for data in results:\n",
    "    countWords = data['count']\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$LexResources\"},\n",
    "    {\"$lookup\": {\n",
    "        \"from\": \"Resources\",\n",
    "        \"localField\": \"LexResources._id\",\n",
    "        \"foreignField\": \"_id\",\n",
    "        \"as\": \"RefResource\"\n",
    "    }},\n",
    "    {\"$group\": {\"_id\": { \"lex\": \"$LexResources._id\", \"RefResource\": \"$RefResource\" }, \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "        \"_id\": \"$_id.lex\",\n",
    "        \"wordList\": \"$_id.RefResource.wordList\",\n",
    "        \"commonWords\": \"$count\"\n",
    "    }},\n",
    "    {\"$unwind\": \"$wordList\"},\n",
    "    {\"$project\": {\n",
    "        \"_id\": 1,\n",
    "        \"commonWords\": 1,\n",
    "        \"totWordsLex\": {\"$size\": \"$wordList\"}\n",
    "    }},\n",
    "    {\"$addFields\": {\"totalWordsTwitter\": countWords}},\n",
    "    {\"$project\": {\n",
    "        \"perc_presence_lex_res\": { \"$round\": [{\"$divide\": [ \"$commonWords\", \"$totWordsLex\" ] }, 6] },\n",
    "        \"perc_presence_twitter\": { \"$round\": [{\"$divide\": [ \"$commonWords\", \"$totalWordsTwitter\" ]}, 6] }\n",
    "    }}\n",
    "]\n",
    "\n",
    "results = disgustCollections.aggregate(pipeline)\n",
    "\n",
    "# Genero i dati per l'istogramma\n",
    "istogramDisgust_lexRes = []\n",
    "istogramDisgust_twitter = []\n",
    "\n",
    "for data in results:\n",
    "    print(data)\n",
    "    log =math.log(data['perc_presence_lex_res'],10)\n",
    "    istogramDisgust_lexRes.append([data['_id'], data['perc_presence_lex_res']])\n",
    "    log =math.log(data['perc_presence_twitter'],10)\n",
    "    istogramDisgust_twitter.append([data['_id'], data['perc_presence_twitter']])\n",
    " \n",
    "# Create the pandas DataFrame\n",
    "dfDisgust_lexRes = pd.DataFrame(istogramDisgust_lexRes, columns = ['Risorsa', 'Percentuale'])\n",
    "dfDisgust_twitter = pd.DataFrame(istogramDisgust_twitter,columns = ['Risorsa', 'Percentuale'])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione istogramma per presenza parole nelle risorse lessicali\n",
    "\n",
    "dfDisgust_lexRes.plot.bar(x='Risorsa')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione istogramma per presenza parole nei twitter\n",
    "\n",
    "dfDisgust_twitter.plot.bar(x='Risorsa')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connessione, creazione cursore e database\n",
    "\n",
    "conn = psycopg2.connect(database = \"maadb\", user = \"postgres\", password = \"admin\", host = \"localhost\", port = \"5432\")\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione tabella risorse lessicali disgust e inserimento\n",
    "cur.execute('''CREATE TABLE RISORSEDISGUST\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    NOME             TEXT       NOT NULL,\n",
    "                    PAROLA           TEXT       NOT NULL\n",
    "                );\n",
    "            '''\n",
    ")\n",
    "nome = \"NRC_disgust\"\n",
    "for word in resourceDisgustNRC_disgust:\n",
    "    cur.execute(\"INSERT INTO RISORSEDISGUST (NOME, PAROLA) VALUES(%s, %s)\",(nome,word))\n",
    "\n",
    "nome = \"sentisense_disgust\"\n",
    "for word in resourceDisgustSentisense_disgust:\n",
    "    cur.execute(\"INSERT INTO RISORSEDISGUST (NOME, PAROLA) VALUES(%s, %s)\",(nome,word))\n",
    "\n",
    "nome = \"sentisense_hate\"\n",
    "for word in resourceDisgustSentisense_hate:\n",
    "    cur.execute(\"INSERT INTO RISORSEDISGUST (NOME, PAROLA) VALUES(%s, %s)\",(nome,word))\n",
    "\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione tabelle per le parole, emoji, emoticon e hashtag\n",
    "\n",
    "cur.execute('''CREATE TABLE PAROLEDISGUST\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    PAROLA           TEXT       NOT NULL, \n",
    "                    NRC_disgust      BOOL       NOT NULL DEFAULT FALSE,\n",
    "                    sentisense_disgust      BOOL       NOT NULL DEFAULT FALSE,\n",
    "                    sentisense_hate      BOOL       NOT NULL DEFAULT FALSE,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ")   \n",
    "\n",
    "cur.execute('''CREATE TABLE EMOJIDISGUST\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    EMOJI       TEXT       NOT NULL,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ") \n",
    "\n",
    "cur.execute('''CREATE TABLE EMOTICONSDISGUST\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    EMOTICONS       TEXT       NOT NULL,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ") \n",
    "\n",
    "cur.execute('''CREATE TABLE HASHTAGDISGUST\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    HASHTAG     TEXT       NOT NULL,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ") \n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inserimento hashtag, emoticon ed emoji\n",
    "\n",
    "for hashTag in hashtagDisgust:\n",
    "    cur.execute(\"INSERT INTO HASHTAGDISGUST (HASHTAG,FREQUENCY) VALUES(%s,%s)\",(hashTag,0))\n",
    "\n",
    "for emoticons in emoticonDisgust:\n",
    "    cur.execute(\"INSERT INTO EMOTICONSDISGUST (EMOTICONS,FREQUENCY) VALUES(%s,%s)\",(emoticons,0))\n",
    "\n",
    "for emojis in emojiDisgust:\n",
    "    cur.execute(\"INSERT INTO EMOJIDISGUST (EMOJI,FREQUENCY) VALUES(%s,%s)\",(emojis,0))\n",
    "\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inserimento parole in db\n",
    "for word in disgustListWord:\n",
    "    cur.execute(\"INSERT INTO PAROLEDISGUST (PAROLA,NRC_disgust,sentisense_disgust,sentisense_hate,FREQUENCY) VALUES(%s,%s,%s,%s,%s)\",(word,False,False,False,0))\n",
    "\n",
    "conn.commit()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze hashtag\n",
    "cur.execute(\"update HASHTAGDISGUST as ha1 set frequency = (select count(*) from HASHTAGDISGUST ha2 where ha1.hashtag = ha2.hashtag);\")\n",
    "cur.execute(\"delete from HASHTAGDISGUST as ha1 where exists (Select 1 From HASHTAGDISGUST as ha2 where ha1.hashtag = ha2.hashtag and ha1.frequency = ha2.frequency and ha2.ctid > ha1.ctid);\")\n",
    "\n",
    "conn.commit()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze emoji\n",
    "cur.execute(\"update EMOJIDISGUST as ha1 set frequency = (select count(*) from EMOJIDISGUST ha2 where ha1.emoji = ha2.emoji);\")\n",
    "cur.execute(\"delete from EMOJIDISGUST as ha1 where exists (Select 1 From EMOJIDISGUST as ha2 where ha1.emoji = ha2.emoji and ha1.frequency = ha2.frequency and ha2.ctid > ha1.ctid);\")\n",
    "\n",
    "conn.commit()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze emoticons\n",
    "cur.execute(\"update EMOTICONSDISGUST as ha1 set frequency = (select count(*) from EMOTICONSDISGUST ha2 where ha1.emoticons = ha2.emoticons);\")\n",
    "cur.execute(\"delete from EMOTICONSDISGUST as ha1 where exists (Select 1 From EMOTICONSDISGUST as ha2 where ha1.emoticons = ha2.emoticons and ha1.frequency = ha2.frequency and ha2.ctid > ha1.ctid);\")\n",
    "\n",
    "conn.commit()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze parole\n",
    "cur.execute(\"select count(*),parola from PAROLEDISGUST group by parola\")\n",
    "result = []\n",
    "for word in cur.fetchall():\n",
    "    result.append(word) \n",
    "\n",
    "cur.execute(\"truncate table PAROLEDISGUST;\")\n",
    "#cur.execute(\"ALTER SEQUENCE parolefear_id_seq RESTART WITH 1;\")\n",
    "\n",
    "for couple in result:\n",
    "    cur.execute(\"INSERT INTO PAROLEDISGUST (PAROLA,NRC_disgust,sentisense_disgust,sentisense_hate,FREQUENCY) VALUES(%s,%s,%s,%s,%s)\",(couple[1],False,False,False,couple[0]))\n",
    "\n",
    "conn.commit()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggiornamento presenza parola nelle risorse lessicali\n",
    "cur.execute(\"UPDATE PAROLEDISGUST SET NRC_disgust = TRUE WHERE parola IN (select p.parola from PAROLEDISGUST as p LEFT OUTER JOIN RISORSEDISGUST as r ON p.parola = r.parola WHERE r.nome = 'NRC_disgust')\")\n",
    "cur.execute(\"UPDATE PAROLEDISGUST SET sentisense_disgust = TRUE WHERE parola IN (select p.parola from PAROLEDISGUST as p LEFT OUTER JOIN RISORSEDISGUST as r ON p.parola = r.parola WHERE r.nome = 'sentisense_disgust')\")\n",
    "cur.execute(\"UPDATE PAROLEDISGUST SET sentisense_hate = TRUE WHERE parola IN (select p.parola from PAROLEDISGUST as p LEFT OUTER JOIN RISORSEDISGUST as r ON p.parola = r.parola WHERE r.nome = 'sentisense_hate')\")\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvataggio in un txt delle parole non presenti in nessuna risorsa lessicale\n",
    "\n",
    "wordsMongo = disgustCollections.find({\"LexResources\": []}, {\"_id\": 0, \"freq\": 0, \"LexResources\": 0})\n",
    "words = []\n",
    "\n",
    "for word in wordsMongo:\n",
    "    words.append(word['word'])\n",
    "\n",
    "with open('Nuove Parole/disgust.txt', 'w',encoding=\"utf8\") as f:\n",
    "    for word in words:\n",
    "        f.write(\"%s\\n\" % word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trattamento dataset Fear "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lista di righe, ogni riga una tweet fear\n",
    "file = open(\"Dataset/dataset_dt_fear_60k.txt\", 'r',encoding=\"utf8\")\n",
    "fear = []\n",
    "hashtagFear = []\n",
    "emoticonFear = []\n",
    "emojiFear = []\n",
    "\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "    riga = re.sub(r'URL', '',riga)\n",
    "    riga = re.sub(r'USERNAME', '',riga)\n",
    "\n",
    "     #array di hashtag singoli\n",
    "    hashTag = (re.findall('#[a-zA-Z0-9]+', riga))\n",
    "\n",
    "    #generiamo l'array di hashtag senza hashtag\n",
    "    for h in hashTag:\n",
    "        hashtagFear.append(h.replace(\"#\",\"\"))\n",
    "\n",
    "    #Rimuoviamo glin hashtag dalla riga\n",
    "    riga = riga.replace('#',\"\")\n",
    "\n",
    "    fear.append(riga)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(fear)):\n",
    "    #Controllo se l'emoticons c'è e in caso la rimuovo\n",
    "    for singleEmoticon in emoticons:\n",
    "        emoticonsFind = re.findall(re.escape(singleEmoticon),fear[i])\n",
    "        if(len(emoticonsFind) != 0):\n",
    "            emoticonFear.extend(emoticonsFind)\n",
    "            fear[i] = fear[i].replace(singleEmoticon,\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(fear)):\n",
    "    #preleviamo le emoji e le rimuoviamo dal testo\n",
    "    data = regex.findall(r'\\X', fear[i])\n",
    "    for word in data:\n",
    "        if any(char in emoji.UNICODE_EMOJI['en'] for char in word):\n",
    "            emojiFear.append(word)\n",
    "            fear[i] = fear[i].replace(word, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processamento slang prima di punteggiatura e lowercase\n",
    "with open(\"Slangs/slangs_before.json\") as f:\n",
    "    slangsBefore = json.load(f)\n",
    "    for i in range(len(fear)):\n",
    "        for word in fear[i].split():\n",
    "            indexOfWord = fear[i].find(word)\n",
    "            if slangsBefore.get(word) is None:\n",
    "                if slangsBefore.get(word.lower()) is None:\n",
    "                    if slangsBefore.get(word.upper()) is None:\n",
    "                        continue\n",
    "                    else:\n",
    "                        fear[i] = fear[i][:indexOfWord] + slangsBefore.get(word.upper()) + fear[i][indexOfWord+len(word):]\n",
    "                else:\n",
    "                    fear[i] = fear[i][:indexOfWord] + slangsBefore.get(word.lower()) + fear[i][indexOfWord+len(word):]\n",
    "            else:\n",
    "                fear[i] = fear[i][:indexOfWord] + slangsBefore.get(word) + fear[i][indexOfWord+len(word):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizzazione di tutte le frasi\n",
    "fearTokenized = []\n",
    "for linea in fear:\n",
    "    result = word_tokenize(linea)\n",
    "    fearTokenized.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Esecuzione della POS tagging per ogni riga della matrice\n",
    "for i in range(len(fearTokenized)):\n",
    "    fearTokenized[i] = nltk.pos_tag(fearTokenized[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trattamento punteggiatura\n",
    "for line in fearTokenized:\n",
    "    newLine = line\n",
    "    listToRemove = []\n",
    "    for i in range(len(line)):\n",
    "        if line[i][0] in puntuaction:\n",
    "            listToRemove.append(line[i])\n",
    "    for element in listToRemove:\n",
    "        newLine.remove(element)\n",
    "    line = newLine\n",
    "\n",
    "linePuntuaction = re.compile(\"[\" + re.escape(\"\".join(puntuaction)) + \"]\")\n",
    "for line in fearTokenized:\n",
    "    for i in range(len(line)):\n",
    "        tupla = [(re.sub(linePuntuaction, \"\", line[i][0])),line[i][1]]\n",
    "        line[i] = tupla\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming\n",
    "#ps = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for line in fearTokenized:\n",
    "    for i in range(len(line)):\n",
    "        tupla = [lemmatizer.lemmatize(line[i][0]),line[i][1]]\n",
    "        line[i] = tupla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trattamnento Stop word dopo Stemming\n",
    "stop_words = set(stopwords.words('english'))\n",
    "fearTokStoped = []\n",
    "for line in fearTokenized:\n",
    "    filtred_word = []\n",
    "    for row in line:\n",
    "        if  row[0] not in stop_words:\n",
    "            filtred_word.append(row) \n",
    "    fearTokStoped.append(filtred_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fearListWord = []\n",
    "fearListTok = []\n",
    "for line in fearTokStoped:\n",
    "    for row in line:\n",
    "        fearListWord.append(row[0])\n",
    "        fearListTok.append(row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lavorazione risorse lessicali\n",
    "resourceFearNRC = []\n",
    "resourceFearSentisense = []\n",
    "#Lista di righe, ogni riga una tweet fear\n",
    "file = open(\"Risorse lessicali\\\\Fear\\\\NRC_fear.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        resourceFearNRC.append(riga)\n",
    "\n",
    "file.close()\n",
    "\n",
    "file = open(\"Risorse lessicali\\Fear\\sentisense_fear.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        resourceFearSentisense.append(riga)\n",
    "\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Caricare le risorse lessicali \n",
    "FearNRC = {\n",
    "    '_id': 'NRC_fear',\n",
    "    'sentiment': 'fear',  \n",
    "    'wordList': resourceFearNRC\n",
    "}\n",
    "\n",
    "FearSentisense = {\n",
    "    '_id': 'sentisense_fear',\n",
    "    'sentiment': 'fear',\n",
    "    'wordList': resourceFearSentisense\n",
    "}\n",
    "\n",
    "resourcesCollection.insert_one(FearNRC)\n",
    "resourcesCollection.insert_one(FearSentisense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conteggio delle frequenze delle parole\n",
    "countWordFear = Counter(fearListWord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione database e collezioni\n",
    "# Inserimento singoli documenti contententi i tweet fear\n",
    "wordsString = {}\n",
    "\n",
    "for i in range(len(fearListWord)):\n",
    "    wordsString = {\n",
    "        \"lemma\": fearListWord[i],\n",
    "        \"POS\": fearListTok[i]\n",
    "    }\n",
    "    fearTwit={\"word\":wordsString}\n",
    "    fearCollections.insert_one(fearTwit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inserimento collezione Twitter con hashtags, emoticons ed emoji \n",
    "fearInsert = {\n",
    "    \"sentiment\":\"fear\",\n",
    "    \"hashtags\": hashtagFear,\n",
    "    \"emoticons\": emoticonFear,\n",
    "    \"emoji\": emojiFear\n",
    "}\n",
    "\n",
    "#inserimento\n",
    "x = collectionFear.insert_one(fearInsert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raggruppa i documenti di input in base all'espressione _id specificata e per ciascun raggruppamento distinto genera un documento\n",
    "pipeline = [\n",
    "    {\"$group\": {\"_id\": \"$word.lemma\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"word\": \"$_id\",\n",
    "      \"freq\": \"$count\"\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"freq\": -1\n",
    "    }},\n",
    "    {\"$out\": \"fear\"}\n",
    "]\n",
    "\n",
    "#post processing per renderlo valido \n",
    "fearCollections.aggregate(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creazione Word Clouds per le parole\n",
    "data = {}\n",
    "for document in fearCollections.find().limit(50):\n",
    "    data[document['word']] = document['freq']\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2)\n",
    "wc.fit_words(data)\n",
    "wc.to_file('Word Clouds/fearWords.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conteggio degli hashtags\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$hashtags\"},\n",
    "    {\"$group\": {\"_id\": \"$hashtags\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"word\": \"$_id\",\n",
    "      \"count\": 1\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"count\": -1\n",
    "    }}\n",
    "]\n",
    "\n",
    "hashtagsCount = collectionFear.aggregate(pipeline)\n",
    "\n",
    "data = {}\n",
    "for document in hashtagsCount:\n",
    "    data[document['word']] = document['count']\n",
    "\n",
    "collectionFear.update_one(\n",
    "  {\"sentiment\": \"fear\"},\n",
    "  {\"$set\": {\"hashtags\": data}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creazione Word Clouds per gli hashtags\n",
    "\n",
    "hashtags = collectionFear.distinct(\"hashtags\")\n",
    "\n",
    "hashtagsLimited = {}\n",
    "for (word,count) in [x for x in hashtags[0].items()][:50]:\n",
    "    hashtagsLimited[word] = count\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2)\n",
    "wc.fit_words(hashtagsLimited)\n",
    "wc.to_file('Word Clouds/fearHashtags.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conteggio delle emoji\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$emoji\"},\n",
    "    {\"$group\": {\"_id\": \"$emoji\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"emoji\": \"$_id\",\n",
    "      \"count\": 1\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"count\": -1\n",
    "    }}\n",
    "]\n",
    "\n",
    "emojiCount = collectionFear.aggregate(pipeline)\n",
    "\n",
    "data = {}\n",
    "for document in emojiCount:\n",
    "    data[document['emoji']] = document['count']\n",
    "\n",
    "collectionFear.update_one(\n",
    "  {\"sentiment\": \"fear\"},\n",
    "  {\"$set\": {\"emoji\": data}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conteggio degli emoticons\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$emoticons\"},\n",
    "    {\"$group\": {\"_id\": \"$emoticons\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"emoticons\": \"$_id\",\n",
    "      \"count\": 1\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"count\": -1\n",
    "    }}\n",
    "]\n",
    "\n",
    "emoticonsCount = collectionFear.aggregate(pipeline)\n",
    "\n",
    "data = {}\n",
    "for document in emoticonsCount:\n",
    "    data[document['emoticons']] = document['count']\n",
    "\n",
    "collectionFear.update_one(\n",
    "  {\"sentiment\": \"fear\"},\n",
    "  {\"$set\": {\"emoticons\": data}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creazione Word Clouds per le emoticons\n",
    "\n",
    "emoticons = collectionFear.distinct(\"emoticons\")\n",
    "\n",
    "emoticonsLimited = {}\n",
    "for (word,count) in [x for x in emoticons[0].items()][:50]:\n",
    "    emoticonsLimited[word] = count\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2)\n",
    "wc.fit_words(emoticonsLimited)\n",
    "wc.to_file('Word Clouds/joyEmoticons.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline di aggregazione per inserire la risorsa lessicale a cui appartiene la parola\n",
    "pipeline = [\n",
    "    {\"$lookup\": {\n",
    "        \"from\": \"Resources\",\n",
    "        \"localField\": \"word\",\n",
    "        \"foreignField\": \"wordList\",\n",
    "        \"pipeline\": [\n",
    "            {\"$match\": {\"$expr\": {\"$regexMatch\": {\"input\": \"$_id\", \"regex\": \"fear\"}}}}\n",
    "        ],\n",
    "        \"as\": \"LexResources\"\n",
    "    }},\n",
    "    {\"$project\": {\n",
    "        \"resources.wordList\": 0,\n",
    "        \"resources.totNumberWords\": 0,\n",
    "        \"resources.sentiment\": 0,\n",
    "        \"LexResources.sentiment\": 0,\n",
    "        \"LexResources.totNumberWords\": 0,\n",
    "        \"LexResources.wordList\": 0\n",
    "    }},\n",
    "    {\"$out\": \"fear\"}\n",
    "]\n",
    "\n",
    "fearCollections.aggregate(pipeline)\n",
    "\n",
    "#Pipeline di aggregazione per calcolare le percentuali richieste\n",
    "\n",
    "pipelineCount = [\n",
    "    {\"$group\": {\"_id\": \"null\", \"count\": { \"$sum\": \"$freq\" }}},\n",
    "    {\"$project\": {\n",
    "        \"_id\": 0,\n",
    "        \"count\": \"$count\"\n",
    "    }}\n",
    "]\n",
    "\n",
    "results = fearCollections.aggregate(pipelineCount)\n",
    "\n",
    "countWords = 0\n",
    "for data in results:\n",
    "    countWords = data['count']\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$LexResources\"},\n",
    "    {\"$lookup\": {\n",
    "        \"from\": \"Resources\",\n",
    "        \"localField\": \"LexResources._id\",\n",
    "        \"foreignField\": \"_id\",\n",
    "        \"as\": \"RefResource\"\n",
    "    }},\n",
    "    {\"$group\": {\"_id\": { \"lex\": \"$LexResources._id\", \"RefResource\": \"$RefResource\" }, \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "        \"_id\": \"$_id.lex\",\n",
    "        \"wordList\": \"$_id.RefResource.wordList\",\n",
    "        \"commonWords\": \"$count\"\n",
    "    }},\n",
    "    {\"$unwind\": \"$wordList\"},\n",
    "    {\"$project\": {\n",
    "        \"_id\": 1,\n",
    "        \"commonWords\": 1,\n",
    "        \"totWordsLex\": {\"$size\": \"$wordList\"}\n",
    "    }},\n",
    "    {\"$addFields\": {\"totalWordsTwitter\": countWords}},\n",
    "    {\"$project\": {\n",
    "        \"perc_presence_lex_res\": { \"$round\": [{\"$divide\": [ \"$commonWords\", \"$totWordsLex\" ] }, 6] },\n",
    "        \"perc_presence_twitter\": { \"$round\": [{\"$divide\": [ \"$commonWords\", \"$totalWordsTwitter\" ]}, 6] }\n",
    "    }}\n",
    "]\n",
    "\n",
    "results = fearCollections.aggregate(pipeline)\n",
    "\n",
    "# Genero i dati per l'istogramma\n",
    "istogramFear_lexRes = []\n",
    "istogramFear_twitter = []\n",
    "\n",
    "for data in results:\n",
    "    print(data)\n",
    "    log =math.log(data['perc_presence_lex_res'],10)\n",
    "    istogramFear_lexRes.append([data['_id'], data['perc_presence_lex_res']])\n",
    "    log =math.log(data['perc_presence_twitter'],10)\n",
    "    istogramFear_twitter.append([data['_id'], data['perc_presence_twitter']])\n",
    "\n",
    "# Create the pandas DataFrame\n",
    "dfFear_lexRes = pd.DataFrame(istogramFear_lexRes, columns = ['Risorsa', 'Percentuale'])\n",
    "dfFear_twitter = pd.DataFrame(istogramFear_twitter,columns = ['Risorsa', 'Percentuale'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione istogramma per presenza parole nei twitter\n",
    "\n",
    "dfFear_lexRes.plot.bar(x='Risorsa')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione istogramma per presenza parole nei twitter\n",
    "\n",
    "dfFear_twitter.plot.bar(x='Risorsa')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connessione, creazione cursore e database\n",
    "conn = psycopg2.connect(database = \"maadb\", user = \"postgres\", password = \"admin\", host = \"localhost\", port = \"5432\")\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione tabella risorse lessicali fear e inserimento\n",
    "cur.execute('''CREATE TABLE RISORSEFEAR\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    NOME             TEXT       NOT NULL,\n",
    "                    PAROLA           TEXT       NOT NULL\n",
    "                );\n",
    "            '''\n",
    ")\n",
    "nome = \"NRC_fear\"\n",
    "for word in resourceFearNRC:\n",
    "    cur.execute(\"INSERT INTO RISORSEFEAR (NOME, PAROLA) VALUES(%s, %s)\",(nome,word))\n",
    "\n",
    "nome = \"sentisense_fear\"\n",
    "for word in resourceFearSentisense:\n",
    "    cur.execute(\"INSERT INTO RISORSEFEAR (NOME, PAROLA) VALUES(%s, %s)\",(nome,word))\n",
    "\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione tabelle per le parole, emoji, emoticon e hashtag\n",
    "\n",
    "cur.execute('''CREATE TABLE PAROLEFEAR\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    PAROLA           TEXT       NOT NULL, \n",
    "                    NRC_fear      BOOL       NOT NULL DEFAULT FALSE,\n",
    "                    sentisense_fear      BOOL       NOT NULL DEFAULT FALSE,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ")   \n",
    "\n",
    "cur.execute('''CREATE TABLE EMOJIFEAR\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    EMOJI       TEXT       NOT NULL,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ") \n",
    "\n",
    "cur.execute('''CREATE TABLE EMOTICONSFEAR\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    EMOTICONS       TEXT       NOT NULL,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ") \n",
    "\n",
    "cur.execute('''CREATE TABLE HASHTAGFEAR\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    HASHTAG     TEXT       NOT NULL,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ") \n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inserimento hashtag, emoticon ed emoji\n",
    "\n",
    "for hashTag in hashtagFear:\n",
    "    cur.execute(\"INSERT INTO HASHTAGFEAR (HASHTAG,FREQUENCY) VALUES(%s,%s)\",(hashTag,0))\n",
    "\n",
    "for emoticons in emoticonFear:\n",
    "    cur.execute(\"INSERT INTO EMOTICONSFEAR (EMOTICONS,FREQUENCY) VALUES(%s,%s)\",(emoticons,0))\n",
    "\n",
    "for emojis in emojiFear:\n",
    "    cur.execute(\"INSERT INTO EMOJIFEAR (EMOJI,FREQUENCY) VALUES(%s,%s)\",(emojis,0))\n",
    "\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inserimento parole in db\n",
    "for word in fearListWord:\n",
    "    cur.execute(\"INSERT INTO PAROLEFEAR (PAROLA,NRC_fear,sentisense_fear,FREQUENCY) VALUES(%s,%s,%s,%s)\",(word,False,False,0))\n",
    "\n",
    "conn.commit()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze hashtag\n",
    "cur.execute(\"update HASHTAGFEAR as ha1 set frequency = (select count(*) from HASHTAGFEAR ha2 where ha1.hashtag = ha2.hashtag);\")\n",
    "cur.execute(\"delete from HASHTAGFEAR as ha1 where exists (Select 1 From HASHTAGFEAR as ha2 where ha1.hashtag = ha2.hashtag and ha1.frequency = ha2.frequency and ha2.ctid > ha1.ctid);\")\n",
    "\n",
    "conn.commit()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze emoji\n",
    "cur.execute(\"update EMOJIFEAR as ha1 set frequency = (select count(*) from EMOJIFEAR ha2 where ha1.emoji = ha2.emoji);\")\n",
    "cur.execute(\"delete from EMOJIFEAR as ha1 where exists (Select 1 From EMOJIFEAR as ha2 where ha1.emoji = ha2.emoji and ha1.frequency = ha2.frequency and ha2.ctid > ha1.ctid);\")\n",
    "\n",
    "conn.commit()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze emoticons\n",
    "cur.execute(\"update EMOTICONSFEAR as ha1 set frequency = (select count(*) from EMOTICONSFEAR ha2 where ha1.emoticons = ha2.emoticons);\")\n",
    "cur.execute(\"delete from EMOTICONSFEAR as ha1 where exists (Select 1 From EMOTICONSFEAR as ha2 where ha1.emoticons = ha2.emoticons and ha1.frequency = ha2.frequency and ha2.ctid > ha1.ctid);\")\n",
    "\n",
    "conn.commit() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze parole\n",
    "cur.execute(\"select count(*),parola from PAROLEFEAR group by parola\")\n",
    "result = []\n",
    "for word in cur.fetchall():\n",
    "    result.append(word) \n",
    "\n",
    "cur.execute(\"truncate table PAROLEFEAR;\")\n",
    "#cur.execute(\"ALTER SEQUENCE PAROLEFEAR RESTART WITH 1;\")\n",
    "\n",
    "for couple in result:\n",
    "    cur.execute(\"INSERT INTO PAROLEFEAR (PAROLA,NRC_fear,sentisense_fear,FREQUENCY) VALUES(%s,%s,%s,%s)\",(couple[1],False,False,couple[0]))\n",
    "\n",
    "conn.commit()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggiornamento presenza parola nelle risorse lessicali\n",
    "cur.execute(\"UPDATE PAROLEFEAR SET NRC_fear = TRUE WHERE parola IN (select p.parola from PAROLEFEAR as p LEFT OUTER JOIN RISORSEFEAR as r ON p.parola = r.parola WHERE r.nome = 'NRC_fear')\")\n",
    "cur.execute(\"UPDATE PAROLEFEAR SET sentisense_fear = TRUE WHERE parola IN (select p.parola from PAROLEFEAR as p LEFT OUTER JOIN RISORSEFEAR as r ON p.parola = r.parola WHERE r.nome = 'sentisense_fear')\")\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvataggio in un txt delle parole non presenti in nessuna risorsa lessicale\n",
    "wordsMongo = fearCollections.find({\"LexResources\": []}, {\"_id\": 0, \"freq\": 0, \"LexResources\": 0})\n",
    "words = []\n",
    "\n",
    "for word in wordsMongo:\n",
    "    words.append(word['word'])\n",
    "\n",
    "with open('Nuove Parole/fear.txt', 'w',encoding=\"utf8\") as f:\n",
    "    for word in words:\n",
    "        f.write(\"%s\\n\" % word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trattamento dataset Joy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lista di righe, ogni riga una tweet joy\n",
    "file = open(\"Dataset/dataset_dt_joy_60k.txt\", 'r',encoding=\"utf8\")\n",
    "joy = []\n",
    "hashtagJoy = []\n",
    "emoticonJoy = []\n",
    "emojiJoy = []\n",
    "\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "    riga = re.sub(r'URL', '',riga)\n",
    "    riga = re.sub(r'USERNAME', '',riga)\n",
    "    \n",
    "    #array di hashtag singoli\n",
    "    hashTag = (re.findall('#[a-zA-Z0-9]+', riga))\n",
    "\n",
    "    #generiamo l'array di hashtag senza hashtag\n",
    "    for h in hashTag:\n",
    "        hashtagJoy.append(h.replace(\"#\",\"\"))\n",
    "\n",
    "    #Rimuoviamo glin hashtag dalla riga\n",
    "    riga = riga.replace('#',\"\")\n",
    "\n",
    "    joy.append(riga)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(joy)):\n",
    "    #Controllo se l'emoticons c'è e in caso la rimuovo\n",
    "    for singleEmoticon in emoticons:\n",
    "        emoticonsFind = re.findall(re.escape(singleEmoticon),joy[i])\n",
    "        if(len(emoticonsFind) != 0):\n",
    "            emoticonFear.extend(emoticonsFind)\n",
    "            joy[i] = joy[i].replace(singleEmoticon,\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(joy)):\n",
    "    #preleviamo le emoji e le rimuoviamo dal testo\n",
    "    data = regex.findall(r'\\X', joy[i])\n",
    "    for word in data:\n",
    "        if any(char in emoji.UNICODE_EMOJI['en'] for char in word):\n",
    "            emojiJoy.append(word)\n",
    "            joy[i] = joy[i].replace(word, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processamento slang prima di punteggiatura e lowercase\n",
    "with open(\"Slangs/slangs_before.json\") as f:\n",
    "    slangsBefore = json.load(f)\n",
    "    for i in range(len(joy)):\n",
    "        for word in joy[i].split():\n",
    "            indexOfWord = joy[i].find(word)\n",
    "            if slangsBefore.get(word) is None:\n",
    "                if slangsBefore.get(word.lower()) is None:\n",
    "                    if slangsBefore.get(word.upper()) is None:\n",
    "                        continue\n",
    "                    else:\n",
    "                        joy[i] = joy[i][:indexOfWord] + slangsBefore.get(word.upper()) + joy[i][indexOfWord+len(word):]\n",
    "                else:\n",
    "                    joy[i] = joy[i][:indexOfWord] + slangsBefore.get(word.lower()) + joy[i][indexOfWord+len(word):]\n",
    "            else:\n",
    "                joy[i] = joy[i][:indexOfWord] + slangsBefore.get(word) + joy[i][indexOfWord+len(word):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizzazione di tutte le frasi\n",
    "joyTokenized = []\n",
    "for linea in joy:\n",
    "    result = word_tokenize(linea)\n",
    "    joyTokenized.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Esecuzione della POS tagging per ogni riga della matrice\n",
    "for i in range(len(joyTokenized)):\n",
    "    joyTokenized[i] = nltk.pos_tag(joyTokenized[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trattamento punteggiatura\n",
    "for line in joyTokenized:\n",
    "    newLine = line\n",
    "    listToRemove = []\n",
    "    for i in range(len(line)):\n",
    "        if line[i][0] in puntuaction:\n",
    "            listToRemove.append(line[i])\n",
    "    for element in listToRemove:\n",
    "        newLine.remove(element)\n",
    "    line = newLine\n",
    "\n",
    "linePuntuaction = re.compile(\"[\" + re.escape(\"\".join(puntuaction)) + \"]\")\n",
    "for line in joyTokenized:\n",
    "    for i in range(len(line)):\n",
    "        tupla = [(re.sub(linePuntuaction, \"\", line[i][0])),line[i][1]]\n",
    "        line[i] = tupla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming\n",
    "#ps = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for line in joyTokenized:\n",
    "    for i in range(len(line)):\n",
    "        tupla = [lemmatizer.lemmatize(line[i][0]),line[i][1]]\n",
    "        line[i] = tupla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trattamnento Stop word dopo Stemming\n",
    "stop_words = set(stopwords.words('english'))\n",
    "joyTokStoped = []\n",
    "for line in joyTokenized:\n",
    "    filtred_word = []\n",
    "    for row in line:\n",
    "        if  row[0] not in stop_words:\n",
    "            filtred_word.append(row) \n",
    "    joyTokStoped.append(filtred_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joyListWord = []\n",
    "joyListTok = []\n",
    "for line in joyTokStoped:\n",
    "    for row in line:\n",
    "        joyListWord.append(row[0])\n",
    "        joyListTok.append(row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lavorazione risorse lessicali\n",
    "resourceJoyEmoSN = []\n",
    "resourceJoyNRC = []\n",
    "resourceJoySentisence = []\n",
    "#Lista di righe, ogni riga una tweet anger\n",
    "file = open(\"Risorse lessicali\\Joy\\EmoSN_joy.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        resourceJoyEmoSN.append(riga)\n",
    "\n",
    "file.close()\n",
    "\n",
    "file = open(\"Risorse lessicali\\\\Joy\\\\NRC_joy.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        resourceJoyNRC.append(riga)\n",
    "\n",
    "file.close()\n",
    "\n",
    "file = open(\"Risorse lessicali\\Joy\\sentisense_joy.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        resourceJoySentisence.append(riga)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Caricare le risorse lessicali \n",
    "\n",
    "JoyEmoSN = {\n",
    "    '_id': 'EmoSN_joy',\n",
    "    'sentiment': 'joy', \n",
    "    'wordList': resourceJoyEmoSN\n",
    "}\n",
    "\n",
    "JoyNRC = {\n",
    "    '_id': 'NRC_joy',\n",
    "    'sentiment': 'joy',  \n",
    "    'wordList': resourceJoyNRC\n",
    "}\n",
    "\n",
    "JoySentisense = {\n",
    "    '_id': 'sentisense_joy',\n",
    "    'sentiment': 'joy',\n",
    "    'wordList': resourceJoySentisence\n",
    "}\n",
    "\n",
    "resourcesCollection.insert_one(JoyEmoSN)\n",
    "resourcesCollection.insert_one(JoyNRC)\n",
    "resourcesCollection.insert_one(JoySentisense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conteggio delle frequenze delle parole\n",
    "countWordJoy = Counter(joyListWord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione database e collezioni\n",
    "# Inserimento singoli documenti contententi i tweet joy\n",
    "wordsString = {}\n",
    "\n",
    "for i in range(len(joyListWord)):\n",
    "    wordsString = {\n",
    "        \"lemma\": joyListWord[i],\n",
    "        \"POS\": joyListTok[i]\n",
    "    }\n",
    "    joyTwit={\"word\":wordsString}\n",
    "    joyCollections.insert_one(joyTwit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inserimento collezione Twitter con hashtags, emoticons ed emoji \n",
    "joyInsert = {\n",
    "    \"sentiment\":\"joy\",\n",
    "    \"hashtags\": hashtagJoy,\n",
    "    \"emoticons\": emoticonJoy,\n",
    "    \"emoji\": emojiJoy\n",
    "}\n",
    "\n",
    "#inserimento\n",
    "x = collectionAJoy.insert_one(joyInsert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raggruppa i documenti di input in base all'espressione _id specificata e per ciascun raggruppamento distinto genera un documento\n",
    "pipeline = [\n",
    "    {\"$group\": {\"_id\": \"$word.lemma\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"word\": \"$_id\",\n",
    "      \"freq\": \"$count\"\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"freq\": -1\n",
    "    }},\n",
    "    {\"$out\": \"joy\"}\n",
    "]\n",
    "\n",
    "#post processing per renderlo valido \n",
    "joyCollections.aggregate(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creazione Word Clouds per le parole\n",
    "data = {}\n",
    "for document in joyCollections.find().limit(50):\n",
    "    data[document['word']] = document['freq']\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2)\n",
    "wc.fit_words(data)\n",
    "wc.to_file('Word Clouds/joyWords.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conteggio degli hashtags\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$hashtags\"},\n",
    "    {\"$group\": {\"_id\": \"$hashtags\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"word\": \"$_id\",\n",
    "      \"count\": 1\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"count\": -1\n",
    "    }}\n",
    "]\n",
    "\n",
    "hashtagsCount = collectionAJoy.aggregate(pipeline)\n",
    "\n",
    "data = {}\n",
    "for document in hashtagsCount:\n",
    "    data[document['word']] = document['count']\n",
    "\n",
    "collectionAJoy.update_one(\n",
    "  {\"sentiment\": \"joy\"},\n",
    "  {\"$set\": {\"hashtags\": data}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creazione Word Clouds per gli hashtags\n",
    "\n",
    "hashtags = collectionAJoy.distinct(\"hashtags\")\n",
    "\n",
    "hashtagsLimited = {}\n",
    "for (word,count) in [x for x in hashtags[0].items()][:50]:\n",
    "    hashtagsLimited[word] = count\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2)\n",
    "wc.fit_words(hashtagsLimited)\n",
    "wc.to_file('Word Clouds/joyHashtags.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conteggio delle emoji\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$emoji\"},\n",
    "    {\"$group\": {\"_id\": \"$emoji\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"emoji\": \"$_id\",\n",
    "      \"count\": 1\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"count\": -1\n",
    "    }}\n",
    "]\n",
    "\n",
    "emojiCount = collectionAJoy.aggregate(pipeline)\n",
    "\n",
    "data = {}\n",
    "for document in emojiCount:\n",
    "    data[document['emoji']] = document['count']\n",
    "\n",
    "collectionAJoy.update_one(\n",
    "  {\"sentiment\": \"joy\"},\n",
    "  {\"$set\": {\"emoji\": data}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creazione Word Clouds per le emoji\n",
    "\n",
    "emojis = collectionAJoy.distinct(\"emoji\")\n",
    "\n",
    "emojisLimited = {}\n",
    "for (word,count) in [x for x in emojis[0].items()][:50]:\n",
    "    emojisLimited[word] = count\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2, font_path='Font Emoji\\Symbola.otf')\n",
    "wc.fit_words(emojisLimited)\n",
    "wc.to_file('Word Clouds/joyEmojis.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conteggio degli emoticons\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$emoticons\"},\n",
    "    {\"$group\": {\"_id\": \"$emoticons\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"emoticons\": \"$_id\",\n",
    "      \"count\": 1\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"count\": -1\n",
    "    }}\n",
    "]\n",
    "\n",
    "emoticonsCount = collectionAJoy.aggregate(pipeline)\n",
    "\n",
    "data = {}\n",
    "for document in emoticonsCount:\n",
    "    data[document['emoticons']] = document['count']\n",
    "\n",
    "collectionAJoy.update_one(\n",
    "  {\"sentiment\": \"joy\"},\n",
    "  {\"$set\": {\"emoticons\": data}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creazione Word Clouds per le emoticons\n",
    "\n",
    "emoticons = collectionAJoy.distinct(\"emoticons\")\n",
    "\n",
    "emoticonsLimited = {}\n",
    "for (word,count) in [x for x in emoticons[0].items()][:50]:\n",
    "    emoticonsLimited[word] = count\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2)\n",
    "wc.fit_words(emoticonsLimited)\n",
    "wc.to_file('Word Clouds/joyEmoticons.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline di aggregazione per inserire la risorsa lessicale a cui appartiene la parola\n",
    "\n",
    "pipeline = [\n",
    "    {\"$lookup\": {\n",
    "        \"from\": \"Resources\",\n",
    "        \"localField\": \"word\",\n",
    "        \"foreignField\": \"wordList\",\n",
    "        \"pipeline\": [\n",
    "            {\"$match\": {\"$expr\": {\"$regexMatch\": {\"input\": \"$_id\", \"regex\": \"joy\"}}}}\n",
    "        ],\n",
    "        \"as\": \"LexResources\"\n",
    "    }},\n",
    "    {\"$project\": {\n",
    "        \"resources.wordList\": 0,\n",
    "        \"resources.totNumberWords\": 0,\n",
    "        \"resources.sentiment\": 0,\n",
    "        \"LexResources.sentiment\": 0,\n",
    "        \"LexResources.totNumberWords\": 0,\n",
    "        \"LexResources.wordList\": 0\n",
    "    }},\n",
    "    {\"$out\": \"joy\"}\n",
    "]\n",
    "\n",
    "joyCollections.aggregate(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline di aggregazione per calcolare le percentuali richieste\n",
    "\n",
    "pipelineCount = [\n",
    "    {\"$group\": {\"_id\": \"null\", \"count\": { \"$sum\": \"$freq\" }}},\n",
    "    {\"$project\": {\n",
    "        \"_id\": 0,\n",
    "        \"count\": \"$count\"\n",
    "    }}\n",
    "]\n",
    "\n",
    "results = joyCollections.aggregate(pipelineCount)\n",
    "\n",
    "countWords = 0\n",
    "for data in results:\n",
    "    countWords = data['count']\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$LexResources\"},\n",
    "    {\"$lookup\": {\n",
    "        \"from\": \"Resources\",\n",
    "        \"localField\": \"LexResources._id\",\n",
    "        \"foreignField\": \"_id\",\n",
    "        \"as\": \"RefResource\"\n",
    "    }},\n",
    "    {\"$group\": {\"_id\": { \"lex\": \"$LexResources._id\", \"RefResource\": \"$RefResource\" }, \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "        \"_id\": \"$_id.lex\",\n",
    "        \"wordList\": \"$_id.RefResource.wordList\",\n",
    "        \"commonWords\": \"$count\"\n",
    "    }},\n",
    "    {\"$unwind\": \"$wordList\"},\n",
    "    {\"$project\": {\n",
    "        \"_id\": 1,\n",
    "        \"commonWords\": 1,\n",
    "        \"totWordsLex\": {\"$size\": \"$wordList\"}\n",
    "    }},\n",
    "    {\"$addFields\": {\"totalWordsTwitter\": countWords}},\n",
    "    {\"$project\": {\n",
    "        \"perc_presence_lex_res\": { \"$round\": [{\"$divide\": [ \"$commonWords\", \"$totWordsLex\" ] }, 6] },\n",
    "        \"perc_presence_twitter\": { \"$round\": [{\"$divide\": [ \"$commonWords\", \"$totalWordsTwitter\" ]}, 6] }\n",
    "    }}\n",
    "]\n",
    "\n",
    "results = joyCollections.aggregate(pipeline)\n",
    "\n",
    "# Genero i dati per l'istogramma\n",
    "istogramJoy_lexRes = []\n",
    "istogramJoy_twitter = []\n",
    "\n",
    "for data in results:\n",
    "    print(data)\n",
    "    log =math.log(data['perc_presence_lex_res'],10)\n",
    "    istogramJoy_lexRes.append([data['_id'], data['perc_presence_lex_res']])\n",
    "    log =math.log(data['perc_presence_twitter'],10)\n",
    "    istogramJoy_twitter.append([data['_id'], data['perc_presence_twitter']])\n",
    " \n",
    "# Create the pandas DataFrame\n",
    "dfJoy_lexRes = pd.DataFrame(istogramJoy_lexRes, columns = ['Risorsa', 'Percentuale'])\n",
    "dfJoy_twitter = pd.DataFrame(istogramJoy_twitter,columns = ['Risorsa', 'Percentuale'])      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione istogramma per presenza parole nelle risorse lessicali\n",
    "\n",
    "dfJoy_lexRes.plot.bar(x='Risorsa')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione istogramma per presenza parole nei twitter\n",
    "\n",
    "dfJoy_twitter.plot.bar(x='Risorsa')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connessione, creazione cursore e database\n",
    "\n",
    "conn = psycopg2.connect(database = \"maadb\", user = \"postgres\", password = \"admin\", host = \"localhost\", port = \"5432\")\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione tabella risorse lessicali joy e inserimento\n",
    "cur.execute('''CREATE TABLE RISORSEJOY\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    NOME             TEXT       NOT NULL,\n",
    "                    PAROLA           TEXT       NOT NULL\n",
    "                );\n",
    "            '''\n",
    ")\n",
    "nome = \"EmoSN_joy\"\n",
    "for word in resourceJoyEmoSN:\n",
    "    cur.execute(\"INSERT INTO RISORSEJOY (NOME, PAROLA) VALUES(%s, %s)\",(nome,word))\n",
    "\n",
    "nome = \"NRC_joy\"\n",
    "for word in resourceJoyNRC:\n",
    "    cur.execute(\"INSERT INTO RISORSEJOY (NOME, PAROLA) VALUES(%s, %s)\",(nome,word))\n",
    "\n",
    "nome = \"sentisense_joy\"\n",
    "for word in resourceJoySentisence:\n",
    "    cur.execute(\"INSERT INTO RISORSEJOY (NOME, PAROLA) VALUES(%s, %s)\",(nome,word))\n",
    "\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione tabelle per le parole, emoji, emoticon e hashtag\n",
    "\n",
    "cur.execute('''CREATE TABLE PAROLEJOY\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    PAROLA           TEXT       NOT NULL, \n",
    "                    EmoSN_joy      BOOL       NOT NULL DEFAULT FALSE,\n",
    "                    NRC_joy      BOOL       NOT NULL DEFAULT FALSE,\n",
    "                    sentisense_joy      BOOL       NOT NULL DEFAULT FALSE,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ")   \n",
    "\n",
    "cur.execute('''CREATE TABLE EMOJIJOY\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    EMOJI       TEXT       NOT NULL,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ") \n",
    "\n",
    "cur.execute('''CREATE TABLE EMOTICONSJOY\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    EMOTICONS       TEXT       NOT NULL,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ") \n",
    "\n",
    "cur.execute('''CREATE TABLE HASHTAGJOY\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    HASHTAG     TEXT       NOT NULL,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ") \n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inserimento hashtag, emoticon ed emoji\n",
    "\n",
    "for hashTag in hashtagJoy:\n",
    "    cur.execute(\"INSERT INTO HASHTAGJOY (HASHTAG,FREQUENCY) VALUES(%s,%s)\",(hashTag,0))\n",
    "\n",
    "for emoticons in emoticonJoy:\n",
    "    cur.execute(\"INSERT INTO EMOTICONSJOY (EMOTICONS,FREQUENCY) VALUES(%s,%s)\",(emoticons,0))\n",
    "\n",
    "for emojis in emojiJoy:\n",
    "    cur.execute(\"INSERT INTO EMOJIJOY (EMOJI,FREQUENCY) VALUES(%s,%s)\",(emojis,0))\n",
    "\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inserimento parole in db\n",
    "for word in joyListWord:\n",
    "    cur.execute(\"INSERT INTO PAROLEJOY (PAROLA,EmoSN_joy,NRC_joy,sentisense_joy,FREQUENCY) VALUES(%s,%s,%s,%s,%s)\",(word,False,False,False,0))\n",
    "\n",
    "conn.commit()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze hashtag\n",
    "cur.execute(\"update HASHTAGJOY as ha1 set frequency = (select count(*) from HASHTAGJOY ha2 where ha1.hashtag = ha2.hashtag);\")\n",
    "cur.execute(\"delete from HASHTAGJOY as ha1 where exists (Select 1 From HASHTAGJOY as ha2 where ha1.hashtag = ha2.hashtag and ha1.frequency = ha2.frequency and ha2.ctid > ha1.ctid);\")\n",
    "\n",
    "conn.commit()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze emoji\n",
    "cur.execute(\"update EMOJIJOY as ha1 set frequency = (select count(*) from EMOJIJOY ha2 where ha1.emoji = ha2.emoji);\")\n",
    "cur.execute(\"delete from EMOJIJOY as ha1 where exists (Select 1 From EMOJIJOY as ha2 where ha1.emoji = ha2.emoji and ha1.frequency = ha2.frequency and ha2.ctid > ha1.ctid);\")\n",
    "\n",
    "conn.commit() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze emoticons\n",
    "cur.execute(\"update EMOTICONSJOY as ha1 set frequency = (select count(*) from EMOTICONSJOY ha2 where ha1.emoticons = ha2.emoticons);\")\n",
    "cur.execute(\"delete from EMOTICONSJOY as ha1 where exists (Select 1 From EMOTICONSJOY as ha2 where ha1.emoticons = ha2.emoticons and ha1.frequency = ha2.frequency and ha2.ctid > ha1.ctid);\")\n",
    "\n",
    "conn.commit()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze parole\n",
    "cur.execute(\"select count(*),parola from PAROLEJOY group by parola\")\n",
    "result = []\n",
    "for word in cur.fetchall():\n",
    "    result.append(word) \n",
    "\n",
    "cur.execute(\"truncate table PAROLEJOY;\")\n",
    "#cur.execute(\"ALTER SEQUENCE parolejoy_id_seq RESTART WITH 1;\")\n",
    "\n",
    "for couple in result:\n",
    "    cur.execute(\"INSERT INTO PAROLEJOY (PAROLA,EmoSN_joy,NRC_joy,sentisense_joy,FREQUENCY) VALUES(%s,%s,%s,%s,%s)\",(couple[1],False,False,False,couple[0]))\n",
    "\n",
    "conn.commit()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggiornamento presenza parola nelle risorse lessicali\n",
    "cur.execute(\"UPDATE PAROLEJOY SET EmoSN_joy = TRUE WHERE parola IN (select p.parola from PAROLEJOY as p LEFT OUTER JOIN RISORSEJOY as r ON p.parola = r.parola WHERE r.nome = 'EmoSN_joy')\")\n",
    "cur.execute(\"UPDATE PAROLEJOY SET NRC_joy = TRUE WHERE parola IN (select p.parola from PAROLEJOY as p LEFT OUTER JOIN RISORSEJOY as r ON p.parola = r.parola WHERE r.nome = 'NRC_joy')\")\n",
    "cur.execute(\"UPDATE PAROLEJOY SET sentisense_joy = TRUE WHERE parola IN (select p.parola from PAROLEJOY as p LEFT OUTER JOIN RISORSEJOY as r ON p.parola = r.parola WHERE r.nome = 'sentisense_joy')\")\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvataggio in un txt delle parole non presenti in nessuna risorsa lessicale\n",
    "\n",
    "wordsMongo = joyCollections.find({\"LexResources\": []}, {\"_id\": 0, \"freq\": 0, \"LexResources\": 0})\n",
    "words = []\n",
    "\n",
    "for word in wordsMongo:\n",
    "    words.append(word['word'])\n",
    "\n",
    "with open('Nuove Parole/joy.txt', 'w',encoding=\"utf8\") as f:\n",
    "    for word in words:\n",
    "        f.write(\"%s\\n\" % word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trattamento dataset Sadness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lista di righe, ogni riga una tweet sadness\n",
    "file = open(\"Dataset/dataset_dt_sadness_60k.txt\", 'r',encoding=\"utf8\")\n",
    "sadness = []\n",
    "hashtagSadness = []\n",
    "emoticonSadness = []\n",
    "emojiSadness = []\n",
    "\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "    riga = re.sub(r'URL', '',riga)\n",
    "    riga = re.sub(r'USERNAME', '',riga)\n",
    "\n",
    "    #array di hashtag singoli\n",
    "    hashTag = (re.findall('#[a-zA-Z0-9]+', riga))\n",
    "\n",
    "    #generiamo l'array di hashtag senza hashtag\n",
    "    for h in hashTag:\n",
    "        hashtagSadness.append(h.replace(\"#\",\"\"))\n",
    "\n",
    "    #Rimuoviamo glin hashtag dalla riga\n",
    "    riga = riga.replace('#',\"\")\n",
    "\n",
    "    sadness.append(riga)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sadness)):\n",
    "    #Controllo se l'emoticons c'è e in caso la rimuovo\n",
    "    for singleEmoticon in emoticons:\n",
    "        emoticonsFind = re.findall(re.escape(singleEmoticon),sadness[i])\n",
    "        if(len(emoticonsFind) != 0):\n",
    "            emoticonSadness.extend(emoticonsFind)\n",
    "            sadness[i] = sadness[i].replace(singleEmoticon,\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sadness)):\n",
    "    #preleviamo le emoji e le rimuoviamo dal testo\n",
    "    data = regex.findall(r'\\X', sadness[i])\n",
    "    for word in data:\n",
    "        if any(char in emoji.UNICODE_EMOJI['en'] for char in word):\n",
    "            emojiSadness.append(word)\n",
    "            sadness[i] = sadness[i].replace(word, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processamento slang prima di punteggiatura e lowercase\n",
    "with open(\"Slangs/slangs_before.json\") as f:\n",
    "    slangsBefore = json.load(f)\n",
    "    for i in range(len(sadness)):\n",
    "        for word in sadness[i].split():\n",
    "            indexOfWord = sadness[i].find(word)\n",
    "            if slangsBefore.get(word) is None:\n",
    "                if slangsBefore.get(word.lower()) is None:\n",
    "                    if slangsBefore.get(word.upper()) is None:\n",
    "                        continue\n",
    "                    else:\n",
    "                        sadness[i] = sadness[i][:indexOfWord] + slangsBefore.get(word.upper()) + sadness[i][indexOfWord+len(word):]\n",
    "                else:\n",
    "                    sadness[i] = sadness[i][:indexOfWord] + slangsBefore.get(word.lower()) + sadness[i][indexOfWord+len(word):]\n",
    "            else:\n",
    "                sadness[i] = sadness[i][:indexOfWord] + slangsBefore.get(word) + sadness[i][indexOfWord+len(word):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizzazione di tutte le frasi\n",
    "sadnessTokenized = []\n",
    "for linea in sadness:\n",
    "    result = word_tokenize(linea)\n",
    "    sadnessTokenized.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Esecuzione della POS tagging per ogni riga della matrice\n",
    "for i in range(len(sadnessTokenized)):\n",
    "    sadnessTokenized[i] = nltk.pos_tag(sadnessTokenized[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trattamento punteggiatura\n",
    "for line in sadnessTokenized:\n",
    "    newLine = line\n",
    "    listToRemove = []\n",
    "    for i in range(len(line)):\n",
    "        if line[i][0] in puntuaction:\n",
    "            listToRemove.append(line[i])\n",
    "    for element in listToRemove:\n",
    "        newLine.remove(element)\n",
    "    line = newLine\n",
    "\n",
    "linePuntuaction = re.compile(\"[\" + re.escape(\"\".join(puntuaction)) + \"]\")\n",
    "for line in sadnessTokenized:\n",
    "    for i in range(len(line)):\n",
    "        tupla = [(re.sub(linePuntuaction, \"\", line[i][0])),line[i][1]]\n",
    "        line[i] = tupla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming\n",
    "#ps = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for line in sadnessTokenized:\n",
    "    for i in range(len(line)):\n",
    "        tupla = [lemmatizer.lemmatize(line[i][0]),line[i][1]]\n",
    "        line[i] = tupla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trattamnento Stop word dopo Stemming\n",
    "stop_words = set(stopwords.words('english'))\n",
    "sadnessTokStoped = []\n",
    "for line in sadnessTokenized:\n",
    "    filtred_word = []\n",
    "    for row in line:\n",
    "        if  row[0] not in stop_words:\n",
    "            filtred_word.append(row) \n",
    "    sadnessTokStoped.append(filtred_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sadnessListWord = []\n",
    "sadnessListTok = []\n",
    "for line in sadnessTokStoped:\n",
    "    for row in line:\n",
    "        sadnessListWord.append(row[0])\n",
    "        sadnessListTok.append(row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lavorazione risorse lessicali\n",
    "resourceSadnessNRC = []\n",
    "resourceSadnessSentisense = []\n",
    "file = open(\"Risorse lessicali\\\\Sadness\\\\NRC_sadness.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        resourceSadnessNRC.append(riga)\n",
    "\n",
    "file.close()\n",
    "\n",
    "file = open(\"Risorse lessicali\\Sadness\\sentisense_sadness.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        resourceSadnessSentisense.append(riga)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.InsertOneResult at 0x1b860332730>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Caricare le risorse lessicali \n",
    "SadnessNRC = {\n",
    "    '_id': 'NRC_sadness',\n",
    "    'sentiment': 'sadness',  \n",
    "    'wordList': resourceSadnessNRC\n",
    "}\n",
    "\n",
    "SadnessSentisense = {\n",
    "    '_id': 'sentisense_sadness',\n",
    "    'sentiment': 'sadness',\n",
    "    'wordList': resourceSadnessSentisense\n",
    "}\n",
    "\n",
    "resourcesCollection.insert_one(SadnessNRC)\n",
    "resourcesCollection.insert_one(SadnessSentisense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione database e collezioni\n",
    "# Inserimento singoli documenti contententi i tweet sadness\n",
    "wordsString = {}\n",
    "\n",
    "for i in range(len(sadnessListWord)):\n",
    "    wordsString = {\n",
    "        \"lemma\": sadnessListWord[i],\n",
    "        \"POS\": sadnessListTok[i]\n",
    "    }\n",
    "    sadnessTwit={\"word\":wordsString}\n",
    "    sadnessCollections.insert_one(sadnessTwit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inserimento collezione Twitter con hashtags, emoticons ed emoji \n",
    "sadnessInsert = {\n",
    "    \"sentiment\":\"sadness\",\n",
    "    \"hashtags\": hashtagSadness,\n",
    "    \"emoticons\": emoticonSadness,\n",
    "    \"emoji\": emojiSadness\n",
    "}\n",
    "\n",
    "#inserimento\n",
    "x = collectionSadness.insert_one(sadnessInsert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.command_cursor.CommandCursor at 0x1b846e2eaf0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Raggruppa i documenti di input in base all'espressione _id specificata e per ciascun raggruppamento distinto genera un documento\n",
    "pipeline = [\n",
    "    {\"$group\": {\"_id\": \"$word.lemma\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"word\": \"$_id\",\n",
    "      \"freq\": \"$count\"\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"freq\": -1\n",
    "    }},\n",
    "    {\"$out\": \"sadness\"}\n",
    "]\n",
    "\n",
    "#post processing per renderlo valido \n",
    "sadnessCollections.aggregate(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<wordcloud.wordcloud.WordCloud at 0x1b846e36f40>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creazione Word Clouds per le parole\n",
    "data = {}\n",
    "for document in sadnessCollections.find().limit(50):\n",
    "    data[document['word']] = document['freq']\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2)\n",
    "wc.fit_words(data)\n",
    "wc.to_file('Word Clouds/sadnessWords.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.UpdateResult at 0x1b846ec3580>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Conteggio degli hashtags\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$hashtags\"},\n",
    "    {\"$group\": {\"_id\": \"$hashtags\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"word\": \"$_id\",\n",
    "      \"count\": 1\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"count\": -1\n",
    "    }}\n",
    "]\n",
    "\n",
    "hashtagsCount = collectionSadness.aggregate(pipeline)\n",
    "\n",
    "data = {}\n",
    "for document in hashtagsCount:\n",
    "    data[document['word']] = document['count']\n",
    "\n",
    "collectionSadness.update_one(\n",
    "  {\"sentiment\": \"sadness\"},\n",
    "  {\"$set\": {\"hashtags\": data}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<wordcloud.wordcloud.WordCloud at 0x1b85e2ea460>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creazione Word Clouds per gli hashtags\n",
    "\n",
    "hashtags = collectionSadness.distinct(\"hashtags\")\n",
    "\n",
    "hashtagsLimited = {}\n",
    "for (word,count) in [x for x in hashtags[0].items()][:50]:\n",
    "    hashtagsLimited[word] = count\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2)\n",
    "wc.fit_words(hashtagsLimited)\n",
    "wc.to_file('Word Clouds/sadnessHashtags.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.UpdateResult at 0x1b846ecc0d0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Conteggio delle emoji\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$emoji\"},\n",
    "    {\"$group\": {\"_id\": \"$emoji\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"emoji\": \"$_id\",\n",
    "      \"count\": 1\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"count\": -1\n",
    "    }}\n",
    "]\n",
    "\n",
    "emojiCount = collectionSadness.aggregate(pipeline)\n",
    "\n",
    "data = {}\n",
    "for document in emojiCount:\n",
    "    data[document['emoji']] = document['count']\n",
    "\n",
    "collectionSadness.update_one(\n",
    "  {\"sentiment\": \"sadness\"},\n",
    "  {\"$set\": {\"emoji\": data}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<wordcloud.wordcloud.WordCloud at 0x1b85e280c10>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creazione Word Clouds per le emoji\n",
    "\n",
    "emojis = collectionSadness.distinct(\"emoji\")\n",
    "\n",
    "emojisLimited = {}\n",
    "for (word,count) in [x for x in emojis[0].items()][:50]:\n",
    "    emojisLimited[word] = count\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2, font_path='Font Emoji\\Symbola.otf')\n",
    "wc.fit_words(emojisLimited)\n",
    "wc.to_file('Word Clouds/sadnessEmojis.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.UpdateResult at 0x1b846eccaf0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Conteggio degli emoticons\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$emoticons\"},\n",
    "    {\"$group\": {\"_id\": \"$emoticons\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"emoticons\": \"$_id\",\n",
    "      \"count\": 1\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"count\": -1\n",
    "    }}\n",
    "]\n",
    "\n",
    "emoticonsCount = collectionSadness.aggregate(pipeline)\n",
    "\n",
    "data = {}\n",
    "for document in emoticonsCount:\n",
    "    data[document['emoticons']] = document['count']\n",
    "\n",
    "collectionSadness.update_one(\n",
    "  {\"sentiment\": \"sadness\"},\n",
    "  {\"$set\": {\"emoticons\": data}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<wordcloud.wordcloud.WordCloud at 0x1b85e2ea2b0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creazione Word Clouds per le emoticons\n",
    "\n",
    "emoticons = collectionSadness.distinct(\"emoticons\")\n",
    "\n",
    "emoticonsLimited = {}\n",
    "for (word,count) in [x for x in emoticons[0].items()][:50]:\n",
    "    emoticonsLimited[word] = count\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2)\n",
    "wc.fit_words(emoticonsLimited)\n",
    "wc.to_file('Word Clouds/sadnessEmoticons.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.command_cursor.CommandCursor at 0x1b85ebdb850>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pipeline di aggregazione per inserire la risorsa lessicale a cui appartiene la parola\n",
    "\n",
    "pipeline = [\n",
    "    {\"$lookup\": {\n",
    "        \"from\": \"Resources\",\n",
    "        \"localField\": \"word\",\n",
    "        \"foreignField\": \"wordList\",\n",
    "        \"pipeline\": [\n",
    "            {\"$match\": {\"$expr\": {\"$regexMatch\": {\"input\": \"$_id\", \"regex\": \"sadness\"}}}}\n",
    "        ],\n",
    "        \"as\": \"LexResources\"\n",
    "    }},\n",
    "    {\"$project\": {\n",
    "        \"resources.wordList\": 0,\n",
    "        \"resources.totNumberWords\": 0,\n",
    "        \"resources.sentiment\": 0,\n",
    "        \"LexResources.sentiment\": 0,\n",
    "        \"LexResources.totNumberWords\": 0,\n",
    "        \"LexResources.wordList\": 0\n",
    "    }},\n",
    "    {\"$out\": \"sadness\"}\n",
    "]\n",
    "\n",
    "sadnessCollections.aggregate(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': 'sentisense_sadness', 'perc_presence_lex_res': 0.401515, 'perc_presence_twitter': 9.9e-05}\n",
      "{'_id': 'NRC_sadness', 'perc_presence_lex_res': 0.439127, 'perc_presence_twitter': 0.000977}\n"
     ]
    }
   ],
   "source": [
    "#Pipeline di aggregazione per calcolare le percentuali richieste\n",
    "\n",
    "pipelineCount = [\n",
    "    {\"$group\": {\"_id\": \"null\", \"count\": { \"$sum\": \"$freq\" }}},\n",
    "    {\"$project\": {\n",
    "        \"_id\": 0,\n",
    "        \"count\": \"$count\"\n",
    "    }}\n",
    "]\n",
    "\n",
    "results = sadnessCollections.aggregate(pipelineCount)\n",
    "\n",
    "countWords = 0\n",
    "for data in results:\n",
    "    countWords = data['count']\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$LexResources\"},\n",
    "    {\"$lookup\": {\n",
    "        \"from\": \"Resources\",\n",
    "        \"localField\": \"LexResources._id\",\n",
    "        \"foreignField\": \"_id\",\n",
    "        \"as\": \"RefResource\"\n",
    "    }},\n",
    "    {\"$group\": {\"_id\": { \"lex\": \"$LexResources._id\", \"RefResource\": \"$RefResource\" }, \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "        \"_id\": \"$_id.lex\",\n",
    "        \"wordList\": \"$_id.RefResource.wordList\",\n",
    "        \"commonWords\": \"$count\"\n",
    "    }},\n",
    "    {\"$unwind\": \"$wordList\"},\n",
    "    {\"$project\": {\n",
    "        \"_id\": 1,\n",
    "        \"commonWords\": 1,\n",
    "        \"totWordsLex\": {\"$size\": \"$wordList\"}\n",
    "    }},\n",
    "    {\"$addFields\": {\"totalWordsTwitter\": countWords}},\n",
    "    {\"$project\": {\n",
    "        \"perc_presence_lex_res\": { \"$round\": [{\"$divide\": [ \"$commonWords\", \"$totWordsLex\" ] }, 6] },\n",
    "        \"perc_presence_twitter\": { \"$round\": [{\"$divide\": [ \"$commonWords\", \"$totalWordsTwitter\" ]}, 6] }\n",
    "    }}\n",
    "]\n",
    "\n",
    "results = sadnessCollections.aggregate(pipeline)\n",
    "\n",
    "# Genero i dati per l'istogramma\n",
    "istogramSadness_lexRes = []\n",
    "istogramSadness_twitter = []\n",
    "\n",
    "for data in results:\n",
    "    print(data)\n",
    "    log =math.log(data['perc_presence_lex_res'],10)\n",
    "    istogramSadness_lexRes.append([data['_id'], data['perc_presence_lex_res']])\n",
    "    log =math.log(data['perc_presence_twitter'],10)\n",
    "    istogramSadness_twitter.append([data['_id'], data['perc_presence_twitter']])\n",
    " \n",
    "# Create the pandas DataFrame\n",
    "dfSadness_lexRes = pd.DataFrame(istogramSadness_lexRes, columns = ['Risorsa', 'Percentuale'])\n",
    "dfSadness_twitter = pd.DataFrame(istogramSadness_twitter,columns = ['Risorsa', 'Percentuale'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Creazione istogramma per presenza parole nelle risorse lessicali\n",
    "\n",
    "dfSadness_lexRes.plot.bar(x='Risorsa')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Creazione istogramma per presenza parole nei twitter\n",
    "\n",
    "dfSadness_twitter.plot.bar(x='Risorsa')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connessione, creazione cursore e database\n",
    "\n",
    "conn = psycopg2.connect(database = \"maadb\", user = \"postgres\", password = \"admin\", host = \"localhost\", port = \"5432\")\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione tabella risorse lessicali sadness e inserimento\n",
    "cur.execute('''CREATE TABLE RISORSESADNESS\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    NOME             TEXT       NOT NULL,\n",
    "                    PAROLA           TEXT       NOT NULL\n",
    "                );\n",
    "            '''\n",
    ")\n",
    "\n",
    "nome = \"NRC_sadness\"\n",
    "for word in resourceSadnessNRC:\n",
    "    cur.execute(\"INSERT INTO RISORSESADNESS (NOME, PAROLA) VALUES(%s, %s)\",(nome,word))\n",
    "\n",
    "nome = \"sentisense_sadness\"\n",
    "for word in resourceSadnessSentisense:\n",
    "    cur.execute(\"INSERT INTO RISORSESADNESS (NOME, PAROLA) VALUES(%s, %s)\",(nome,word))\n",
    "\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione tabelle per le parole, emoji, emoticon e hashtag\n",
    "\n",
    "cur.execute('''CREATE TABLE PAROLESADNESS\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    PAROLA           TEXT       NOT NULL,\n",
    "                    NRC_sadness      BOOL       NOT NULL DEFAULT FALSE,\n",
    "                    sentisense_sadness      BOOL       NOT NULL DEFAULT FALSE,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ")   \n",
    "\n",
    "cur.execute('''CREATE TABLE EMOJISADNESS\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    EMOJI       TEXT       NOT NULL,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ") \n",
    "\n",
    "cur.execute('''CREATE TABLE EMOTICONSSADNESS\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    EMOTICONS       TEXT       NOT NULL,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ") \n",
    "\n",
    "cur.execute('''CREATE TABLE HASHTAGSADNESS\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    HASHTAG     TEXT       NOT NULL,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ") \n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inserimento hashtag, emoticon ed emoji\n",
    "\n",
    "for hashTag in hashtagSadness:\n",
    "    cur.execute(\"INSERT INTO HASHTAGSADNESS (HASHTAG,FREQUENCY) VALUES(%s,%s)\",(hashTag,0))\n",
    "\n",
    "for emoticons in emoticonSadness:\n",
    "    cur.execute(\"INSERT INTO EMOTICONSSADNESS (EMOTICONS,FREQUENCY) VALUES(%s,%s)\",(emoticons,0))\n",
    "\n",
    "for emojis in emojiSadness:\n",
    "    cur.execute(\"INSERT INTO EMOJISADNESS (EMOJI,FREQUENCY) VALUES(%s,%s)\",(emojis,0))\n",
    "\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inserimento parole in db\n",
    "for word in sadnessListWord:\n",
    "    cur.execute(\"INSERT INTO PAROLESADNESS (PAROLA,NRC_sadness,sentisense_sadness,FREQUENCY) VALUES(%s,%s,%s,%s)\",(word,False,False,0))\n",
    "\n",
    "conn.commit()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze hashtag\n",
    "cur.execute(\"update HASHTAGSADNESS as ha1 set frequency = (select count(*) from HASHTAGSADNESS ha2 where ha1.hashtag = ha2.hashtag);\")\n",
    "cur.execute(\"delete from HASHTAGSADNESS as ha1 where exists (Select 1 From HASHTAGSADNESS as ha2 where ha1.hashtag = ha2.hashtag and ha1.frequency = ha2.frequency and ha2.ctid > ha1.ctid);\")\n",
    "\n",
    "conn.commit()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze emoji\n",
    "cur.execute(\"update EMOJISADNESS as ha1 set frequency = (select count(*) from EMOJISADNESS ha2 where ha1.emoji = ha2.emoji);\")\n",
    "cur.execute(\"delete from EMOJISADNESS as ha1 where exists (Select 1 From EMOJISADNESS as ha2 where ha1.emoji = ha2.emoji and ha1.frequency = ha2.frequency and ha2.ctid > ha1.ctid);\")\n",
    "\n",
    "conn.commit() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze emoticons\n",
    "cur.execute(\"update EMOTICONSSADNESS as ha1 set frequency = (select count(*) from EMOTICONSSADNESS ha2 where ha1.emoticons = ha2.emoticons);\")\n",
    "cur.execute(\"delete from EMOTICONSSADNESS as ha1 where exists (Select 1 From EMOTICONSSADNESS as ha2 where ha1.emoticons = ha2.emoticons and ha1.frequency = ha2.frequency and ha2.ctid > ha1.ctid);\")\n",
    "\n",
    "conn.commit()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze parole\n",
    "cur.execute(\"select count(*),parola from PAROLESADNESS group by parola\")\n",
    "result = []\n",
    "for word in cur.fetchall():\n",
    "    result.append(word) \n",
    "\n",
    "cur.execute(\"truncate table PAROLESADNESS;\")\n",
    "cur.execute(\"ALTER SEQUENCE parolesadness_id_seq RESTART WITH 1;\")\n",
    "\n",
    "for couple in result:\n",
    "    cur.execute(\"INSERT INTO PAROLESADNESS (PAROLA,NRC_sadness,sentisense_sadness,FREQUENCY) VALUES(%s,%s,%s,%s)\",(couple[1],False,False,couple[0]))\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggiornamento presenza parola nelle risorse lessicali\n",
    "cur.execute(\"UPDATE PAROLESADNESS SET NRC_sadness = TRUE WHERE parola IN (select p.parola from PAROLESADNESS as p LEFT OUTER JOIN RISORSESADNESS as r ON p.parola = r.parola WHERE r.nome = 'NRC_sadness')\")\n",
    "cur.execute(\"UPDATE PAROLESADNESS SET sentisense_sadness = TRUE WHERE parola IN (select p.parola from PAROLESADNESS as p LEFT OUTER JOIN RISORSESADNESS as r ON p.parola = r.parola WHERE r.nome = 'sentisense_sadness')\")\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvataggio in un txt delle parole non presenti in nessuna risorsa lessicale\n",
    "\n",
    "wordsMongo = sadnessCollections.find({\"LexResources\": []}, {\"_id\": 0, \"freq\": 0, \"LexResources\": 0})\n",
    "words = []\n",
    "\n",
    "for word in wordsMongo:\n",
    "    words.append(word['word'])\n",
    "\n",
    "with open('Nuove Parole/sadness.txt', 'w',encoding=\"utf8\") as f:\n",
    "    for word in words:\n",
    "        f.write(\"%s\\n\" % word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trattamento dataset Surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lista di righe, ogni riga una tweet surprise\n",
    "file = open(\"Dataset/dataset_dt_surprise_60k.txt\", 'r',encoding=\"utf8\")\n",
    "surprise = []\n",
    "hashtagSurprise = []\n",
    "emoticonSurprise = []\n",
    "emojiSurprise = []\n",
    "\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "    riga = re.sub(r'URL', '',riga)\n",
    "    riga = re.sub(r'USERNAME', '',riga)\n",
    "\n",
    "     #array di hashtag singoli\n",
    "    hashTag = (re.findall('#[a-zA-Z0-9]+', riga))\n",
    "\n",
    "    #generiamo l'array di hashtag senza hashtag\n",
    "    for h in hashTag:\n",
    "        hashtagSurprise.append(h.replace(\"#\",\"\"))\n",
    "\n",
    "    #Rimuoviamo glin hashtag dalla riga\n",
    "    riga = riga.replace('#',\"\")\n",
    "\n",
    "    surprise.append(riga)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(surprise)):\n",
    "    #Controllo se l'emoticons c'è e in caso la rimuovo\n",
    "    for singleEmoticon in emoticons:\n",
    "        emoticonJoy = re.findall(re.escape(singleEmoticon),surprise[i])\n",
    "        if(len(emoticonsFind) != 0):\n",
    "            emoticonSurprise.extend(emoticonsFind)\n",
    "            surprise[i] = surprise[i].replace(singleEmoticon,\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(surprise)):\n",
    "    #preleviamo le emoji e le rimuoviamo dal testo\n",
    "    data = regex.findall(r'\\X', surprise[i])\n",
    "    for word in data:\n",
    "        if any(char in emoji.UNICODE_EMOJI['en'] for char in word):\n",
    "            emojiSurprise = [].append(word)\n",
    "            surprise[i] = surprise[i].replace(word, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processamento slang prima di punteggiatura e lowercase\n",
    "with open(\"Slangs/slangs_before.json\") as f:\n",
    "    slangsBefore = json.load(f)\n",
    "    for i in range(len(surprise)):\n",
    "        for word in surprise[i].split():\n",
    "            indexOfWord = surprise[i].find(word)\n",
    "            if slangsBefore.get(word) is None:\n",
    "                if slangsBefore.get(word.lower()) is None:\n",
    "                    if slangsBefore.get(word.upper()) is None:\n",
    "                        continue\n",
    "                    else:\n",
    "                        surprise[i] = surprise[i][:indexOfWord] + slangsBefore.get(word.upper()) + surprise[i][indexOfWord+len(word):]\n",
    "                else:\n",
    "                    surprise[i] = surprise[i][:indexOfWord] + slangsBefore.get(word.lower()) + surprise[i][indexOfWord+len(word):]\n",
    "            else:\n",
    "                surprise[i] = surprise[i][:indexOfWord] + slangsBefore.get(word) + surprise[i][indexOfWord+len(word):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(surprise)):\n",
    "    #Trattamento punteggiatura\n",
    "    line = re.compile(\"[\" + re.escape(\"\".join(puntuaction)) + \"]\")\n",
    "    surprise[i] = re.sub(line, \"\", surprise[i])\n",
    "\n",
    "    #Rendiamo la farse in minuscolo\n",
    "    surprise[i] = surprise[i].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizzazione di tutte le frasi\n",
    "surpriseTokenized = []\n",
    "for linea in surprise:\n",
    "    result = word_tokenize(linea)\n",
    "    surpriseTokenized.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Esecuzione della POS tagging per ogni riga della matrice\n",
    "for i in range(len(surpriseTokenized)):\n",
    "    surpriseTokenized[i] = nltk.pos_tag(surpriseTokenized[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Slangs vengono modificati con la loro definizione\n",
    "# with open(\"Slangs/slangs.json\", \"r\", encoding='utf-8') as f:\n",
    "#     slangs = json.load(f)\n",
    "#     for row in range(len(surpriseTokenized)):\n",
    "#         for column in range(len(surpriseTokenized[row])):\n",
    "#             if surpriseTokenized[row][column] in slangs:\n",
    "#                 surpriseTokenized[row][column] = slangs.get(surpriseTokenized[row][column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming\n",
    "#ps = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for line in surpriseTokenized:\n",
    "    for i in range(len(line)):\n",
    "        tupla = [lemmatizer.lemmatize(line[i][0]),line[i][1]]\n",
    "        line[i] = tupla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trattamnento Stop word dopo Stemming\n",
    "stop_words = set(stopwords.words('english'))\n",
    "surpriseTokStoped = []\n",
    "for line in surpriseTokenized:\n",
    "    filtred_word = []\n",
    "    for row in line:\n",
    "        if  row[0] not in stop_words:\n",
    "            filtred_word.append(row) \n",
    "    surpriseTokStoped.append(filtred_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surpriseListWord = []\n",
    "surpriseListTok = []\n",
    "for line in surpriseTokStoped:\n",
    "    for row in line:\n",
    "        surpriseListWord.append(row[0])\n",
    "        surpriseListTok.append(row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lavorazione risorse lessicali\n",
    "resourceSurprise = []\n",
    "file = open(\"Risorse lessicali\\\\Surprise\\\\NRC_surprise.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        if(riga not in resourceSurprise):\n",
    "            resourceSurprise.append(line)\n",
    "\n",
    "file.close()\n",
    "\n",
    "file = open(\"Risorse lessicali\\Surprise\\sentisense_surprise.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        if(riga not in resourceSurprise):\n",
    "            resourceSurprise.append(line)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trattamento dataset Trust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lista di righe, ogni riga una tweet trust\n",
    "file = open(\"Dataset/dataset_dt_trust_60k.txt\", 'r',encoding=\"utf8\")\n",
    "trust = []\n",
    "hashtagTrust = []\n",
    "emoticonTrust = []\n",
    "emojiTrust = []\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "    riga = re.sub(r'URL', '',riga)\n",
    "    riga = re.sub(r'USERNAME', '',riga)\n",
    "    \n",
    "    #array di hashtag singoli\n",
    "    hashTag = (re.findall('#[a-zA-Z0-9]+', riga))\n",
    "    \n",
    "    #generiamo l'array di hashtag senza hashtag\n",
    "    for h in hashTag:\n",
    "        hashtagTrust.append(h.replace(\"#\",\"\"))\n",
    "\n",
    "    #Rimuoviamo glin hashtag dalla riga\n",
    "    riga = riga.replace('#',\"\")\n",
    "\n",
    "    trust.append(riga)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(trust)):\n",
    "    #Controllo se l'emoticons c'è e in caso la rimuovo\n",
    "    for singleEmoticon in emoticons:\n",
    "        emoticonJoy = re.findall(re.escape(singleEmoticon),trust[i])\n",
    "        if(len(emoticonsFind) != 0):\n",
    "            emoticonTrust.extend(emoticonsFind)\n",
    "            trust[i] = trust[i].replace(singleEmoticon,\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(trust)):\n",
    "    #preleviamo le emoji e le rimuoviamo dal testo\n",
    "    data = regex.findall(r'\\X', trust[i])\n",
    "    for word in data:\n",
    "        if any(char in emoji.UNICODE_EMOJI['en'] for char in word):\n",
    "            emojiTrust = [].append(word)\n",
    "            trust[i] = trust[i].replace(word, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processamento slang prima di punteggiatura e lowercase\n",
    "with open(\"Slangs/slangs_before.json\") as f:\n",
    "    slangsBefore = json.load(f)\n",
    "    for i in range(len(trust)):\n",
    "        for word in trust[i].split():\n",
    "            indexOfWord = trust[i].find(word)\n",
    "            if slangsBefore.get(word) is None:\n",
    "                if slangsBefore.get(word.lower()) is None:\n",
    "                    if slangsBefore.get(word.upper()) is None:\n",
    "                        continue\n",
    "                    else:\n",
    "                        trust[i] = trust[i][:indexOfWord] + slangsBefore.get(word.upper()) + trust[i][indexOfWord+len(word):]\n",
    "                else:\n",
    "                    trust[i] = trust[i][:indexOfWord] + slangsBefore.get(word.lower()) + trust[i][indexOfWord+len(word):]\n",
    "            else:\n",
    "                trust[i] = trust[i][:indexOfWord] + slangsBefore.get(word) + trust[i][indexOfWord+len(word):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizzazione di tutte le frasi\n",
    "trustTokenized = []\n",
    "for linea in trust:\n",
    "    result = word_tokenize(linea)\n",
    "    trustTokenized.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Esecuzione della POS tagging per ogni riga della matrice\n",
    "for i in range(len(trustTokenized)):\n",
    "    trustTokenized[i] = nltk.pos_tag(trustTokenized[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming\n",
    "#ps = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for line in trustTokenized:\n",
    "    for i in range(len(line)):\n",
    "        tupla = [lemmatizer.lemmatize(line[i][0]),line[i][1]]\n",
    "        line[i] = tupla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trattamnento Stop word dopo Stemming\n",
    "stop_words = set(stopwords.words('english'))\n",
    "trustTokStoped = []\n",
    "for line in trustTokenized:\n",
    "    filtred_word = []\n",
    "    for row in line:\n",
    "        if  row[0] not in stop_words:\n",
    "            filtred_word.append(row) \n",
    "    trustTokStoped.append(filtred_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trustListWord = []\n",
    "trustListTok = []\n",
    "for line in trustTokStoped:\n",
    "    for row in line:\n",
    "        trustListWord.append(row[0])\n",
    "        trustListTok.append(row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lavorazione risorse lessicali\n",
    "resourceTrust = []\n",
    "file = open(\"Risorse lessicali\\\\Trust\\\\NRC_trust.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        if(riga not in resourceTrust):\n",
    "            resourceTrust.append(line)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Slangs vengono modificati con la loro definizione\n",
    "# with open(\"Slangs/slangs.json\", \"r\", encoding='utf-8') as f:\n",
    "#     slangs = json.load(f)\n",
    "#     for row in range(len(trustTokenized)):\n",
    "#         for column in range(len(trustTokenized[row])):\n",
    "#             if trustTokenized[row][column] in slangs:\n",
    "#                 trustTokenized[row][column] = slangs.get(trustTokenized[row][column])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "727cd1b8d9e0a8fe67ccb60934071f3fe824c12f3f18648f9575d2adb7fe198a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
