{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lores\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\lores\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\lores\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\lores\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\lores\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Import per il resto delle celle\n",
    "import re\n",
    "import emoji\n",
    "import regex\n",
    "import nltk\n",
    "import json\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from collections import Counter\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "import pymongo\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import psycopg2\n",
    "%matplotlib inline\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "emoticonsList = [':‑)',':)',':-]',':]',':->',':>','8-)','8)',':-}',':}',':o)',':c)',':^)','=]','=)',':‑D',':D','8‑D','8D','=D','=3','B^D','c:',\n",
    "'C:','x‑D','xD','X‑D','XD',':-))',':‑(',':(',':‑c',':c',':‑<',':<',':‑[',':[',':-||','>:[',':{',':@',':('',;(',\":'‑(\",\":'(\",':=(',\":'‑)\",\":')\",\n",
    "':\"D',\"D‑':\",'D:<','D:','D8','D;','D=','DX',':‑O',':O',':‑o',':o',':-0','8‑0','>:O','=O','=o','=0',':-3',':3','=3','x3','X3','>:3',':-*',\n",
    "':*',':×',';‑)',';)','*-)','*)',';‑]',';]',';^)',';>',':‑,',';D',';3',':‑P',':P','X‑P','XP','x‑p','xp',':‑p',':p',':‑Þ',':Þ',':‑þ',':þ',':‑b',':b',\n",
    "'d:','=p','>:P',':-/',':/',':‑.',\">:/\",'=/',':L','=L',':S',':‑|',':|',':$','://)','://3',':‑X',':X',':‑#',':#',':‑&',':&','O:‑)','O:)','0:‑3'\n",
    ",'0:3','0:‑)','0:)','0;^)','>:‑)','>:)','}:‑)','}:)','3:‑)','3:)','>;‑)','>;)','>:3',';3','|;‑)','|‑O','B-)',':‑J','#‑)','%‑)','%)',':‑###..'\n",
    ",':###..','<:‑|',\"',:-|\",\"',:-l\",':E','8-X','8=X','x-3','x=3','~:>','v.v','._.','._.;','QQ  ','qq','Qq','X_X','x_x','+_+','X_x','x_X'\n",
    "'<_<','>_>','<.<','>.>','O_O','o_o','O-O','o‑o','O_o','o_O','>.<','>_<','^5','o/\\o','>_>^ ^<_<','V.v.V','V=(° °)=V','(^^^)','(::[]::)',\n",
    "'(o)(o)','( • )( • )','[̲̅$̲̅(̲̅1̲̅)̲̅$̲̅]','[̲̅$̲̅(̲̅5̲̅)̲̅$̲̅]','[̲̅$̲̅(̲̅10)̲̅$̲̅]','[̲̅$̲̅(̲̅100)̲̅$̲̅]','( ͡° ͜ʖ ͡°)','ヽ༼ຈل͜ຈ༽ﾉ','(๑ˇεˇ๑)','(◕‿◕✿)','( ༎ຶ ۝ ༎ຶ )','(=ʘᆽʘ=)∫','ʕ •ᴥ•ʔ',\n",
    "'(>_<)','(>_<)>','(>w<)',\"(';')\",'(^_^;)','(-_-;)','(~_~;)','(・.・;)','(・_・;)','^^;','(・・;)','^_^;','(#^.^#)'\n",
    ",'(^^;)','(⁄ ⁄•⁄ω⁄•⁄ ⁄)','(-.-)y-°°°','(^.^)y-.o○','(-_-)zzz','(^_-)-☆','(^_-)','((+_+))','(+o+)','(°°)','(°-°) (°.°)',\n",
    "'(°_°)','(°_°>)','(°レ°)','(o|o)','<(｀^´)>','^_^','(°o°)','(^_^)/','(^O^)／','(^o^)／','(^^)/','(≧∇≦)/','(/◕ヮ◕)/','(^o^)丿','∩(·ω·)∩',\n",
    "'(·ω·)','^ω^','_(._.)_','_(_^_)_','<(_ _)>','<m(__)m>','m(__)m','m(_ _)m','(凸ಠ益ಠ)凸',\"('_')\",'(/_;)','(T_T)','(;_;)','(;_;','(;_:)','(;O;)'\n",
    ",'(:_;)','(ToT)','(Ｔ▽Ｔ)',';_;',';-;',';n;',';;','Q.Q','T.T','TnT','QQ','Q_Q',\n",
    "'(ー_ー)!!','(-.-)','(-_-)','(一一)','(；一_一)','(=_=)','(=^・^=)','(=^・・^=)','=^_^=','(..)','(._.)',\n",
    "'.o○','○o.','_旦~~','( ^^)','_U~~','( ^^)','☆ミ','☆彡','>°)))彡','(Q))',\n",
    "'><ヨヨ (°))<<','>°))))彡','<°)))彡','>°))彡','<+','))><<','<*))','>=<','<コ:彡',\n",
    "'Ｃ:.ミ','~>°)～～～','～°·_·°～','(°°)～','●～*','￣|○',':3ミ','^m^','(・・?','(?_?)','>^_^<','<^!^>',\n",
    "'^/^','（*^_^*','§^.^§','(^<^)','(^.^)','(^ム^)','(^·^)','(^.^)','(^_^.)','(^_^)','(^^)','(^J^)',\n",
    "'(*^.^*)','^_^','(#^.^#)','（^—^）','(^^)/~~~','(^_^)/~','(;_;)/~~~','(^.^)/~~~',\n",
    "'(-_-)/~~~','($··)/~~~','(@^^)/~~~','(T_T)/~~~','(V)o￥o(V)','＼(~o~)／','＼(^o^)／','＼(-o-)／','ヽ(^。^)ノ',\n",
    "'ヽ(^o^)丿','(*^0^*)','(*_*)','(*_*;','(+_+) (@_@)','(@_@。','(＠_＠;)',\n",
    " '＼(◎o◎)／！','(*^^)v','(^^)v','(^_^)v','（’-’*) (＾ｖ＾)','(＾▽＾)','(・∀・)','(´∀`)','(⌒▽⌒）',\n",
    " '＼(^o^)／','\\(^o^)/','(~o~)','(~_~)','(^^ゞ','ˊ＿>ˋ','(p_-)', '(-_q)','((d[-_-]b))','(-\"-)','(ーー゛)',\n",
    " '(^_^メ)', '(-_-メ)','(~_~メ)','(－－〆)','(・へ・)','(｀´)'\n",
    " , '<`～´>','<`ヘ´>','(ーー;)','(^0_0^)','(＾ｖ＾)', '(＾ｕ＾)','(＾◇＾)','( ^)o(^ )','(^O^)','(^o^)','(^○^)',')^o^(',\n",
    " '(*^▽^*)','(￣ー￣)','(￣□￣;)', '°o°','°O°',':O','o_O','o_0', 'o.O','(o.o)','oO','(°◇°)','（ ﾟ Дﾟ)','(*￣m￣)',\n",
    " 'ヽ(´ー｀)┌','¯\\_(ツ)_/¯','¯\\(°_o)/¯','(´･ω･`)', '(‘A`)','(づ￣ ³￣)づ','(*^3^)/~☆','(︶｡︶✽)','(-_-) zzz'\n",
    " , 'uwu','UwU','OWO','OwO','～°·_·°～', '(°°)～',':3ミ','(´･ω･`)','(`･ω･´)','(｀-´)>', '（　´_ゝ`）'] \n",
    "\n",
    "puntuaction = ['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '.',\n",
    "           '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', \n",
    "           '`', '{', '|', '}', '~', '»', '«', '“', '”', \"-\", \"...\",\"..\",\".,\",\",.\"]\n",
    "\n",
    "\n",
    "mongoClient = pymongo.MongoClient(\"mongodb://admin:admin@localhost:27017/\")\n",
    "mydb = mongoClient[\"maadb\"]\n",
    "\n",
    "collectionTwitter = mydb[\"Twitter\"] #Questa é per la collezione dei emoji emoticon hashtag per ogni emozione\n",
    "collectionAnger = mydb[\"CollectionAnger\"]\n",
    "collectionAnticipation = mydb[\"CollectionAnticipation\"]\n",
    "collectionDisgust = mydb[\"CollectionDisgust\"]\n",
    "collectionFear = mydb[\"CollectionFear\"]\n",
    "collectionHope = mydb[\"CollectionHope\"]\n",
    "collectionAJoy = mydb[\"CollectionJoy\"]\n",
    "collectionSadness = mydb[\"CollectionSadness\"]\n",
    "collectionSurprise = mydb[\"CollectionSurprise\"]\n",
    "collectionTrust = mydb[\"CollectionTrust\"]\n",
    "\n",
    "angerCollections = mydb[\"anger\"] #Collezioni per solo i twitt di anger\n",
    "anticipationCollections = mydb[\"anticipation\"]\n",
    "disgustCollections = mydb[\"disgust\"]\n",
    "fearCollections = mydb[\"fear\"]\n",
    "joyCollections = mydb[\"joy\"]\n",
    "sadnessCollections = mydb[\"sadness\"]\n",
    "surpriseCollections = mydb[\"surprise\"]\n",
    "trustCollections = mydb[\"trust\"]\n",
    "resourcesCollection = mydb[\"Resources\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trattamento dataset Anger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lista di righe, ogni riga una tweet anger\n",
    "file = open(\"Dataset/dataset_dt_anger_60k.txt\", 'r',encoding=\"utf8\")\n",
    "anger = []\n",
    "hashtagAnger = []\n",
    "emoticonAnger = []\n",
    "emojiAnger = []\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    #Eliminazione URL e USERNAME\n",
    "    riga = re.sub(r'URL', '',riga)\n",
    "    riga = re.sub(r'USERNAME', '',riga)\n",
    "\n",
    "    #array di hashtag singoli\n",
    "    hashTag = (re.findall('#[a-zA-Z0-9]+', riga))\n",
    "\n",
    "     #generiamo l'array di hashtag senza hashtag\n",
    "    for h in hashTag:\n",
    "        hashtagAnger.append(h.replace(\"#\",\"\"))\n",
    "\n",
    "    #Rimuoviamo glin hashtag dalla riga\n",
    "    riga = riga.replace('#',\"\")\n",
    "\n",
    "    #Salvataggio frase\n",
    "    anger.append(riga)\n",
    "\n",
    "\n",
    "file.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(anger)):\n",
    "    #Controllo se l'emoticons c'è e in caso la rimuovo\n",
    "    for singleEmoticon in emoticonsList:\n",
    "        emoticonsFind = re.findall(re.escape(singleEmoticon),anger[i])\n",
    "        if(len(emoticonsFind) != 0):\n",
    "            emoticonAnger.extend(emoticonsFind)\n",
    "            anger[i] = anger[i].replace(singleEmoticon,\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(anger)):\n",
    "    #preleviamo le emoji e le rimuoviamo dal testo\n",
    "    data = regex.findall(r'\\X', anger[i])\n",
    "    for word in data:\n",
    "        if any(char in emoji.UNICODE_EMOJI['en'] for char in word):\n",
    "            emojiAnger.append(word)\n",
    "            anger[i] = anger[i].replace(word, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processamento slang \n",
    "with open(\"Slangs/slangs.json\") as f:\n",
    "    slangsBefore = json.load(f)\n",
    "    for i in range(len(anger)):\n",
    "        for word in anger[i].split():\n",
    "            indexOfWord = anger[i].find(word)\n",
    "            if slangsBefore.get(word) is None:\n",
    "                if slangsBefore.get(word.lower()) is None:\n",
    "                    if slangsBefore.get(word.upper()) is None:\n",
    "                        continue\n",
    "                    else:\n",
    "                        anger[i] = anger[i][:indexOfWord] + slangsBefore.get(word.upper()) + anger[i][indexOfWord+len(word):]\n",
    "                else:\n",
    "                    anger[i] = anger[i][:indexOfWord] + slangsBefore.get(word.lower()) + anger[i][indexOfWord+len(word):]\n",
    "            else:\n",
    "                anger[i] = anger[i][:indexOfWord] + slangsBefore.get(word) + anger[i][indexOfWord+len(word):]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizzazione di tutte le frasi\n",
    "angerTokenized = []\n",
    "for linea in anger:\n",
    "    result = word_tokenize(linea)\n",
    "    angerTokenized.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Esecuzione della POS tagging per ogni riga della matrice\n",
    "for i in range(len(angerTokenized)):\n",
    "    angerTokenized[i] = nltk.pos_tag(angerTokenized[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trattamento punteggiatura\n",
    "for line in angerTokenized:\n",
    "    newLine = line\n",
    "    listToRemove = []\n",
    "    for i in range(len(line)):\n",
    "        if line[i][0] in puntuaction:\n",
    "            listToRemove.append(line[i])\n",
    "    for element in listToRemove:\n",
    "        newLine.remove(element)\n",
    "    line = newLine\n",
    "\n",
    "linePuntuaction = re.compile(\"[\" + re.escape(\"\".join(puntuaction)) + \"]\")\n",
    "for line in angerTokenized:\n",
    "    for i in range(len(line)):\n",
    "        tupla = [(re.sub(linePuntuaction, \"\", line[i][0])),line[i][1]]\n",
    "        line[i] = tupla\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemming\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for line in angerTokenized:\n",
    "    for i in range(len(line)):\n",
    "        tupla = [lemmatizer.lemmatize(line[i][0]),line[i][1]]\n",
    "        line[i] = tupla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trattamnento Stop word dopo Lemming\n",
    "stop_words = set(stopwords.words('english'))\n",
    "angerTokStoped = []\n",
    "for line in angerTokenized:\n",
    "    filtred_word = []\n",
    "    for row in line:\n",
    "        if  row[0] not in stop_words:\n",
    "            filtred_word.append(row) \n",
    "    angerTokStoped.append(filtred_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#liste singole di parole e token\n",
    "angerListWord = []\n",
    "angerListTok = []\n",
    "for line in angerTokStoped:\n",
    "    for row in line:\n",
    "        if row[0] != '':\n",
    "            angerListWord.append(row[0])\n",
    "        angerListTok.append(row[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lavorazione risorse lessicali\n",
    "resourceAngerEmoSN = []\n",
    "resourceAngerNRC = []\n",
    "resourceAngerSentisense = []\n",
    "\n",
    "#Lista di righe, ogni riga una tweet anger\n",
    "file = open(\"Risorse lessicali\\Anger\\EmoSN_anger.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        resourceAngerEmoSN.append(riga)\n",
    "\n",
    "file.close()\n",
    "\n",
    "file = open(\"Risorse lessicali\\\\Anger\\\\NRC_anger.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        resourceAngerNRC.append(riga)\n",
    "\n",
    "file.close()\n",
    "\n",
    "file = open(\"Risorse lessicali\\Anger\\sentisense_anger.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        resourceAngerSentisense.append(riga)\n",
    "\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.InsertOneResult at 0x28ffd4b97f0>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Caricare le risorse lessicali \n",
    "\n",
    "AngerEmoSN = {\n",
    "    '_id': 'EmoSN_anger',\n",
    "    'sentiment': 'anger', \n",
    "    'wordList': resourceAngerEmoSN\n",
    "}\n",
    "\n",
    "AngerNRC = {\n",
    "    '_id': 'NRC_anger',\n",
    "    'sentiment': 'anger',  \n",
    "    'wordList': resourceAngerNRC\n",
    "}\n",
    "\n",
    "AngerSentisense = {\n",
    "    '_id': 'sentisense_anger',\n",
    "    'sentiment': 'anger',\n",
    "    'wordList': resourceAngerSentisense\n",
    "}\n",
    "\n",
    "resourcesCollection.insert_one(AngerEmoSN)\n",
    "resourcesCollection.insert_one(AngerNRC)\n",
    "resourcesCollection.insert_one(AngerSentisense)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione database e collezioni\n",
    "# Inserimento singoli documenti contententi i tweet anger\n",
    "wordsString = {}\n",
    "\n",
    "for i in range(len(angerListWord)):\n",
    "    wordsString = {\n",
    "        \"lemma\": angerListWord[i],\n",
    "        \"POS\": angerListTok[i]\n",
    "    }\n",
    "    angerTwit={\"word\":wordsString}\n",
    "    angerCollections.insert_one(angerTwit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inserimento collezione Twitter con hashtags, emoticons ed emoji \n",
    "angerInsert = {\n",
    "    \"sentiment\":\"anger\",\n",
    "    \"hashtags\": hashtagAnger,\n",
    "    \"emoticons\": emoticonAnger,\n",
    "    \"emoji\": emojiAnger\n",
    "}\n",
    "\n",
    "#inserimento\n",
    "x = collectionAnger.insert_one(angerInsert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.command_cursor.CommandCursor at 0x28ffd4b95b0>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Raggruppa i documenti di input in base all'espressione _id specificata e per ciascun raggruppamento distinto genera un documento\n",
    "pipeline = [\n",
    "    {\"$group\": {\"_id\": \"$word.lemma\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"word\": \"$_id\",\n",
    "      \"freq\": \"$count\"\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"freq\": -1\n",
    "    }},\n",
    "    {\"$out\": \"anger\"}\n",
    "]\n",
    "\n",
    "#post processing per renderlo valido \n",
    "angerCollections.aggregate(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<wordcloud.wordcloud.WordCloud at 0x28f8859aca0>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creazione Word Clouds per le parole\n",
    "data = {}\n",
    "for document in angerCollections.find().limit(50):\n",
    "    data[document['word']] = document['freq']\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2)\n",
    "wc.fit_words(data)\n",
    "wc.to_file('Word Clouds/Anger/angerWords.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.UpdateResult at 0x28fec81e3d0>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Conteggio degli hashtags\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$hashtags\"},\n",
    "    {\"$group\": {\"_id\": \"$hashtags\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"word\": \"$_id\",\n",
    "      \"count\": 1\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"count\": -1\n",
    "    }}\n",
    "]\n",
    "\n",
    "hashtagsCount = collectionAnger.aggregate(pipeline)\n",
    "\n",
    "data = {}\n",
    "for document in hashtagsCount:\n",
    "    data[document['word']] = document['count']\n",
    "\n",
    "collectionAnger.update_one(\n",
    "  {\"sentiment\": \"anger\"},\n",
    "  {\"$set\": {\"hashtags\": data}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<wordcloud.wordcloud.WordCloud at 0x28f8859a9a0>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creazione Word Clouds per gli hashtags\n",
    "\n",
    "hashtags = collectionAnger.distinct(\"hashtags\")\n",
    "\n",
    "hashtagsLimited = {}\n",
    "for (word,count) in [x for x in hashtags[0].items()][:50]:\n",
    "    hashtagsLimited[word] = count\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2)\n",
    "wc.fit_words(hashtagsLimited)\n",
    "wc.to_file('Word Clouds/Anger/angerHashtags.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.UpdateResult at 0x28fec81e310>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Conteggio delle emoji\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$emoji\"},\n",
    "    {\"$group\": {\"_id\": \"$emoji\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"emoji\": \"$_id\",\n",
    "      \"count\": 1\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"count\": -1\n",
    "    }}\n",
    "]\n",
    "\n",
    "emojiCount = collectionAnger.aggregate(pipeline)\n",
    "\n",
    "data = {}\n",
    "for document in emojiCount:\n",
    "    data[document['emoji']] = document['count']\n",
    "\n",
    "collectionAnger.update_one(\n",
    "  {\"sentiment\": \"anger\"},\n",
    "  {\"$set\": {\"emoji\": data}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<wordcloud.wordcloud.WordCloud at 0x28ffd4b9790>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creazione Word Clouds per le emoji\n",
    "\n",
    "emojis = collectionAnger.distinct(\"emoji\")\n",
    "\n",
    "emojisLimited = {}\n",
    "for (word,count) in [x for x in emojis[0].items()][:50]:\n",
    "    emojisLimited[word] = count\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2, font_path='Font Emoji\\Symbola.otf')\n",
    "wc.fit_words(emojisLimited)\n",
    "wc.to_file('Word Clouds/Anger/angerEmojis.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.UpdateResult at 0x28fec81e970>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Conteggio degli emoticons\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$emoticons\"},\n",
    "    {\"$group\": {\"_id\": \"$emoticons\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"emoticons\": \"$_id\",\n",
    "      \"count\": 1\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"count\": -1\n",
    "    }}\n",
    "]\n",
    "\n",
    "emoticonsCount = collectionAnger.aggregate(pipeline)\n",
    "\n",
    "data = {}\n",
    "for document in emoticonsCount:\n",
    "    data[document['emoticons']] = document['count']\n",
    "\n",
    "collectionAnger.update_one(\n",
    "  {\"sentiment\": \"anger\"},\n",
    "  {\"$set\": {\"emoticons\": data}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<wordcloud.wordcloud.WordCloud at 0x28ffd4b9220>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creazione Word Clouds per le emoticons\n",
    "\n",
    "emoticons = collectionAnger.distinct(\"emoticons\")\n",
    "\n",
    "emoticonsLimited = {}\n",
    "for (word,count) in [x for x in emoticons[0].items()][:50]:\n",
    "    emoticonsLimited[word] = count\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2)\n",
    "wc.fit_words(emoticonsLimited)\n",
    "wc.to_file('Word Clouds/Anger/angerEmoticons.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.command_cursor.CommandCursor at 0x28fec81e7f0>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pipeline di aggregazione per inserire la risorsa lessicale a cui appartiene la parola\n",
    "\n",
    "pipeline = [\n",
    "    {\"$lookup\": {\n",
    "        \"from\": \"Resources\",\n",
    "        \"localField\": \"word\",\n",
    "        \"foreignField\": \"wordList\",\n",
    "        \"pipeline\": [\n",
    "            {\"$match\": {\"$expr\": {\"$regexMatch\": {\"input\": \"$_id\", \"regex\": \"anger\"}}}}\n",
    "        ],\n",
    "        \"as\": \"LexResources\"\n",
    "    }},\n",
    "    {\"$project\": {\n",
    "        \"resources.wordList\": 0,\n",
    "        \"resources.totNumberWords\": 0,\n",
    "        \"resources.sentiment\": 0,\n",
    "        \"LexResources.sentiment\": 0,\n",
    "        \"LexResources.totNumberWords\": 0,\n",
    "        \"LexResources.wordList\": 0\n",
    "    }},\n",
    "    {\"$out\": \"anger\"}\n",
    "]\n",
    "\n",
    "angerCollections.aggregate(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': 'sentisense_anger', 'perc_presence_lex_res': 0.545455, 'perc_presence_twitter': 5.9e-05}\n",
      "{'_id': 'NRC_anger', 'perc_presence_lex_res': 0.485164, 'perc_presence_twitter': 0.001193}\n",
      "{'_id': 'EmoSN_anger', 'perc_presence_lex_res': 0.567797, 'perc_presence_twitter': 0.000396}\n"
     ]
    }
   ],
   "source": [
    "#Pipeline di aggregazione per calcolare le percentuali richieste\n",
    "\n",
    "pipelineCount = [\n",
    "    {\"$group\": {\"_id\": \"null\", \"count\": { \"$sum\": \"$freq\" }}},\n",
    "    {\"$project\": {\n",
    "        \"_id\": 0,\n",
    "        \"count\": \"$count\"\n",
    "    }}\n",
    "]\n",
    "\n",
    "results = angerCollections.aggregate(pipelineCount)\n",
    "\n",
    "countWords = 0\n",
    "for data in results:\n",
    "    countWords = data['count']\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$LexResources\"},\n",
    "    {\"$lookup\": {\n",
    "        \"from\": \"Resources\",\n",
    "        \"localField\": \"LexResources._id\",\n",
    "        \"foreignField\": \"_id\",\n",
    "        \"as\": \"RefResource\"\n",
    "    }},\n",
    "    {\"$group\": {\"_id\": { \"lex\": \"$LexResources._id\", \"RefResource\": \"$RefResource\" }, \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "        \"_id\": \"$_id.lex\",\n",
    "        \"wordList\": \"$_id.RefResource.wordList\",\n",
    "        \"commonWords\": \"$count\"\n",
    "    }},\n",
    "    {\"$unwind\": \"$wordList\"},\n",
    "    {\"$project\": {\n",
    "        \"_id\": 1,\n",
    "        \"commonWords\": 1,\n",
    "        \"totWordsLex\": {\"$size\": \"$wordList\"}\n",
    "    }},\n",
    "    {\"$addFields\": {\"totalWordsTwitter\": countWords}},\n",
    "    {\"$project\": {\n",
    "        \"perc_presence_lex_res\": { \"$round\": [{\"$divide\": [ \"$commonWords\", \"$totWordsLex\" ] }, 6] },\n",
    "        \"perc_presence_twitter\": { \"$round\": [{\"$divide\": [ \"$commonWords\", \"$totalWordsTwitter\" ]}, 6] }\n",
    "    }}\n",
    "]\n",
    "\n",
    "results = angerCollections.aggregate(pipeline)\n",
    "\n",
    "# Genero i dati per l'istogramma\n",
    "istogramAnger_lexRes = []\n",
    "istogramAnger_twitter = []\n",
    "\n",
    "for data in results:\n",
    "    print(data)\n",
    "    log =math.log(data['perc_presence_lex_res'],10)\n",
    "    istogramAnger_lexRes.append([data['_id'], data['perc_presence_lex_res']])\n",
    "    log =math.log(data['perc_presence_twitter'],10)\n",
    "    istogramAnger_twitter.append([data['_id'], data['perc_presence_twitter']])\n",
    " \n",
    "# Create the pandas DataFrame\n",
    "dfAnger_lexRes = pd.DataFrame(istogramAnger_lexRes, columns = ['Risorsa', 'Percentuale'])\n",
    "dfAnger_twitter = pd.DataFrame(istogramAnger_twitter,columns = ['Risorsa', 'Percentuale'])      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Creazione istogramma per presenza parole nelle risorse lessicali\n",
    "\n",
    "dfAnger_lexRes.plot.bar(x='Risorsa')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Creazione istogramma per presenza parole nei twitter\n",
    "\n",
    "dfAnger_twitter.plot.bar(x='Risorsa')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connessione, creazione cursore e database\n",
    "\n",
    "conn = psycopg2.connect(database = \"maadb\", user = \"postgres\", password = \"admin\", host = \"localhost\", port = \"5432\")\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione tabella risorse lessicali anger e inserimento\n",
    "cur.execute('''CREATE TABLE RISORSEANGER\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    NOME             TEXT       NOT NULL,\n",
    "                    PAROLA           TEXT       NOT NULL\n",
    "                );\n",
    "            '''\n",
    ")\n",
    "nome = \"EmoSN_anger\"\n",
    "for word in resourceAngerEmoSN:\n",
    "    cur.execute(\"INSERT INTO RISORSEANGER (NOME, PAROLA) VALUES(%s, %s)\",(nome,word))\n",
    "\n",
    "nome = \"NRC_anger\"\n",
    "for word in resourceAngerNRC:\n",
    "    cur.execute(\"INSERT INTO RISORSEANGER (NOME, PAROLA) VALUES(%s, %s)\",(nome,word))\n",
    "\n",
    "nome = \"sentisense_anger\"\n",
    "for word in resourceAngerSentisense:\n",
    "    cur.execute(\"INSERT INTO RISORSEANGER (NOME, PAROLA) VALUES(%s, %s)\",(nome,word))\n",
    "\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione tabelle per le parole, emoji, emoticon e hashtag\n",
    "\n",
    "cur.execute('''CREATE TABLE PAROLEANGER\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    PAROLA           TEXT       NOT NULL, \n",
    "                    EmoSN_anger      BOOL       NOT NULL DEFAULT FALSE,\n",
    "                    NRC_anger      BOOL       NOT NULL DEFAULT FALSE,\n",
    "                    sentisense_anger      BOOL       NOT NULL DEFAULT FALSE,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ")   \n",
    "\n",
    "cur.execute('''CREATE TABLE EMOJIANGER\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    EMOJI       TEXT       NOT NULL,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ") \n",
    "\n",
    "cur.execute('''CREATE TABLE EMOTICONSANGER\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    EMOTICONS       TEXT       NOT NULL,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ") \n",
    "\n",
    "cur.execute('''CREATE TABLE HASHTAGANGER\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    HASHTAG     TEXT       NOT NULL,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ") \n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inserimento hashtag, emoticon ed emoji\n",
    "\n",
    "for hashTag in hashtagAnger:\n",
    "    cur.execute(\"INSERT INTO HASHTAGANGER (HASHTAG,FREQUENCY) VALUES(%s,%s)\",(hashTag,0))\n",
    "\n",
    "for emoticons in emoticonAnger:\n",
    "    cur.execute(\"INSERT INTO EMOTICONSANGER (EMOTICONS,FREQUENCY) VALUES(%s,%s)\",(emoticons,0))\n",
    "\n",
    "for emojis in emojiAnger:\n",
    "    cur.execute(\"INSERT INTO EMOJIANGER (EMOJI,FREQUENCY) VALUES(%s,%s)\",(emojis,0))\n",
    "\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inserimento parole in db\n",
    "for word in angerListWord:\n",
    "    cur.execute(\"INSERT INTO PAROLEANGER (PAROLA,EmoSN_anger,NRC_anger,sentisense_anger,FREQUENCY) VALUES(%s,%s,%s,%s,%s)\",(word,False,False,False,0))\n",
    "\n",
    "conn.commit()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze hashtag\n",
    "cur.execute(\"update hashtaganger as ha1 set frequency = (select count(*) from hashtaganger ha2 where ha1.hashtag = ha2.hashtag);\")\n",
    "cur.execute(\"delete from hashtaganger as ha1 where exists (Select 1 From hashtaganger as ha2 where ha1.hashtag = ha2.hashtag and ha1.frequency = ha2.frequency and ha2.ctid > ha1.ctid);\")\n",
    "\n",
    "conn.commit()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze emoji\n",
    "cur.execute(\"update EMOJIANGER as ha1 set frequency = (select count(*) from EMOJIANGER ha2 where ha1.emoji = ha2.emoji);\")\n",
    "cur.execute(\"delete from EMOJIANGER as ha1 where exists (Select 1 From EMOJIANGER as ha2 where ha1.emoji = ha2.emoji and ha1.frequency = ha2.frequency and ha2.ctid > ha1.ctid);\")\n",
    "\n",
    "conn.commit()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze emoticons\n",
    "cur.execute(\"update EMOTICONSANGER as ha1 set frequency = (select count(*) from EMOTICONSANGER ha2 where ha1.emoticons = ha2.emoticons);\")\n",
    "cur.execute(\"delete from EMOTICONSANGER as ha1 where exists (Select 1 From EMOTICONSANGER as ha2 where ha1.emoticons = ha2.emoticons and ha1.frequency = ha2.frequency and ha2.ctid > ha1.ctid);\")\n",
    "\n",
    "conn.commit()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze parole (ci mette più di 100 minuti)\n",
    "\"\"\"\n",
    "cur.execute(\"update PAROLEANGER as ha1 set frequency = (select count(*) from PAROLEANGER ha2 where ha1.parola = ha2.parola);\")\n",
    "cur.execute(\"delete from PAROLEANGER as ha1 where exists (Select 1 From PAROLEANGER as ha2 where ha1.parola = ha2.parola and ha1.frequency = ha2.frequency and ha2.ctid > ha1.ctid);\")\n",
    "\n",
    "conn.commit()  \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze parole\n",
    "cur.execute(\"select count(*),parola from paroleanger group by parola\")\n",
    "result = []\n",
    "for word in cur.fetchall():\n",
    "    result.append(word) \n",
    "\n",
    "cur.execute(\"truncate table paroleanger;\")\n",
    "cur.execute(\"ALTER SEQUENCE paroleanger_id_seq RESTART WITH 1;\")\n",
    "\n",
    "for couple in result:\n",
    "    cur.execute(\"INSERT INTO PAROLEANGER (PAROLA,EmoSN_anger,NRC_anger,sentisense_anger,FREQUENCY) VALUES(%s,%s,%s,%s,%s)\",(couple[1],False,False,False,couple[0]))\n",
    "\n",
    "conn.commit()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggiornamento presenza parola nelle risorse lessicali\n",
    "cur.execute(\"UPDATE paroleanger SET EmoSN_anger = TRUE WHERE parola IN (select p.parola from paroleanger as p LEFT OUTER JOIN risorseanger as r ON p.parola = r.parola WHERE r.nome = 'EmoSN_anger')\")\n",
    "cur.execute(\"UPDATE paroleanger SET NRC_ANGER = TRUE WHERE parola IN (select p.parola from paroleanger as p LEFT OUTER JOIN risorseanger as r ON p.parola = r.parola WHERE r.nome = 'NRC_anger')\")\n",
    "cur.execute(\"UPDATE paroleanger SET sentisense_anger = TRUE WHERE parola IN (select p.parola from paroleanger as p LEFT OUTER JOIN risorseanger as r ON p.parola = r.parola WHERE r.nome = 'sentisense_anger')\")\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvataggio in un txt delle parole non presenti in nessuna risorsa lessicale\n",
    "\n",
    "wordsMongo = angerCollections.find({\"LexResources\": []}, {\"_id\": 0, \"freq\": 0, \"LexResources\": 0})\n",
    "words = []\n",
    "\n",
    "for word in wordsMongo:\n",
    "    words.append(word['word'])\n",
    "\n",
    "with open('Nuove Parole/anger.txt', 'w',encoding=\"utf8\") as f:\n",
    "    for word in words:\n",
    "        f.write(\"%s\\n\" % word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trattamento dataset Anticipation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lista di righe, ogni riga una tweet anticipation\n",
    "file = open(\"Dataset/dataset_dt_anticipation_60k.txt\", 'r',encoding=\"utf8\")\n",
    "anticipation = []\n",
    "hashtagAnticipation = []\n",
    "emoticonAnticipation = []\n",
    "emojiAnticipation = []\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "    riga = re.sub(r'URL', '',riga)\n",
    "    riga = re.sub(r'USERNAME', '',riga)\n",
    "\n",
    "    #array di hashtag singoli\n",
    "    hashTag = (re.findall('#[a-zA-Z0-9]+', riga))\n",
    "\n",
    "    #generiamo l'array di hashtag senza hashtag\n",
    "    for h in hashTag:\n",
    "        hashtagAnticipation.append(h.replace(\"#\",\"\"))\n",
    "\n",
    "    #Rimuoviamo glin hashtag dalla riga\n",
    "    riga = riga.replace('#',\"\")\n",
    "    \n",
    "    anticipation.append(riga)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(anticipation)):\n",
    "    #Controllo se l'emoticons c'è e in caso la rimuovo\n",
    "    for singleEmoticon in emoticonsList:\n",
    "        emoticonsFind = re.findall(re.escape(singleEmoticon),anticipation[i])\n",
    "        if(len(emoticonsFind) != 0):\n",
    "            emoticonAnticipation.extend(emoticonsFind)\n",
    "            anticipation[i] = anticipation[i].replace(singleEmoticon,\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(anticipation)):\n",
    "    #preleviamo le emoji e le rimuoviamo dal testo\n",
    "    data = regex.findall(r'\\X', anticipation[i])\n",
    "    for word in data:\n",
    "        if any(char in emoji.UNICODE_EMOJI['en'] for char in word):\n",
    "            emojiAnticipation.append(word)\n",
    "            anticipation[i] = anticipation[i].replace(word, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processamento slang prima di punteggiatura e lowercase\n",
    "with open(\"Slangs/slangs.json\") as f:\n",
    "    slangsBefore = json.load(f)\n",
    "    for i in range(len(anticipation)):\n",
    "        for word in anticipation[i].split():\n",
    "            indexOfWord = anticipation[i].find(word)\n",
    "            if slangsBefore.get(word) is None:\n",
    "                if slangsBefore.get(word.lower()) is None:\n",
    "                    if slangsBefore.get(word.upper()) is None:\n",
    "                        continue\n",
    "                    else:\n",
    "                        anticipation[i] = anticipation[i][:indexOfWord] + slangsBefore.get(word.upper()) + anticipation[i][indexOfWord+len(word):]\n",
    "                else:\n",
    "                    anticipation[i] = anticipation[i][:indexOfWord] + slangsBefore.get(word.lower()) + anticipation[i][indexOfWord+len(word):]\n",
    "            else:\n",
    "                anticipation[i] = anticipation[i][:indexOfWord] + slangsBefore.get(word) + anticipation[i][indexOfWord+len(word):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizzazione di tutte le frasi\n",
    "anticipationTokenized = []\n",
    "for linea in anticipation:\n",
    "    result = word_tokenize(linea)\n",
    "    anticipationTokenized.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Esecuzione della POS tagging per ogni riga della matrice\n",
    "for i in range(len(anticipationTokenized)):\n",
    "    anticipationTokenized[i] = nltk.pos_tag(anticipationTokenized[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trattamento punteggiatura\n",
    "for line in anticipationTokenized:\n",
    "    newLine = line\n",
    "    listToRemove = []\n",
    "    for i in range(len(line)):\n",
    "        if line[i][0] in puntuaction:\n",
    "            listToRemove.append(line[i])\n",
    "    for element in listToRemove:\n",
    "        newLine.remove(element)\n",
    "    line = newLine\n",
    "\n",
    "linePuntuaction = re.compile(\"[\" + re.escape(\"\".join(puntuaction)) + \"]\")\n",
    "for line in anticipationTokenized:\n",
    "    for i in range(len(line)):\n",
    "        tupla = [(re.sub(linePuntuaction, \"\", line[i][0])),line[i][1]]\n",
    "        line[i] = tupla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming\n",
    "#ps = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for line in anticipationTokenized:\n",
    "    for i in range(len(line)):\n",
    "        tupla = [lemmatizer.lemmatize(line[i][0]),line[i][1]]\n",
    "        line[i] = tupla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trattamnento Stop word dopo Stemming\n",
    "stop_words = set(stopwords.words('english'))\n",
    "anticipationTokStoped = []\n",
    "for line in anticipationTokenized:\n",
    "    filtred_word = []\n",
    "    for row in line:\n",
    "        if  row[0] not in stop_words:\n",
    "            filtred_word.append(row) \n",
    "    anticipationTokStoped.append(filtred_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anticipationListWord = []\n",
    "anticipationListTok = []\n",
    "for line in anticipationTokStoped:\n",
    "    for row in line:\n",
    "        if row[0] != '':\n",
    "            anticipationListWord.append(row[0])\n",
    "        anticipationListTok.append(row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lavorazione risorse lessicali\n",
    "resourceNRC_anticipation = []\n",
    "resourceSentisense_anticipation = []\n",
    "#Lista di righe, ogni riga una tweet anger\n",
    "file = open(\"Risorse lessicali\\\\Anticipation\\\\NRC_anticipation.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        resourceNRC_anticipation.append(riga)\n",
    "\n",
    "file.close()\n",
    "\n",
    "file = open(\"Risorse lessicali\\Anticipation\\sentisense_anticipation.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        resourceSentisense_anticipation.append(riga)\n",
    "\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Caricare le risorse lessicali \n",
    "\n",
    "\n",
    "AnticipationNRC = {\n",
    "    '_id': 'NRC_anticipation',\n",
    "    'sentiment': 'anticipation',  \n",
    "    'wordList': resourceNRC_anticipation\n",
    "}\n",
    "\n",
    "AnticipationSentisense = {\n",
    "    '_id': 'sentisense_anticipation',\n",
    "    'sentiment': 'anticipation',\n",
    "    'wordList': resourceSentisense_anticipation\n",
    "}\n",
    "\n",
    "resourcesCollection.insert_one(AnticipationNRC)\n",
    "resourcesCollection.insert_one(AnticipationSentisense)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione database e collezioni\n",
    "# Inserimento singoli documenti contententi i tweet anger\n",
    "wordsString = {}\n",
    "\n",
    "for i in range(len(anticipationListWord)):\n",
    "    wordsString = {\n",
    "        \"lemma\": anticipationListWord[i],\n",
    "        \"POS\": anticipationListTok[i]\n",
    "    }\n",
    "    anticipationTwit={\"word\":wordsString}\n",
    "    anticipationCollections.insert_one(anticipationTwit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inserimento Documento\n",
    "anticipationInsert = {\n",
    "    \"sentiment\":\"anticipation\",\n",
    "    \"hashtags\": hashtagAnticipation,\n",
    "    \"emoticons\": emoticonAnticipation,\n",
    "    \"emoji\": emojiAnticipation\n",
    "}\n",
    "\n",
    "#inserimento\n",
    "x = collectionAnticipation.insert_one(anticipationInsert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raggruppa i documenti di input in base all'espressione _id specificata e per ciascun raggruppamento distinto genera un documento\n",
    "\n",
    "pipeline = [\n",
    "    {\"$group\": {\"_id\": \"$word.lemma\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"word\": \"$_id\",\n",
    "      \"freq\": \"$count\"\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"freq\": -1\n",
    "    }},\n",
    "    {\"$out\": \"anticipation\"}\n",
    "]\n",
    "\n",
    "\n",
    "anticipationCollections.aggregate(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creazione Word Clouds per le parole\n",
    "data = {}\n",
    "for document in anticipationCollections.find().limit(50):\n",
    "    data[document['word']] = document['freq']\n",
    "\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2)\n",
    "wc.fit_words(data)\n",
    "wc.to_file('Word Clouds/Anticipation/anticipationWords.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conteggio degli hashtags\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$hashtags\"},\n",
    "    {\"$group\": {\"_id\": \"$hashtags\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"word\": \"$_id\",\n",
    "      \"count\": 1\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"count\": -1\n",
    "    }}\n",
    "]\n",
    "\n",
    "hashtagsCount = collectionAnticipation.aggregate(pipeline)\n",
    "\n",
    "data = {}\n",
    "for document in hashtagsCount:\n",
    "    data[document['word']] = document['count']\n",
    "\n",
    "collectionAnticipation.update_one(\n",
    "  {\"sentiment\": \"anticipation\"},\n",
    "  {\"$set\": {\"hashtags\": data}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creazione Word Clouds per gli hashtag\n",
    "hashtags = collectionAnticipation.distinct(\"hashtags\")\n",
    "\n",
    "hashtagsLimited = {}\n",
    "for (word,count) in [x for x in hashtags[0].items()][:50]:\n",
    "    hashtagsLimited[word] = count\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2)\n",
    "wc.fit_words(hashtagsLimited)\n",
    "wc.to_file('Word Clouds/Anticipation/anticipationHashtags.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conteggio delle emoji\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$emoji\"},\n",
    "    {\"$group\": {\"_id\": \"$emoji\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"emoji\": \"$_id\",\n",
    "      \"count\": 1\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"count\": -1\n",
    "    }}\n",
    "]\n",
    "\n",
    "emojiCount = collectionAnticipation.aggregate(pipeline)\n",
    "\n",
    "data = {}\n",
    "for document in emojiCount:\n",
    "    data[document['emoji']] = document['count']\n",
    "\n",
    "collectionAnticipation.update_one(\n",
    "  {\"sentiment\": \"anticipation\"},\n",
    "  {\"$set\": {\"emoji\": data}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creazione Word Clouds per le emoji\n",
    "emojis = collectionAnticipation.distinct(\"emoji\")\n",
    "\n",
    "emojisLimited = {}\n",
    "for (word,count) in [x for x in emojis[0].items()][:50]:\n",
    "    emojisLimited[word] = count\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2, font_path='Font Emoji\\Symbola.otf')\n",
    "wc.fit_words(emojisLimited)\n",
    "wc.to_file('Word Clouds/Anticipation/anticipationEmojis.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conteggio degli emoticons\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$emoticons\"},\n",
    "    {\"$group\": {\"_id\": \"$emoticons\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"emoticons\": \"$_id\",\n",
    "      \"count\": 1\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"count\": -1\n",
    "    }}\n",
    "]\n",
    "\n",
    "emoticonsCount = collectionAnticipation.aggregate(pipeline)\n",
    "\n",
    "data = {}\n",
    "for document in emoticonsCount:\n",
    "    data[document['emoticons']] = document['count']\n",
    "\n",
    "collectionAnticipation.update_one(\n",
    "  {\"sentiment\": \"anticipation\"},\n",
    "  {\"$set\": {\"emoticons\": data}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creazione Word Clouds per le emoticons\n",
    "emoticons = collectionAnticipation.distinct(\"emoticons\")\n",
    "\n",
    "emoticonsLimited = {}\n",
    "for (word,count) in [x for x in emoticons[0].items()][:50]:\n",
    "    emoticonsLimited[word] = count\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2)\n",
    "wc.fit_words(emoticonsLimited)\n",
    "wc.to_file('Word Clouds/Anticipation/anticipationEmoticons.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline di aggregazione per inserire la risorsa lessicale a cui appartiene la parola\n",
    "pipeline = [\n",
    "    {\"$lookup\": {\n",
    "        \"from\": \"Resources\",\n",
    "        \"localField\": \"word\",\n",
    "        \"foreignField\": \"wordList\",\n",
    "        \"pipeline\": [\n",
    "            {\"$match\": {\"$expr\": {\"$regexMatch\": {\"input\": \"$_id\", \"regex\": \"anticipation\"}}}}\n",
    "        ],\n",
    "        \"as\": \"LexResources\"\n",
    "    }},\n",
    "    {\"$project\": {\n",
    "        \"resources.wordList\": 0,\n",
    "        \"resources.totNumberWords\": 0,\n",
    "        \"resources.sentiment\": 0,\n",
    "        \"LexResources.sentiment\": 0,\n",
    "        \"LexResources.totNumberWords\": 0,\n",
    "        \"LexResources.wordList\": 0\n",
    "    }},\n",
    "    {\"$out\": \"anticipation\"}\n",
    "]\n",
    "\n",
    "anticipationCollections.aggregate(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline di aggregazione per calcolare le percentuali richieste\n",
    "\n",
    "pipelineCount = [\n",
    "    {\"$group\": {\"_id\": \"null\", \"count\": { \"$sum\": \"$freq\" }}},\n",
    "    {\"$project\": {\n",
    "        \"_id\": 0,\n",
    "        \"count\": \"$count\"\n",
    "    }}\n",
    "]\n",
    "\n",
    "results = anticipationCollections.aggregate(pipelineCount)\n",
    "\n",
    "countWords = 0\n",
    "for data in results:\n",
    "    countWords = data['count']\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$LexResources\"},\n",
    "    {\"$lookup\": {\n",
    "        \"from\": \"Resources\",\n",
    "        \"localField\": \"LexResources._id\",\n",
    "        \"foreignField\": \"_id\",\n",
    "        \"as\": \"RefResource\"\n",
    "    }},\n",
    "    {\"$group\": {\"_id\": { \"lex\": \"$LexResources._id\", \"RefResource\": \"$RefResource\" }, \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "        \"_id\": \"$_id.lex\",\n",
    "        \"wordList\": \"$_id.RefResource.wordList\",\n",
    "        \"commonWords\": \"$count\"\n",
    "    }},\n",
    "    {\"$unwind\": \"$wordList\"},\n",
    "    {\"$project\": {\n",
    "        \"_id\": 1,\n",
    "        \"commonWords\": 1,\n",
    "        \"totWordsLex\": {\"$size\": \"$wordList\"}\n",
    "    }},\n",
    "    {\"$addFields\": {\"totalWordsTwitter\": countWords}},\n",
    "    {\"$project\": {\n",
    "        \"perc_presence_lex_res\": { \"$round\": [{\"$divide\": [ \"$commonWords\", \"$totWordsLex\" ] }, 6] },\n",
    "        \"perc_presence_twitter\": { \"$round\": [{\"$divide\": [ \"$commonWords\", \"$totalWordsTwitter\" ]}, 6] }\n",
    "    }}\n",
    "]\n",
    "\n",
    "results = anticipationCollections.aggregate(pipeline)\n",
    "\n",
    "# Genero i dati per l'istogramma\n",
    "istogramAnticipation_lexRes = []\n",
    "istogramAnticipation_twitter = []\n",
    "\n",
    "for data in results:\n",
    "    print(data)\n",
    "    log =math.log(data['perc_presence_lex_res'],10)\n",
    "    istogramAnticipation_lexRes.append([data['_id'], data['perc_presence_lex_res']])\n",
    "    log =math.log(data['perc_presence_twitter'],10)\n",
    "    istogramAnticipation_twitter.append([data['_id'], data['perc_presence_twitter']])\n",
    " \n",
    "# Create the pandas DataFrame\n",
    "dfAnticipation_lexRes = pd.DataFrame(istogramAnticipation_lexRes, columns = ['Risorsa', 'Percentuale'])\n",
    "dfAnticipation_twitter = pd.DataFrame(istogramAnticipation_twitter,columns = ['Risorsa', 'Percentuale'])      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione istogramma per presenza parole nei twitter\n",
    "\n",
    "dfAnticipation_lexRes.plot.bar(x='Risorsa')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione istogramma per presenza parole nei twitter\n",
    "\n",
    "dfAnticipation_twitter.plot.bar(x='Risorsa')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connessione, creazione cursore e database\n",
    "conn = psycopg2.connect(database = \"maadb\", user = \"postgres\", password = \"admin\", host = \"localhost\", port = \"5432\")\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione tabella risorse lessicali anticipation e inserimento\n",
    "cur.execute('''CREATE TABLE RISORSEANTICIPATION\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    NOME             TEXT       NOT NULL,\n",
    "                    PAROLA           TEXT       NOT NULL\n",
    "                );\n",
    "            '''\n",
    ")\n",
    "nome = \"NRC_anticipation\"\n",
    "for word in resourceNRC_anticipation:\n",
    "    cur.execute(\"INSERT INTO RISORSEANTICIPATION (NOME, PAROLA) VALUES(%s, %s)\",(nome,word))\n",
    "\n",
    "nome = \"sentisense_anticipation\"\n",
    "for word in resourceSentisense_anticipation:\n",
    "    cur.execute(\"INSERT INTO RISORSEANTICIPATION (NOME, PAROLA) VALUES(%s, %s)\",(nome,word))\n",
    "\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione tabelle per le parole, emoji, emoticon e hashtag\n",
    "\n",
    "cur.execute('''CREATE TABLE PAROLEANTICIPATION\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    PAROLA           TEXT       NOT NULL, \n",
    "                    NRC_anticipation      BOOL       NOT NULL DEFAULT FALSE,\n",
    "                    sentisense_anticipation      BOOL       NOT NULL DEFAULT FALSE,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ")   \n",
    "\n",
    "cur.execute('''CREATE TABLE EMOJIANTICIPATION\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    EMOJI       TEXT       NOT NULL,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ") \n",
    "\n",
    "cur.execute('''CREATE TABLE EMOTICONSANTICIPATION\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    EMOTICONS       TEXT       NOT NULL,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ") \n",
    "\n",
    "cur.execute('''CREATE TABLE HASHTAGANTICIPATION\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    HASHTAG     TEXT       NOT NULL,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ") \n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inserimento hashtag, emoticon ed emoji\n",
    "\n",
    "for hashTag in hashtagAnticipation:\n",
    "    cur.execute(\"INSERT INTO HASHTAGANTICIPATION (HASHTAG,FREQUENCY) VALUES(%s,%s)\",(hashTag,0))\n",
    "\n",
    "for emoticons in emoticonAnticipation:\n",
    "    cur.execute(\"INSERT INTO EMOTICONSANTICIPATION (EMOTICONS,FREQUENCY) VALUES(%s,%s)\",(emoticons,0))\n",
    "\n",
    "for emojis in emojiAnticipation:\n",
    "    cur.execute(\"INSERT INTO EMOJIANTICIPATION (EMOJI,FREQUENCY) VALUES(%s,%s)\",(emojis,0))\n",
    "\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inserimento parole in db\n",
    "for word in anticipationListWord:\n",
    "    cur.execute(\"INSERT INTO PAROLEANTICIPATION (PAROLA,NRC_anticipation,sentisense_anticipation,FREQUENCY) VALUES(%s,%s,%s,%s)\",(word,False,False,0))\n",
    "\n",
    "conn.commit()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze hashtag\n",
    "cur.execute(\"update HASHTAGANTICIPATION as ha1 set frequency = (select count(*) from HASHTAGANTICIPATION ha2 where ha1.hashtag = ha2.hashtag);\")\n",
    "cur.execute(\"delete from HASHTAGANTICIPATION as ha1 where exists (Select 1 From HASHTAGANTICIPATION as ha2 where ha1.hashtag = ha2.hashtag and ha1.frequency = ha2.frequency and ha2.ctid > ha1.ctid);\")\n",
    "\n",
    "conn.commit()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze emoji\n",
    "cur.execute(\"update EMOJIANTICIPATION as ha1 set frequency = (select count(*) from EMOJIANTICIPATION ha2 where ha1.emoji = ha2.emoji);\")\n",
    "cur.execute(\"delete from EMOJIANTICIPATION as ha1 where exists (Select 1 From EMOJIANTICIPATION as ha2 where ha1.emoji = ha2.emoji and ha1.frequency = ha2.frequency and ha2.ctid > ha1.ctid);\")\n",
    "\n",
    "conn.commit()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze emoticons\n",
    "cur.execute(\"update EMOTICONSANTICIPATION as ha1 set frequency = (select count(*) from EMOTICONSANTICIPATION ha2 where ha1.emoticons = ha2.emoticons);\")\n",
    "cur.execute(\"delete from EMOTICONSANTICIPATION as ha1 where exists (Select 1 From EMOTICONSANTICIPATION as ha2 where ha1.emoticons = ha2.emoticons and ha1.frequency = ha2.frequency and ha2.ctid > ha1.ctid);\")\n",
    "\n",
    "conn.commit()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze parole\n",
    "cur.execute(\"select count(*),parola from PAROLEANTICIPATION group by parola\")\n",
    "result = []\n",
    "for word in cur.fetchall():\n",
    "    result.append(word) \n",
    "\n",
    "cur.execute(\"truncate table PAROLEANTICIPATION;\")\n",
    "cur.execute(\"ALTER SEQUENCE paroleanticipation_id_seq RESTART WITH 1;\")\n",
    "\n",
    "for couple in result:\n",
    "    cur.execute(\"INSERT INTO PAROLEANTICIPATION (PAROLA,NRC_anticipation,sentisense_anticipation,FREQUENCY) VALUES(%s,%s,%s,%s)\",(couple[1],False,False,couple[0]))\n",
    "\n",
    "conn.commit()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggiornamento presenza parola nelle risorse lessicali\n",
    "cur.execute(\"UPDATE PAROLEANTICIPATION SET NRC_anticipation = TRUE WHERE parola IN (select p.parola from PAROLEANTICIPATION as p LEFT OUTER JOIN RISORSEANTICIPATION as r ON p.parola = r.parola WHERE r.nome = 'NRC_anticipation')\")\n",
    "cur.execute(\"UPDATE PAROLEANTICIPATION SET sentisense_anticipation = TRUE WHERE parola IN (select p.parola from PAROLEANTICIPATION as p LEFT OUTER JOIN RISORSEANTICIPATION as r ON p.parola = r.parola WHERE r.nome = 'sentisense_anticipation')\")\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvataggio in un txt delle parole non presenti in nessuna risorsa lessicale\n",
    "wordsMongo = anticipationCollections.find({\"LexResources\": []}, {\"_id\": 0, \"freq\": 0, \"LexResources\": 0})\n",
    "words = []\n",
    "\n",
    "for word in wordsMongo:\n",
    "    words.append(word['word'])\n",
    "\n",
    "with open('Nuove Parole/anticipation.txt', 'w',encoding=\"utf8\") as f:\n",
    "    for word in words:\n",
    "        f.write(\"%s\\n\" % word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trattamento dataset Disgust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lista di righe, ogni riga una tweet disgust\n",
    "file = open(\"Dataset/dataset_dt_disgust_60k.txt\", 'r',encoding=\"utf8\")\n",
    "disgust = []\n",
    "hashtagDisgust = []\n",
    "emoticonDisgust = []\n",
    "emojiDisgust = []\n",
    "\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "    riga = re.sub(r'URL', '',riga)\n",
    "    riga = re.sub(r'USERNAME', '',riga)\n",
    "\n",
    "    #array di hashtag singoli\n",
    "    hashTag = (re.findall('#[a-zA-Z0-9]+', riga))\n",
    "    \n",
    "    #generiamo l'array di hashtag senza hashtag\n",
    "    for h in hashTag:\n",
    "        hashtagDisgust.append(h.replace(\"#\",\"\"))\n",
    "\n",
    "    #Rimuoviamo glin hashtag dalla riga\n",
    "    riga = riga.replace('#',\"\")\n",
    "\n",
    "    disgust.append(riga)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(disgust)):\n",
    "    #Controllo se l'emoticons c'è e in caso la rimuovo\n",
    "    for singleEmoticon in emoticonsList:\n",
    "        emoticonsFind = re.findall(re.escape(singleEmoticon),disgust[i])\n",
    "        if(len(emoticonsFind) != 0):\n",
    "            emoticonDisgust.extend(emoticonsFind)\n",
    "            disgust[i] = disgust[i].replace(singleEmoticon,\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(disgust)):\n",
    "    #preleviamo le emoji e le rimuoviamo dal testo\n",
    "    data = regex.findall(r'\\X', disgust[i])\n",
    "    for word in data:\n",
    "        if any(char in emoji.UNICODE_EMOJI['en'] for char in word):\n",
    "            emojiDisgust.append(word)\n",
    "            disgust[i] = disgust[i].replace(word, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processamento slang prima di punteggiatura e lowercase\n",
    "with open(\"Slangs/slangs.json\") as f:\n",
    "    slangsBefore = json.load(f)\n",
    "    for i in range(len(disgust)):\n",
    "        for word in disgust[i].split():\n",
    "            indexOfWord = disgust[i].find(word)\n",
    "            if slangsBefore.get(word) is None:\n",
    "                if slangsBefore.get(word.lower()) is None:\n",
    "                    if slangsBefore.get(word.upper()) is None:\n",
    "                        continue\n",
    "                    else:\n",
    "                        disgust[i] = disgust[i][:indexOfWord] + slangsBefore.get(word.upper()) + disgust[i][indexOfWord+len(word):]\n",
    "                else:\n",
    "                    disgust[i] = disgust[i][:indexOfWord] + slangsBefore.get(word.lower()) + disgust[i][indexOfWord+len(word):]\n",
    "            else:\n",
    "                disgust[i] = disgust[i][:indexOfWord] + slangsBefore.get(word) + disgust[i][indexOfWord+len(word):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizzazione di tutte le frasi\n",
    "disgustTokenized = []\n",
    "for linea in disgust:\n",
    "    result = word_tokenize(linea)\n",
    "    disgustTokenized.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Esecuzione della POS tagging per ogni riga della matrice\n",
    "for i in range(len(disgustTokenized)):\n",
    "    disgustTokenized[i] = nltk.pos_tag(disgustTokenized[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trattamento punteggiatura\n",
    "for line in disgustTokenized:\n",
    "    newLine = line\n",
    "    listToRemove = []\n",
    "    for i in range(len(line)):\n",
    "        if line[i][0] in puntuaction:\n",
    "            listToRemove.append(line[i])\n",
    "    for element in listToRemove:\n",
    "        newLine.remove(element)\n",
    "    line = newLine\n",
    "\n",
    "linePuntuaction = re.compile(\"[\" + re.escape(\"\".join(puntuaction)) + \"]\")\n",
    "for line in disgustTokenized:\n",
    "    for i in range(len(line)):\n",
    "        tupla = [(re.sub(linePuntuaction, \"\", line[i][0])),line[i][1]]\n",
    "        line[i] = tupla\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming\n",
    "#ps = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for line in disgustTokenized:\n",
    "    for i in range(len(line)):\n",
    "        tupla = [lemmatizer.lemmatize(line[i][0]),line[i][1]]\n",
    "        line[i] = tupla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trattamnento Stop word dopo Stemming\n",
    "stop_words = set(stopwords.words('english'))\n",
    "disgustTokStoped = []\n",
    "for line in disgustTokenized:\n",
    "    filtred_word = []\n",
    "    for row in line:\n",
    "        if  row[0] not in stop_words:\n",
    "            filtred_word.append(row) \n",
    "    disgustTokStoped.append(filtred_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disgustListWord = []\n",
    "disgustListTok = []\n",
    "for line in disgustTokStoped:\n",
    "    for row in line:\n",
    "        if row[0] != '':\n",
    "            disgustListWord.append(row[0])\n",
    "        disgustListTok.append(row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lavorazione risorse lessicali\n",
    "resourceDisgustSentisense_disgust = []\n",
    "resourceDisgustNRC_disgust = []\n",
    "resourceDisgustSentisense_hate = []\n",
    "#Lista di righe, ogni riga una tweet disgust\n",
    "file = open(\"Risorse lessicali\\Disgust-Hate\\sentisense_disgust.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        resourceDisgustSentisense_disgust.append(riga)\n",
    "\n",
    "file.close()\n",
    "\n",
    "file = open(\"C:Risorse lessicali\\\\Disgust-Hate\\\\NRC_disgust.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        resourceDisgustNRC_disgust.append(riga)\n",
    "\n",
    "file.close()\n",
    "\n",
    "file = open(\"Risorse lessicali\\Disgust-Hate\\sentisense_hate.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        resourceDisgustSentisense_hate.append(riga)\n",
    "\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Caricare le risorse lessicali \n",
    "\n",
    "DisgustSentisense = {\n",
    "    '_id': 'Sentisense_disgust',\n",
    "    'sentiment': 'disgust', \n",
    "    'wordList': resourceDisgustSentisense_disgust\n",
    "}\n",
    "\n",
    "DisgustNRC_disgust = {\n",
    "    '_id': 'NRC_disgust',\n",
    "    'sentiment': 'disgust',  \n",
    "    'wordList': resourceDisgustNRC_disgust\n",
    "}\n",
    "\n",
    "DisgustSentisense_hate = {\n",
    "    '_id': 'Sentisense_hatesense',\n",
    "    'sentiment': 'disgust',\n",
    "    'wordList': resourceDisgustSentisense_hate\n",
    "}\n",
    "\n",
    "resourcesCollection.insert_one(DisgustSentisense)\n",
    "resourcesCollection.insert_one(DisgustNRC_disgust)\n",
    "resourcesCollection.insert_one(DisgustSentisense_hate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione database e collezioni\n",
    "# Inserimento singoli documenti contententi i tweet disgust\n",
    "wordsString = {}\n",
    "\n",
    "for i in range(len(disgustListWord)):\n",
    "    wordsString = {\n",
    "        \"lemma\": disgustListWord[i],\n",
    "        \"POS\": disgustListTok[i]\n",
    "    }\n",
    "    disgustTwit={\"word\":wordsString}\n",
    "    disgustCollections.insert_one(disgustTwit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inserimento collezione Twitter con hashtags, emoticons ed emoji \n",
    "disgustInsert = {\n",
    "    \"sentiment\":\"disgust\",\n",
    "    \"hashtags\": hashtagDisgust,\n",
    "    \"emoticons\": emoticonDisgust,\n",
    "    \"emoji\": emojiDisgust\n",
    "}\n",
    "\n",
    "#inserimento\n",
    "collectionDisgust.insert_one(disgustInsert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raggruppa i documenti di input in base all'espressione _id specificata e per ciascun raggruppamento distinto genera un documento\n",
    "pipeline = [\n",
    "    {\"$group\": {\"_id\": \"$word.lemma\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"word\": \"$_id\",\n",
    "      \"freq\": \"$count\"\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"freq\": -1\n",
    "    }},\n",
    "    {\"$out\": \"disgust\"}\n",
    "]\n",
    "\n",
    "#post processing per renderlo valido \n",
    "disgustCollections.aggregate(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creazione Word Clouds per le parole\n",
    "data = {}\n",
    "for document in disgustCollections.find().limit(50):\n",
    "    data[document['word']] = document['freq']\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2)\n",
    "wc.fit_words(data)\n",
    "wc.to_file('Word Clouds/Disgust/disgustWords.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conteggio degli hashtags\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$hashtags\"},\n",
    "    {\"$group\": {\"_id\": \"$hashtags\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"word\": \"$_id\",\n",
    "      \"count\": 1\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"count\": -1\n",
    "    }}\n",
    "]\n",
    "\n",
    "hashtagsCount = collectionDisgust.aggregate(pipeline)\n",
    "\n",
    "data = {}\n",
    "for document in hashtagsCount:\n",
    "    data[document['word']] = document['count']\n",
    "\n",
    "collectionDisgust.update_one(\n",
    "  {\"sentiment\": \"disgust\"},\n",
    "  {\"$set\": {\"hashtags\": data}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creazione Word Clouds per gli hashtags\n",
    "\n",
    "hashtags = collectionDisgust.distinct(\"hashtags\")\n",
    "\n",
    "hashtagsLimited = {}\n",
    "for (word,count) in [x for x in hashtags[0].items()][:50]:\n",
    "    hashtagsLimited[word] = count\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2)\n",
    "wc.fit_words(hashtagsLimited)\n",
    "wc.to_file('Word Clouds/Disgust/disgustHashtags.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conteggio delle emoji\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$emoji\"},\n",
    "    {\"$group\": {\"_id\": \"$emoji\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"emoji\": \"$_id\",\n",
    "      \"count\": 1\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"count\": -1\n",
    "    }}\n",
    "]\n",
    "\n",
    "emojiCount = collectionDisgust.aggregate(pipeline)\n",
    "\n",
    "data = {}\n",
    "for document in emojiCount:\n",
    "    data[document['emoji']] = document['count']\n",
    "\n",
    "collectionDisgust.update_one(\n",
    "  {\"sentiment\": \"disgust\"},\n",
    "  {\"$set\": {\"emoji\": data}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creazione Word Clouds per le emoji\n",
    "\n",
    "emojis = collectionDisgust.distinct(\"emoji\")\n",
    "\n",
    "emojisLimited = {}\n",
    "for (word,count) in [x for x in emojis[0].items()][:50]:\n",
    "    emojisLimited[word] = count\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2, font_path='Font Emoji\\Symbola.otf')\n",
    "wc.fit_words(emojisLimited)\n",
    "wc.to_file('Word Clouds/Disgust/disgustEmojis.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conteggio degli emoticons\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$emoticons\"},\n",
    "    {\"$group\": {\"_id\": \"$emoticons\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"emoticons\": \"$_id\",\n",
    "      \"count\": 1\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"count\": -1\n",
    "    }}\n",
    "]\n",
    "\n",
    "emoticonsCount = collectionDisgust.aggregate(pipeline)\n",
    "\n",
    "data = {}\n",
    "for document in emoticonsCount:\n",
    "  data[document['emoticons']] = document['count']\n",
    "\n",
    "collectionDisgust.update_one(\n",
    "  {\"sentiment\": \"disgust\"},\n",
    "  {\"$set\": {\"emoticons\": data}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creazione Word Clouds per le emoticons\n",
    "emoticons = collectionDisgust.distinct(\"emoticons\")\n",
    "\n",
    "emoticonsLimited = {}\n",
    "for (word,count) in [x for x in emoticons[0].items()][:50]:\n",
    "    emoticonsLimited[word] = count\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2)\n",
    "wc.fit_words(emoticonsLimited)\n",
    "wc.to_file('Word Clouds/Disgust/disgustEmoticons.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline di aggregazione per inserire la risorsa lessicale a cui appartiene la parola\n",
    "\n",
    "pipeline = [\n",
    "    {\"$lookup\": {\n",
    "        \"from\": \"Resources\",\n",
    "        \"localField\": \"word\",\n",
    "        \"foreignField\": \"wordList\",\n",
    "        \"pipeline\": [\n",
    "            {\"$match\": {\"$expr\": {\"$regexMatch\": {\"input\": \"$_id\", \"regex\": \"disgust\"}}}}\n",
    "        ],\n",
    "        \"as\": \"LexResources\"\n",
    "    }},\n",
    "    {\"$project\": {\n",
    "        \"resources.wordList\": 0,\n",
    "        \"resources.totNumberWords\": 0,\n",
    "        \"resources.sentiment\": 0,\n",
    "        \"LexResources.sentiment\": 0,\n",
    "        \"LexResources.totNumberWords\": 0,\n",
    "        \"LexResources.wordList\": 0\n",
    "    }},\n",
    "    {\"$out\": \"disgust\"}\n",
    "]\n",
    "\n",
    "disgustCollections.aggregate(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline di aggregazione per calcolare le percentuali richieste\n",
    "\n",
    "pipelineCount = [\n",
    "    {\"$group\": {\"_id\": \"null\", \"count\": { \"$sum\": \"$freq\" }}},\n",
    "    {\"$project\": {\n",
    "        \"_id\": 0,\n",
    "        \"count\": \"$count\"\n",
    "    }}\n",
    "]\n",
    "\n",
    "results = disgustCollections.aggregate(pipelineCount)\n",
    "\n",
    "countWords = 0\n",
    "for data in results:\n",
    "    countWords = data['count']\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$LexResources\"},\n",
    "    {\"$lookup\": {\n",
    "        \"from\": \"Resources\",\n",
    "        \"localField\": \"LexResources._id\",\n",
    "        \"foreignField\": \"_id\",\n",
    "        \"as\": \"RefResource\"\n",
    "    }},\n",
    "    {\"$group\": {\"_id\": { \"lex\": \"$LexResources._id\", \"RefResource\": \"$RefResource\" }, \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "        \"_id\": \"$_id.lex\",\n",
    "        \"wordList\": \"$_id.RefResource.wordList\",\n",
    "        \"commonWords\": \"$count\"\n",
    "    }},\n",
    "    {\"$unwind\": \"$wordList\"},\n",
    "    {\"$project\": {\n",
    "        \"_id\": 1,\n",
    "        \"commonWords\": 1,\n",
    "        \"totWordsLex\": {\"$size\": \"$wordList\"}\n",
    "    }},\n",
    "    {\"$addFields\": {\"totalWordsTwitter\": countWords}},\n",
    "    {\"$project\": {\n",
    "        \"perc_presence_lex_res\": { \"$round\": [{\"$divide\": [ \"$commonWords\", \"$totWordsLex\" ] }, 6] },\n",
    "        \"perc_presence_twitter\": { \"$round\": [{\"$divide\": [ \"$commonWords\", \"$totalWordsTwitter\" ]}, 6] }\n",
    "    }}\n",
    "]\n",
    "\n",
    "results = disgustCollections.aggregate(pipeline)\n",
    "\n",
    "# Genero i dati per l'istogramma\n",
    "istogramDisgust_lexRes = []\n",
    "istogramDisgust_twitter = []\n",
    "\n",
    "for data in results:\n",
    "    print(data)\n",
    "    log =math.log(data['perc_presence_lex_res'],10)\n",
    "    istogramDisgust_lexRes.append([data['_id'], data['perc_presence_lex_res']])\n",
    "    log =math.log(data['perc_presence_twitter'],10)\n",
    "    istogramDisgust_twitter.append([data['_id'], data['perc_presence_twitter']])\n",
    " \n",
    "# Create the pandas DataFrame\n",
    "dfDisgust_lexRes = pd.DataFrame(istogramDisgust_lexRes, columns = ['Risorsa', 'Percentuale'])\n",
    "dfDisgust_twitter = pd.DataFrame(istogramDisgust_twitter,columns = ['Risorsa', 'Percentuale'])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione istogramma per presenza parole nelle risorse lessicali\n",
    "\n",
    "dfDisgust_lexRes.plot.bar(x='Risorsa')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione istogramma per presenza parole nei twitter\n",
    "\n",
    "dfDisgust_twitter.plot.bar(x='Risorsa')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connessione, creazione cursore e database\n",
    "\n",
    "conn = psycopg2.connect(database = \"maadb\", user = \"postgres\", password = \"admin\", host = \"localhost\", port = \"5432\")\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione tabella risorse lessicali disgust e inserimento\n",
    "cur.execute('''CREATE TABLE RISORSEDISGUST\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    NOME             TEXT       NOT NULL,\n",
    "                    PAROLA           TEXT       NOT NULL\n",
    "                );\n",
    "            '''\n",
    ")\n",
    "nome = \"NRC_disgust\"\n",
    "for word in resourceDisgustNRC_disgust:\n",
    "    cur.execute(\"INSERT INTO RISORSEDISGUST (NOME, PAROLA) VALUES(%s, %s)\",(nome,word))\n",
    "\n",
    "nome = \"sentisense_disgust\"\n",
    "for word in resourceDisgustSentisense_disgust:\n",
    "    cur.execute(\"INSERT INTO RISORSEDISGUST (NOME, PAROLA) VALUES(%s, %s)\",(nome,word))\n",
    "\n",
    "nome = \"sentisense_hate\"\n",
    "for word in resourceDisgustSentisense_hate:\n",
    "    cur.execute(\"INSERT INTO RISORSEDISGUST (NOME, PAROLA) VALUES(%s, %s)\",(nome,word))\n",
    "\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione tabelle per le parole, emoji, emoticon e hashtag\n",
    "\n",
    "cur.execute('''CREATE TABLE PAROLEDISGUST\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    PAROLA           TEXT       NOT NULL, \n",
    "                    NRC_disgust      BOOL       NOT NULL DEFAULT FALSE,\n",
    "                    sentisense_disgust      BOOL       NOT NULL DEFAULT FALSE,\n",
    "                    sentisense_hate      BOOL       NOT NULL DEFAULT FALSE,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ")   \n",
    "\n",
    "cur.execute('''CREATE TABLE EMOJIDISGUST\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    EMOJI       TEXT       NOT NULL,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ") \n",
    "\n",
    "cur.execute('''CREATE TABLE EMOTICONSDISGUST\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    EMOTICONS       TEXT       NOT NULL,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ") \n",
    "\n",
    "cur.execute('''CREATE TABLE HASHTAGDISGUST\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    HASHTAG     TEXT       NOT NULL,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ") \n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inserimento hashtag, emoticon ed emoji\n",
    "\n",
    "for hashTag in hashtagDisgust:\n",
    "    cur.execute(\"INSERT INTO HASHTAGDISGUST (HASHTAG,FREQUENCY) VALUES(%s,%s)\",(hashTag,0))\n",
    "\n",
    "for emoticons in emoticonDisgust:\n",
    "    cur.execute(\"INSERT INTO EMOTICONSDISGUST (EMOTICONS,FREQUENCY) VALUES(%s,%s)\",(emoticons,0))\n",
    "\n",
    "for emojis in emojiDisgust:\n",
    "    cur.execute(\"INSERT INTO EMOJIDISGUST (EMOJI,FREQUENCY) VALUES(%s,%s)\",(emojis,0))\n",
    "\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inserimento parole in db\n",
    "for word in disgustListWord:\n",
    "    cur.execute(\"INSERT INTO PAROLEDISGUST (PAROLA,NRC_disgust,sentisense_disgust,sentisense_hate,FREQUENCY) VALUES(%s,%s,%s,%s,%s)\",(word,False,False,False,0))\n",
    "\n",
    "conn.commit()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze hashtag\n",
    "cur.execute(\"update HASHTAGDISGUST as ha1 set frequency = (select count(*) from HASHTAGDISGUST ha2 where ha1.hashtag = ha2.hashtag);\")\n",
    "cur.execute(\"delete from HASHTAGDISGUST as ha1 where exists (Select 1 From HASHTAGDISGUST as ha2 where ha1.hashtag = ha2.hashtag and ha1.frequency = ha2.frequency and ha2.ctid > ha1.ctid);\")\n",
    "\n",
    "conn.commit()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze emoji\n",
    "cur.execute(\"update EMOJIDISGUST as ha1 set frequency = (select count(*) from EMOJIDISGUST ha2 where ha1.emoji = ha2.emoji);\")\n",
    "cur.execute(\"delete from EMOJIDISGUST as ha1 where exists (Select 1 From EMOJIDISGUST as ha2 where ha1.emoji = ha2.emoji and ha1.frequency = ha2.frequency and ha2.ctid > ha1.ctid);\")\n",
    "\n",
    "conn.commit()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze emoticons\n",
    "cur.execute(\"update EMOTICONSDISGUST as ha1 set frequency = (select count(*) from EMOTICONSDISGUST ha2 where ha1.emoticons = ha2.emoticons);\")\n",
    "cur.execute(\"delete from EMOTICONSDISGUST as ha1 where exists (Select 1 From EMOTICONSDISGUST as ha2 where ha1.emoticons = ha2.emoticons and ha1.frequency = ha2.frequency and ha2.ctid > ha1.ctid);\")\n",
    "\n",
    "conn.commit()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze parole\n",
    "cur.execute(\"select count(*),parola from PAROLEDISGUST group by parola\")\n",
    "result = []\n",
    "for word in cur.fetchall():\n",
    "    result.append(word) \n",
    "\n",
    "cur.execute(\"truncate table PAROLEDISGUST;\")\n",
    "cur.execute(\"ALTER SEQUENCE paroledisgust_id_seq RESTART WITH 1;\")\n",
    "\n",
    "for couple in result:\n",
    "    cur.execute(\"INSERT INTO PAROLEDISGUST (PAROLA,NRC_disgust,sentisense_disgust,sentisense_hate,FREQUENCY) VALUES(%s,%s,%s,%s,%s)\",(couple[1],False,False,False,couple[0]))\n",
    "\n",
    "conn.commit()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggiornamento presenza parola nelle risorse lessicali\n",
    "cur.execute(\"UPDATE PAROLEDISGUST SET NRC_disgust = TRUE WHERE parola IN (select p.parola from PAROLEDISGUST as p LEFT OUTER JOIN RISORSEDISGUST as r ON p.parola = r.parola WHERE r.nome = 'NRC_disgust')\")\n",
    "cur.execute(\"UPDATE PAROLEDISGUST SET sentisense_disgust = TRUE WHERE parola IN (select p.parola from PAROLEDISGUST as p LEFT OUTER JOIN RISORSEDISGUST as r ON p.parola = r.parola WHERE r.nome = 'sentisense_disgust')\")\n",
    "cur.execute(\"UPDATE PAROLEDISGUST SET sentisense_hate = TRUE WHERE parola IN (select p.parola from PAROLEDISGUST as p LEFT OUTER JOIN RISORSEDISGUST as r ON p.parola = r.parola WHERE r.nome = 'sentisense_hate')\")\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvataggio in un txt delle parole non presenti in nessuna risorsa lessicale\n",
    "\n",
    "wordsMongo = disgustCollections.find({\"LexResources\": []}, {\"_id\": 0, \"freq\": 0, \"LexResources\": 0})\n",
    "words = []\n",
    "\n",
    "for word in wordsMongo:\n",
    "    words.append(word['word'])\n",
    "\n",
    "with open('Nuove Parole/disgust.txt', 'w',encoding=\"utf8\") as f:\n",
    "    for word in words:\n",
    "        f.write(\"%s\\n\" % word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trattamento dataset Fear "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lista di righe, ogni riga una tweet fear\n",
    "file = open(\"Dataset/dataset_dt_fear_60k.txt\", 'r',encoding=\"utf8\")\n",
    "fear = []\n",
    "hashtagFear = []\n",
    "emoticonFear = []\n",
    "emojiFear = []\n",
    "\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "    riga = re.sub(r'URL', '',riga)\n",
    "    riga = re.sub(r'USERNAME', '',riga)\n",
    "\n",
    "     #array di hashtag singoli\n",
    "    hashTag = (re.findall('#[a-zA-Z0-9]+', riga))\n",
    "\n",
    "    #generiamo l'array di hashtag senza hashtag\n",
    "    for h in hashTag:\n",
    "        hashtagFear.append(h.replace(\"#\",\"\"))\n",
    "\n",
    "    #Rimuoviamo glin hashtag dalla riga\n",
    "    riga = riga.replace('#',\"\")\n",
    "\n",
    "    fear.append(riga)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(fear)):\n",
    "    #Controllo se l'emoticons c'è e in caso la rimuovo\n",
    "    for singleEmoticon in emoticonsList:\n",
    "        emoticonsFind = re.findall(re.escape(singleEmoticon),fear[i])\n",
    "        if(len(emoticonsFind) != 0):\n",
    "            emoticonFear.extend(emoticonsFind)\n",
    "            fear[i] = fear[i].replace(singleEmoticon,\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(fear)):\n",
    "    #preleviamo le emoji e le rimuoviamo dal testo\n",
    "    data = regex.findall(r'\\X', fear[i])\n",
    "    for word in data:\n",
    "        if any(char in emoji.UNICODE_EMOJI['en'] for char in word):\n",
    "            emojiFear.append(word)\n",
    "            fear[i] = fear[i].replace(word, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processamento slang prima di punteggiatura e lowercase\n",
    "with open(\"Slangs/slangs.json\") as f:\n",
    "    slangsBefore = json.load(f)\n",
    "    for i in range(len(fear)):\n",
    "        for word in fear[i].split():\n",
    "            indexOfWord = fear[i].find(word)\n",
    "            if slangsBefore.get(word) is None:\n",
    "                if slangsBefore.get(word.lower()) is None:\n",
    "                    if slangsBefore.get(word.upper()) is None:\n",
    "                        continue\n",
    "                    else:\n",
    "                        fear[i] = fear[i][:indexOfWord] + slangsBefore.get(word.upper()) + fear[i][indexOfWord+len(word):]\n",
    "                else:\n",
    "                    fear[i] = fear[i][:indexOfWord] + slangsBefore.get(word.lower()) + fear[i][indexOfWord+len(word):]\n",
    "            else:\n",
    "                fear[i] = fear[i][:indexOfWord] + slangsBefore.get(word) + fear[i][indexOfWord+len(word):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizzazione di tutte le frasi\n",
    "fearTokenized = []\n",
    "for linea in fear:\n",
    "    result = word_tokenize(linea)\n",
    "    fearTokenized.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Esecuzione della POS tagging per ogni riga della matrice\n",
    "for i in range(len(fearTokenized)):\n",
    "    fearTokenized[i] = nltk.pos_tag(fearTokenized[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trattamento punteggiatura\n",
    "for line in fearTokenized:\n",
    "    newLine = line\n",
    "    listToRemove = []\n",
    "    for i in range(len(line)):\n",
    "        if line[i][0] in puntuaction:\n",
    "            listToRemove.append(line[i])\n",
    "    for element in listToRemove:\n",
    "        newLine.remove(element)\n",
    "    line = newLine\n",
    "\n",
    "linePuntuaction = re.compile(\"[\" + re.escape(\"\".join(puntuaction)) + \"]\")\n",
    "for line in fearTokenized:\n",
    "    for i in range(len(line)):\n",
    "        tupla = [(re.sub(linePuntuaction, \"\", line[i][0])),line[i][1]]\n",
    "        line[i] = tupla\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming\n",
    "#ps = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for line in fearTokenized:\n",
    "    for i in range(len(line)):\n",
    "        tupla = [lemmatizer.lemmatize(line[i][0]),line[i][1]]\n",
    "        line[i] = tupla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trattamnento Stop word dopo Stemming\n",
    "stop_words = set(stopwords.words('english'))\n",
    "fearTokStoped = []\n",
    "for line in fearTokenized:\n",
    "    filtred_word = []\n",
    "    for row in line:\n",
    "        if  row[0] not in stop_words:\n",
    "            filtred_word.append(row) \n",
    "    fearTokStoped.append(filtred_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fearListWord = []\n",
    "fearListTok = []\n",
    "for line in fearTokStoped:\n",
    "    for row in line:\n",
    "        if row[0] != '':\n",
    "            fearListWord.append(row[0])\n",
    "        fearListTok.append(row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lavorazione risorse lessicali\n",
    "resourceFearNRC = []\n",
    "resourceFearSentisense = []\n",
    "#Lista di righe, ogni riga una tweet fear\n",
    "file = open(\"Risorse lessicali\\\\Fear\\\\NRC_fear.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        resourceFearNRC.append(riga)\n",
    "\n",
    "file.close()\n",
    "\n",
    "file = open(\"Risorse lessicali\\Fear\\sentisense_fear.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        resourceFearSentisense.append(riga)\n",
    "\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Caricare le risorse lessicali \n",
    "FearNRC = {\n",
    "    '_id': 'NRC_fear',\n",
    "    'sentiment': 'fear',  \n",
    "    'wordList': resourceFearNRC\n",
    "}\n",
    "\n",
    "FearSentisense = {\n",
    "    '_id': 'sentisense_fear',\n",
    "    'sentiment': 'fear',\n",
    "    'wordList': resourceFearSentisense\n",
    "}\n",
    "\n",
    "resourcesCollection.insert_one(FearNRC)\n",
    "resourcesCollection.insert_one(FearSentisense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione database e collezioni\n",
    "# Inserimento singoli documenti contententi i tweet fear\n",
    "wordsString = {}\n",
    "\n",
    "for i in range(len(fearListWord)):\n",
    "    wordsString = {\n",
    "        \"lemma\": fearListWord[i],\n",
    "        \"POS\": fearListTok[i]\n",
    "    }\n",
    "    fearTwit={\"word\":wordsString}\n",
    "    fearCollections.insert_one(fearTwit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inserimento collezione Twitter con hashtags, emoticons ed emoji \n",
    "fearInsert = {\n",
    "    \"sentiment\":\"fear\",\n",
    "    \"hashtags\": hashtagFear,\n",
    "    \"emoticons\": emoticonFear,\n",
    "    \"emoji\": emojiFear\n",
    "}\n",
    "\n",
    "#inserimento\n",
    "x = collectionFear.insert_one(fearInsert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raggruppa i documenti di input in base all'espressione _id specificata e per ciascun raggruppamento distinto genera un documento\n",
    "pipeline = [\n",
    "    {\"$group\": {\"_id\": \"$word.lemma\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"word\": \"$_id\",\n",
    "      \"freq\": \"$count\"\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"freq\": -1\n",
    "    }},\n",
    "    {\"$out\": \"fear\"}\n",
    "]\n",
    "\n",
    "#post processing per renderlo valido \n",
    "fearCollections.aggregate(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creazione Word Clouds per le parole\n",
    "data = {}\n",
    "for document in fearCollections.find().limit(50):\n",
    "    data[document['word']] = document['freq']\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2)\n",
    "wc.fit_words(data)\n",
    "wc.to_file('Word Clouds/Fear/fearWords.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conteggio degli hashtags\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$hashtags\"},\n",
    "    {\"$group\": {\"_id\": \"$hashtags\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"word\": \"$_id\",\n",
    "      \"count\": 1\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"count\": -1\n",
    "    }}\n",
    "]\n",
    "\n",
    "hashtagsCount = collectionFear.aggregate(pipeline)\n",
    "\n",
    "data = {}\n",
    "for document in hashtagsCount:\n",
    "    data[document['word']] = document['count']\n",
    "\n",
    "collectionFear.update_one(\n",
    "  {\"sentiment\": \"fear\"},\n",
    "  {\"$set\": {\"hashtags\": data}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creazione Word Clouds per gli hashtags\n",
    "\n",
    "hashtags = collectionFear.distinct(\"hashtags\")\n",
    "\n",
    "hashtagsLimited = {}\n",
    "for (word,count) in [x for x in hashtags[0].items()][:50]:\n",
    "    hashtagsLimited[word] = count\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2)\n",
    "wc.fit_words(hashtagsLimited)\n",
    "wc.to_file('Word Clouds/Fear/fearHashtags.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conteggio delle emoji\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$emoji\"},\n",
    "    {\"$group\": {\"_id\": \"$emoji\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"emoji\": \"$_id\",\n",
    "      \"count\": 1\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"count\": -1\n",
    "    }}\n",
    "]\n",
    "\n",
    "emojiCount = collectionFear.aggregate(pipeline)\n",
    "\n",
    "data = {}\n",
    "for document in emojiCount:\n",
    "    data[document['emoji']] = document['count']\n",
    "\n",
    "collectionFear.update_one(\n",
    "  {\"sentiment\": \"fear\"},\n",
    "  {\"$set\": {\"emoji\": data}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creazione Word Clouds per le emoji\n",
    "\n",
    "emojis = collectionFear.distinct(\"emoji\")\n",
    "\n",
    "emojisLimited = {}\n",
    "for (word,count) in [x for x in emojis[0].items()][:50]:\n",
    "    emojisLimited[word] = count\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2, font_path='Font Emoji\\Symbola.otf')\n",
    "wc.fit_words(emojisLimited)\n",
    "wc.to_file('Word Clouds/Fear/fearEmojis.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conteggio degli emoticons\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$emoticons\"},\n",
    "    {\"$group\": {\"_id\": \"$emoticons\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"emoticons\": \"$_id\",\n",
    "      \"count\": 1\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"count\": -1\n",
    "    }}\n",
    "]\n",
    "\n",
    "emoticonsCount = collectionFear.aggregate(pipeline)\n",
    "\n",
    "data = {}\n",
    "for document in emoticonsCount:\n",
    "    data[document['emoticons']] = document['count']\n",
    "\n",
    "collectionFear.update_one(\n",
    "  {\"sentiment\": \"fear\"},\n",
    "  {\"$set\": {\"emoticons\": data}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creazione Word Clouds per le emoticons\n",
    "\n",
    "emoticons = collectionFear.distinct(\"emoticons\")\n",
    "\n",
    "emoticonsLimited = {}\n",
    "for (word,count) in [x for x in emoticons[0].items()][:50]:\n",
    "    emoticonsLimited[word] = count\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2)\n",
    "wc.fit_words(emoticonsLimited)\n",
    "wc.to_file('Word Clouds/Fear/fearEmoticons.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline di aggregazione per inserire la risorsa lessicale a cui appartiene la parola\n",
    "pipeline = [\n",
    "    {\"$lookup\": {\n",
    "        \"from\": \"Resources\",\n",
    "        \"localField\": \"word\",\n",
    "        \"foreignField\": \"wordList\",\n",
    "        \"pipeline\": [\n",
    "            {\"$match\": {\"$expr\": {\"$regexMatch\": {\"input\": \"$_id\", \"regex\": \"fear\"}}}}\n",
    "        ],\n",
    "        \"as\": \"LexResources\"\n",
    "    }},\n",
    "    {\"$project\": {\n",
    "        \"resources.wordList\": 0,\n",
    "        \"resources.totNumberWords\": 0,\n",
    "        \"resources.sentiment\": 0,\n",
    "        \"LexResources.sentiment\": 0,\n",
    "        \"LexResources.totNumberWords\": 0,\n",
    "        \"LexResources.wordList\": 0\n",
    "    }},\n",
    "    {\"$out\": \"fear\"}\n",
    "]\n",
    "\n",
    "fearCollections.aggregate(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline di aggregazione per calcolare le percentuali richieste\n",
    "\n",
    "pipelineCount = [\n",
    "    {\"$group\": {\"_id\": \"null\", \"count\": { \"$sum\": \"$freq\" }}},\n",
    "    {\"$project\": {\n",
    "        \"_id\": 0,\n",
    "        \"count\": \"$count\"\n",
    "    }}\n",
    "]\n",
    "\n",
    "results = fearCollections.aggregate(pipelineCount)\n",
    "\n",
    "countWords = 0\n",
    "for data in results:\n",
    "    countWords = data['count']\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$LexResources\"},\n",
    "    {\"$lookup\": {\n",
    "        \"from\": \"Resources\",\n",
    "        \"localField\": \"LexResources._id\",\n",
    "        \"foreignField\": \"_id\",\n",
    "        \"as\": \"RefResource\"\n",
    "    }},\n",
    "    {\"$group\": {\"_id\": { \"lex\": \"$LexResources._id\", \"RefResource\": \"$RefResource\" }, \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "        \"_id\": \"$_id.lex\",\n",
    "        \"wordList\": \"$_id.RefResource.wordList\",\n",
    "        \"commonWords\": \"$count\"\n",
    "    }},\n",
    "    {\"$unwind\": \"$wordList\"},\n",
    "    {\"$project\": {\n",
    "        \"_id\": 1,\n",
    "        \"commonWords\": 1,\n",
    "        \"totWordsLex\": {\"$size\": \"$wordList\"}\n",
    "    }},\n",
    "    {\"$addFields\": {\"totalWordsTwitter\": countWords}},\n",
    "    {\"$project\": {\n",
    "        \"perc_presence_lex_res\": { \"$round\": [{\"$divide\": [ \"$commonWords\", \"$totWordsLex\" ] }, 6] },\n",
    "        \"perc_presence_twitter\": { \"$round\": [{\"$divide\": [ \"$commonWords\", \"$totalWordsTwitter\" ]}, 6] }\n",
    "    }}\n",
    "]\n",
    "\n",
    "results = fearCollections.aggregate(pipeline)\n",
    "\n",
    "# Genero i dati per l'istogramma\n",
    "istogramFear_lexRes = []\n",
    "istogramFear_twitter = []\n",
    "\n",
    "for data in results:\n",
    "    print(data)\n",
    "    log =math.log(data['perc_presence_lex_res'],10)\n",
    "    istogramFear_lexRes.append([data['_id'], data['perc_presence_lex_res']])\n",
    "    log =math.log(data['perc_presence_twitter'],10)\n",
    "    istogramFear_twitter.append([data['_id'], data['perc_presence_twitter']])\n",
    "\n",
    "# Create the pandas DataFrame\n",
    "dfFear_lexRes = pd.DataFrame(istogramFear_lexRes, columns = ['Risorsa', 'Percentuale'])\n",
    "dfFear_twitter = pd.DataFrame(istogramFear_twitter,columns = ['Risorsa', 'Percentuale'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione istogramma per presenza parole nei twitter\n",
    "\n",
    "dfFear_lexRes.plot.bar(x='Risorsa')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione istogramma per presenza parole nei twitter\n",
    "\n",
    "dfFear_twitter.plot.bar(x='Risorsa')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connessione, creazione cursore e database\n",
    "conn = psycopg2.connect(database = \"maadb\", user = \"postgres\", password = \"admin\", host = \"localhost\", port = \"5432\")\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione tabella risorse lessicali fear e inserimento\n",
    "cur.execute('''CREATE TABLE RISORSEFEAR\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    NOME             TEXT       NOT NULL,\n",
    "                    PAROLA           TEXT       NOT NULL\n",
    "                );\n",
    "            '''\n",
    ")\n",
    "nome = \"NRC_fear\"\n",
    "for word in resourceFearNRC:\n",
    "    cur.execute(\"INSERT INTO RISORSEFEAR (NOME, PAROLA) VALUES(%s, %s)\",(nome,word))\n",
    "\n",
    "nome = \"sentisense_fear\"\n",
    "for word in resourceFearSentisense:\n",
    "    cur.execute(\"INSERT INTO RISORSEFEAR (NOME, PAROLA) VALUES(%s, %s)\",(nome,word))\n",
    "\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione tabelle per le parole, emoji, emoticon e hashtag\n",
    "\n",
    "cur.execute('''CREATE TABLE PAROLEFEAR\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    PAROLA           TEXT       NOT NULL, \n",
    "                    NRC_fear      BOOL       NOT NULL DEFAULT FALSE,\n",
    "                    sentisense_fear      BOOL       NOT NULL DEFAULT FALSE,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ")   \n",
    "\n",
    "cur.execute('''CREATE TABLE EMOJIFEAR\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    EMOJI       TEXT       NOT NULL,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ") \n",
    "\n",
    "cur.execute('''CREATE TABLE EMOTICONSFEAR\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    EMOTICONS       TEXT       NOT NULL,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ") \n",
    "\n",
    "cur.execute('''CREATE TABLE HASHTAGFEAR\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    HASHTAG     TEXT       NOT NULL,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ") \n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inserimento hashtag, emoticon ed emoji\n",
    "\n",
    "for hashTag in hashtagFear:\n",
    "    cur.execute(\"INSERT INTO HASHTAGFEAR (HASHTAG,FREQUENCY) VALUES(%s,%s)\",(hashTag,0))\n",
    "\n",
    "for emoticons in emoticonFear:\n",
    "    cur.execute(\"INSERT INTO EMOTICONSFEAR (EMOTICONS,FREQUENCY) VALUES(%s,%s)\",(emoticons,0))\n",
    "\n",
    "for emojis in emojiFear:\n",
    "    cur.execute(\"INSERT INTO EMOJIFEAR (EMOJI,FREQUENCY) VALUES(%s,%s)\",(emojis,0))\n",
    "\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inserimento parole in db\n",
    "for word in fearListWord:\n",
    "    cur.execute(\"INSERT INTO PAROLEFEAR (PAROLA,NRC_fear,sentisense_fear,FREQUENCY) VALUES(%s,%s,%s,%s)\",(word,False,False,0))\n",
    "\n",
    "conn.commit()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze hashtag\n",
    "cur.execute(\"update HASHTAGFEAR as ha1 set frequency = (select count(*) from HASHTAGFEAR ha2 where ha1.hashtag = ha2.hashtag);\")\n",
    "cur.execute(\"delete from HASHTAGFEAR as ha1 where exists (Select 1 From HASHTAGFEAR as ha2 where ha1.hashtag = ha2.hashtag and ha1.frequency = ha2.frequency and ha2.ctid > ha1.ctid);\")\n",
    "\n",
    "conn.commit()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze emoji\n",
    "cur.execute(\"update EMOJIFEAR as ha1 set frequency = (select count(*) from EMOJIFEAR ha2 where ha1.emoji = ha2.emoji);\")\n",
    "cur.execute(\"delete from EMOJIFEAR as ha1 where exists (Select 1 From EMOJIFEAR as ha2 where ha1.emoji = ha2.emoji and ha1.frequency = ha2.frequency and ha2.ctid > ha1.ctid);\")\n",
    "\n",
    "conn.commit()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze emoticons\n",
    "cur.execute(\"update EMOTICONSFEAR as ha1 set frequency = (select count(*) from EMOTICONSFEAR ha2 where ha1.emoticons = ha2.emoticons);\")\n",
    "cur.execute(\"delete from EMOTICONSFEAR as ha1 where exists (Select 1 From EMOTICONSFEAR as ha2 where ha1.emoticons = ha2.emoticons and ha1.frequency = ha2.frequency and ha2.ctid > ha1.ctid);\")\n",
    "\n",
    "conn.commit() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze parole\n",
    "cur.execute(\"select count(*),parola from PAROLEFEAR group by parola\")\n",
    "result = []\n",
    "for word in cur.fetchall():\n",
    "    result.append(word) \n",
    "\n",
    "cur.execute(\"truncate table PAROLEFEAR;\")\n",
    "cur.execute(\"ALTER SEQUENCE parolefear_id_seq RESTART WITH 1;\")\n",
    "\n",
    "for couple in result:\n",
    "    cur.execute(\"INSERT INTO PAROLEFEAR (PAROLA,NRC_fear,sentisense_fear,FREQUENCY) VALUES(%s,%s,%s,%s)\",(couple[1],False,False,couple[0]))\n",
    "\n",
    "conn.commit()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggiornamento presenza parola nelle risorse lessicali\n",
    "cur.execute(\"UPDATE PAROLEFEAR SET NRC_fear = TRUE WHERE parola IN (select p.parola from PAROLEFEAR as p LEFT OUTER JOIN RISORSEFEAR as r ON p.parola = r.parola WHERE r.nome = 'NRC_fear')\")\n",
    "cur.execute(\"UPDATE PAROLEFEAR SET sentisense_fear = TRUE WHERE parola IN (select p.parola from PAROLEFEAR as p LEFT OUTER JOIN RISORSEFEAR as r ON p.parola = r.parola WHERE r.nome = 'sentisense_fear')\")\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvataggio in un txt delle parole non presenti in nessuna risorsa lessicale\n",
    "wordsMongo = fearCollections.find({\"LexResources\": []}, {\"_id\": 0, \"freq\": 0, \"LexResources\": 0})\n",
    "words = []\n",
    "\n",
    "for word in wordsMongo:\n",
    "    words.append(word['word'])\n",
    "\n",
    "with open('Nuove Parole/fear.txt', 'w',encoding=\"utf8\") as f:\n",
    "    for word in words:\n",
    "        f.write(\"%s\\n\" % word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trattamento dataset Joy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lista di righe, ogni riga una tweet joy\n",
    "file = open(\"Dataset/dataset_dt_joy_60k.txt\", 'r',encoding=\"utf8\")\n",
    "joy = []\n",
    "hashtagJoy = []\n",
    "emoticonJoy = []\n",
    "emojiJoy = []\n",
    "\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "    riga = re.sub(r'URL', '',riga)\n",
    "    riga = re.sub(r'USERNAME', '',riga)\n",
    "    \n",
    "    #array di hashtag singoli\n",
    "    hashTag = (re.findall('#[a-zA-Z0-9]+', riga))\n",
    "\n",
    "    #generiamo l'array di hashtag senza hashtag\n",
    "    for h in hashTag:\n",
    "        hashtagJoy.append(h.replace(\"#\",\"\"))\n",
    "\n",
    "    #Rimuoviamo glin hashtag dalla riga\n",
    "    riga = riga.replace('#',\"\")\n",
    "\n",
    "    joy.append(riga)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(joy)):\n",
    "    #Controllo se l'emoticons c'è e in caso la rimuovo\n",
    "    for singleEmoticon in emoticonsList:\n",
    "        emoticonsFind = re.findall(re.escape(singleEmoticon),joy[i])\n",
    "        if(len(emoticonsFind) != 0):\n",
    "            emoticonJoy.extend(emoticonsFind)\n",
    "            joy[i] = joy[i].replace(singleEmoticon,\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(joy)):\n",
    "    #preleviamo le emoji e le rimuoviamo dal testo\n",
    "    data = regex.findall(r'\\X', joy[i])\n",
    "    for word in data:\n",
    "        if any(char in emoji.UNICODE_EMOJI['en'] for char in word):\n",
    "            emojiJoy.append(word)\n",
    "            joy[i] = joy[i].replace(word, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processamento slang prima di punteggiatura e lowercase\n",
    "with open(\"Slangs/slangs.json\") as f:\n",
    "    slangsBefore = json.load(f)\n",
    "    for i in range(len(joy)):\n",
    "        for word in joy[i].split():\n",
    "            indexOfWord = joy[i].find(word)\n",
    "            if slangsBefore.get(word) is None:\n",
    "                if slangsBefore.get(word.lower()) is None:\n",
    "                    if slangsBefore.get(word.upper()) is None:\n",
    "                        continue\n",
    "                    else:\n",
    "                        joy[i] = joy[i][:indexOfWord] + slangsBefore.get(word.upper()) + joy[i][indexOfWord+len(word):]\n",
    "                else:\n",
    "                    joy[i] = joy[i][:indexOfWord] + slangsBefore.get(word.lower()) + joy[i][indexOfWord+len(word):]\n",
    "            else:\n",
    "                joy[i] = joy[i][:indexOfWord] + slangsBefore.get(word) + joy[i][indexOfWord+len(word):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizzazione di tutte le frasi\n",
    "joyTokenized = []\n",
    "for linea in joy:\n",
    "    result = word_tokenize(linea)\n",
    "    joyTokenized.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Esecuzione della POS tagging per ogni riga della matrice\n",
    "for i in range(len(joyTokenized)):\n",
    "    joyTokenized[i] = nltk.pos_tag(joyTokenized[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trattamento punteggiatura\n",
    "for line in joyTokenized:\n",
    "    newLine = line\n",
    "    listToRemove = []\n",
    "    for i in range(len(line)):\n",
    "        if line[i][0] in puntuaction:\n",
    "            listToRemove.append(line[i])\n",
    "    for element in listToRemove:\n",
    "        newLine.remove(element)\n",
    "    line = newLine\n",
    "\n",
    "linePuntuaction = re.compile(\"[\" + re.escape(\"\".join(puntuaction)) + \"]\")\n",
    "for line in joyTokenized:\n",
    "    for i in range(len(line)):\n",
    "        tupla = [(re.sub(linePuntuaction, \"\", line[i][0])),line[i][1]]\n",
    "        line[i] = tupla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming\n",
    "#ps = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for line in joyTokenized:\n",
    "    for i in range(len(line)):\n",
    "        tupla = [lemmatizer.lemmatize(line[i][0]),line[i][1]]\n",
    "        line[i] = tupla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trattamnento Stop word dopo Stemming\n",
    "stop_words = set(stopwords.words('english'))\n",
    "joyTokStoped = []\n",
    "for line in joyTokenized:\n",
    "    filtred_word = []\n",
    "    for row in line:\n",
    "        if  row[0] not in stop_words:\n",
    "            filtred_word.append(row) \n",
    "    joyTokStoped.append(filtred_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joyListWord = []\n",
    "joyListTok = []\n",
    "for line in joyTokStoped:\n",
    "    for row in line:\n",
    "        if row[0] != '':\n",
    "            joyListWord.append(row[0])\n",
    "        joyListTok.append(row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lavorazione risorse lessicali\n",
    "resourceJoyEmoSN = []\n",
    "resourceJoyNRC = []\n",
    "resourceJoySentisence = []\n",
    "#Lista di righe, ogni riga una tweet anger\n",
    "file = open(\"Risorse lessicali\\Joy\\EmoSN_joy.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        resourceJoyEmoSN.append(riga)\n",
    "\n",
    "file.close()\n",
    "\n",
    "file = open(\"Risorse lessicali\\\\Joy\\\\NRC_joy.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        resourceJoyNRC.append(riga)\n",
    "\n",
    "file.close()\n",
    "\n",
    "file = open(\"Risorse lessicali\\Joy\\sentisense_joy.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        resourceJoySentisence.append(riga)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Caricare le risorse lessicali \n",
    "\n",
    "JoyEmoSN = {\n",
    "    '_id': 'EmoSN_joy',\n",
    "    'sentiment': 'joy', \n",
    "    'wordList': resourceJoyEmoSN\n",
    "}\n",
    "\n",
    "JoyNRC = {\n",
    "    '_id': 'NRC_joy',\n",
    "    'sentiment': 'joy',  \n",
    "    'wordList': resourceJoyNRC\n",
    "}\n",
    "\n",
    "JoySentisense = {\n",
    "    '_id': 'sentisense_joy',\n",
    "    'sentiment': 'joy',\n",
    "    'wordList': resourceJoySentisence\n",
    "}\n",
    "\n",
    "resourcesCollection.insert_one(JoyEmoSN)\n",
    "resourcesCollection.insert_one(JoyNRC)\n",
    "resourcesCollection.insert_one(JoySentisense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione database e collezioni\n",
    "# Inserimento singoli documenti contententi i tweet joy\n",
    "wordsString = {}\n",
    "\n",
    "for i in range(len(joyListWord)):\n",
    "    wordsString = {\n",
    "        \"lemma\": joyListWord[i],\n",
    "        \"POS\": joyListTok[i]\n",
    "    }\n",
    "    joyTwit={\"word\":wordsString}\n",
    "    joyCollections.insert_one(joyTwit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inserimento collezione Twitter con hashtags, emoticons ed emoji \n",
    "joyInsert = {\n",
    "    \"sentiment\":\"joy\",\n",
    "    \"hashtags\": hashtagJoy,\n",
    "    \"emoticons\": emoticonJoy,\n",
    "    \"emoji\": emojiJoy\n",
    "}\n",
    "\n",
    "#inserimento\n",
    "x = collectionAJoy.insert_one(joyInsert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raggruppa i documenti di input in base all'espressione _id specificata e per ciascun raggruppamento distinto genera un documento\n",
    "pipeline = [\n",
    "    {\"$group\": {\"_id\": \"$word.lemma\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"word\": \"$_id\",\n",
    "      \"freq\": \"$count\"\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"freq\": -1\n",
    "    }},\n",
    "    {\"$out\": \"joy\"}\n",
    "]\n",
    "\n",
    "#post processing per renderlo valido \n",
    "joyCollections.aggregate(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creazione Word Clouds per le parole\n",
    "data = {}\n",
    "for document in joyCollections.find().limit(50):\n",
    "    data[document['word']] = document['freq']\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2)\n",
    "wc.fit_words(data)\n",
    "wc.to_file('Word Clouds/Joy/joyWords.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conteggio degli hashtags\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$hashtags\"},\n",
    "    {\"$group\": {\"_id\": \"$hashtags\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"word\": \"$_id\",\n",
    "      \"count\": 1\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"count\": -1\n",
    "    }}\n",
    "]\n",
    "\n",
    "hashtagsCount = collectionAJoy.aggregate(pipeline)\n",
    "\n",
    "data = {}\n",
    "for document in hashtagsCount:\n",
    "    data[document['word']] = document['count']\n",
    "\n",
    "collectionAJoy.update_one(\n",
    "  {\"sentiment\": \"joy\"},\n",
    "  {\"$set\": {\"hashtags\": data}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creazione Word Clouds per gli hashtags\n",
    "\n",
    "hashtags = collectionAJoy.distinct(\"hashtags\")\n",
    "\n",
    "hashtagsLimited = {}\n",
    "for (word,count) in [x for x in hashtags[0].items()][:50]:\n",
    "    hashtagsLimited[word] = count\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2)\n",
    "wc.fit_words(hashtagsLimited)\n",
    "wc.to_file('Word Clouds/Joy/joyHashtags.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conteggio delle emoji\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$emoji\"},\n",
    "    {\"$group\": {\"_id\": \"$emoji\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"emoji\": \"$_id\",\n",
    "      \"count\": 1\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"count\": -1\n",
    "    }}\n",
    "]\n",
    "\n",
    "emojiCount = collectionAJoy.aggregate(pipeline)\n",
    "\n",
    "data = {}\n",
    "for document in emojiCount:\n",
    "    data[document['emoji']] = document['count']\n",
    "\n",
    "collectionAJoy.update_one(\n",
    "  {\"sentiment\": \"joy\"},\n",
    "  {\"$set\": {\"emoji\": data}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creazione Word Clouds per le emoji\n",
    "\n",
    "emojis = collectionAJoy.distinct(\"emoji\")\n",
    "\n",
    "emojisLimited = {}\n",
    "for (word,count) in [x for x in emojis[0].items()][:50]:\n",
    "    emojisLimited[word] = count\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2, font_path='Font Emoji\\Symbola.otf')\n",
    "wc.fit_words(emojisLimited)\n",
    "wc.to_file('Word Clouds/Joy/joyEmojis.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conteggio degli emoticons\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$emoticons\"},\n",
    "    {\"$group\": {\"_id\": \"$emoticons\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"emoticons\": \"$_id\",\n",
    "      \"count\": 1\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"count\": -1\n",
    "    }}\n",
    "]\n",
    "\n",
    "emoticonsCount = collectionAJoy.aggregate(pipeline)\n",
    "\n",
    "data = {}\n",
    "for document in emoticonsCount:\n",
    "    data[document['emoticons']] = document['count']\n",
    "\n",
    "collectionAJoy.update_one(\n",
    "  {\"sentiment\": \"joy\"},\n",
    "  {\"$set\": {\"emoticons\": data}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creazione Word Clouds per le emoticons\n",
    "\n",
    "emoticons = collectionAJoy.distinct(\"emoticons\")\n",
    "\n",
    "emoticonsLimited = {}\n",
    "for (word,count) in [x for x in emoticons[0].items()][:50]:\n",
    "    emoticonsLimited[word] = count\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2)\n",
    "wc.fit_words(emoticonsLimited)\n",
    "wc.to_file('Word Clouds/Joy/joyEmoticons.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline di aggregazione per inserire la risorsa lessicale a cui appartiene la parola\n",
    "\n",
    "pipeline = [\n",
    "    {\"$lookup\": {\n",
    "        \"from\": \"Resources\",\n",
    "        \"localField\": \"word\",\n",
    "        \"foreignField\": \"wordList\",\n",
    "        \"pipeline\": [\n",
    "            {\"$match\": {\"$expr\": {\"$regexMatch\": {\"input\": \"$_id\", \"regex\": \"joy\"}}}}\n",
    "        ],\n",
    "        \"as\": \"LexResources\"\n",
    "    }},\n",
    "    {\"$project\": {\n",
    "        \"resources.wordList\": 0,\n",
    "        \"resources.totNumberWords\": 0,\n",
    "        \"resources.sentiment\": 0,\n",
    "        \"LexResources.sentiment\": 0,\n",
    "        \"LexResources.totNumberWords\": 0,\n",
    "        \"LexResources.wordList\": 0\n",
    "    }},\n",
    "    {\"$out\": \"joy\"}\n",
    "]\n",
    "\n",
    "joyCollections.aggregate(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline di aggregazione per calcolare le percentuali richieste\n",
    "\n",
    "pipelineCount = [\n",
    "    {\"$group\": {\"_id\": \"null\", \"count\": { \"$sum\": \"$freq\" }}},\n",
    "    {\"$project\": {\n",
    "        \"_id\": 0,\n",
    "        \"count\": \"$count\"\n",
    "    }}\n",
    "]\n",
    "\n",
    "results = joyCollections.aggregate(pipelineCount)\n",
    "\n",
    "countWords = 0\n",
    "for data in results:\n",
    "    countWords = data['count']\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$LexResources\"},\n",
    "    {\"$lookup\": {\n",
    "        \"from\": \"Resources\",\n",
    "        \"localField\": \"LexResources._id\",\n",
    "        \"foreignField\": \"_id\",\n",
    "        \"as\": \"RefResource\"\n",
    "    }},\n",
    "    {\"$group\": {\"_id\": { \"lex\": \"$LexResources._id\", \"RefResource\": \"$RefResource\" }, \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "        \"_id\": \"$_id.lex\",\n",
    "        \"wordList\": \"$_id.RefResource.wordList\",\n",
    "        \"commonWords\": \"$count\"\n",
    "    }},\n",
    "    {\"$unwind\": \"$wordList\"},\n",
    "    {\"$project\": {\n",
    "        \"_id\": 1,\n",
    "        \"commonWords\": 1,\n",
    "        \"totWordsLex\": {\"$size\": \"$wordList\"}\n",
    "    }},\n",
    "    {\"$addFields\": {\"totalWordsTwitter\": countWords}},\n",
    "    {\"$project\": {\n",
    "        \"perc_presence_lex_res\": { \"$round\": [{\"$divide\": [ \"$commonWords\", \"$totWordsLex\" ] }, 6] },\n",
    "        \"perc_presence_twitter\": { \"$round\": [{\"$divide\": [ \"$commonWords\", \"$totalWordsTwitter\" ]}, 6] }\n",
    "    }}\n",
    "]\n",
    "\n",
    "results = joyCollections.aggregate(pipeline)\n",
    "\n",
    "# Genero i dati per l'istogramma\n",
    "istogramJoy_lexRes = []\n",
    "istogramJoy_twitter = []\n",
    "\n",
    "for data in results:\n",
    "    print(data)\n",
    "    log =math.log(data['perc_presence_lex_res'],10)\n",
    "    istogramJoy_lexRes.append([data['_id'], data['perc_presence_lex_res']])\n",
    "    log =math.log(data['perc_presence_twitter'],10)\n",
    "    istogramJoy_twitter.append([data['_id'], data['perc_presence_twitter']])\n",
    " \n",
    "# Create the pandas DataFrame\n",
    "dfJoy_lexRes = pd.DataFrame(istogramJoy_lexRes, columns = ['Risorsa', 'Percentuale'])\n",
    "dfJoy_twitter = pd.DataFrame(istogramJoy_twitter,columns = ['Risorsa', 'Percentuale'])      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione istogramma per presenza parole nelle risorse lessicali\n",
    "\n",
    "dfJoy_lexRes.plot.bar(x='Risorsa')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione istogramma per presenza parole nei twitter\n",
    "\n",
    "dfJoy_twitter.plot.bar(x='Risorsa')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connessione, creazione cursore e database\n",
    "\n",
    "conn = psycopg2.connect(database = \"maadb\", user = \"postgres\", password = \"admin\", host = \"localhost\", port = \"5432\")\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione tabella risorse lessicali joy e inserimento\n",
    "cur.execute('''CREATE TABLE RISORSEJOY\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    NOME             TEXT       NOT NULL,\n",
    "                    PAROLA           TEXT       NOT NULL\n",
    "                );\n",
    "            '''\n",
    ")\n",
    "nome = \"EmoSN_joy\"\n",
    "for word in resourceJoyEmoSN:\n",
    "    cur.execute(\"INSERT INTO RISORSEJOY (NOME, PAROLA) VALUES(%s, %s)\",(nome,word))\n",
    "\n",
    "nome = \"NRC_joy\"\n",
    "for word in resourceJoyNRC:\n",
    "    cur.execute(\"INSERT INTO RISORSEJOY (NOME, PAROLA) VALUES(%s, %s)\",(nome,word))\n",
    "\n",
    "nome = \"sentisense_joy\"\n",
    "for word in resourceJoySentisence:\n",
    "    cur.execute(\"INSERT INTO RISORSEJOY (NOME, PAROLA) VALUES(%s, %s)\",(nome,word))\n",
    "\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione tabelle per le parole, emoji, emoticon e hashtag\n",
    "\n",
    "cur.execute('''CREATE TABLE PAROLEJOY\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    PAROLA           TEXT       NOT NULL, \n",
    "                    EmoSN_joy      BOOL       NOT NULL DEFAULT FALSE,\n",
    "                    NRC_joy      BOOL       NOT NULL DEFAULT FALSE,\n",
    "                    sentisense_joy      BOOL       NOT NULL DEFAULT FALSE,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ")   \n",
    "\n",
    "cur.execute('''CREATE TABLE EMOJIJOY\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    EMOJI       TEXT       NOT NULL,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ") \n",
    "\n",
    "cur.execute('''CREATE TABLE EMOTICONSJOY\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    EMOTICONS       TEXT       NOT NULL,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ") \n",
    "\n",
    "cur.execute('''CREATE TABLE HASHTAGJOY\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    HASHTAG     TEXT       NOT NULL,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ") \n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inserimento hashtag, emoticon ed emoji\n",
    "\n",
    "for hashTag in hashtagJoy:\n",
    "    cur.execute(\"INSERT INTO HASHTAGJOY (HASHTAG,FREQUENCY) VALUES(%s,%s)\",(hashTag,0))\n",
    "\n",
    "for emoticons in emoticonJoy:\n",
    "    cur.execute(\"INSERT INTO EMOTICONSJOY (EMOTICONS,FREQUENCY) VALUES(%s,%s)\",(emoticons,0))\n",
    "\n",
    "for emojis in emojiJoy:\n",
    "    cur.execute(\"INSERT INTO EMOJIJOY (EMOJI,FREQUENCY) VALUES(%s,%s)\",(emojis,0))\n",
    "\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inserimento parole in db\n",
    "for word in joyListWord:\n",
    "    cur.execute(\"INSERT INTO PAROLEJOY (PAROLA,EmoSN_joy,NRC_joy,sentisense_joy,FREQUENCY) VALUES(%s,%s,%s,%s,%s)\",(word,False,False,False,0))\n",
    "\n",
    "conn.commit()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze hashtag\n",
    "cur.execute(\"update HASHTAGJOY as ha1 set frequency = (select count(*) from HASHTAGJOY ha2 where ha1.hashtag = ha2.hashtag);\")\n",
    "cur.execute(\"delete from HASHTAGJOY as ha1 where exists (Select 1 From HASHTAGJOY as ha2 where ha1.hashtag = ha2.hashtag and ha1.frequency = ha2.frequency and ha2.ctid > ha1.ctid);\")\n",
    "\n",
    "conn.commit()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze emoji\n",
    "cur.execute(\"update EMOJIJOY as ha1 set frequency = (select count(*) from EMOJIJOY ha2 where ha1.emoji = ha2.emoji);\")\n",
    "cur.execute(\"delete from EMOJIJOY as ha1 where exists (Select 1 From EMOJIJOY as ha2 where ha1.emoji = ha2.emoji and ha1.frequency = ha2.frequency and ha2.ctid > ha1.ctid);\")\n",
    "\n",
    "conn.commit() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze emoticons\n",
    "cur.execute(\"update EMOTICONSJOY as ha1 set frequency = (select count(*) from EMOTICONSJOY ha2 where ha1.emoticons = ha2.emoticons);\")\n",
    "cur.execute(\"delete from EMOTICONSJOY as ha1 where exists (Select 1 From EMOTICONSJOY as ha2 where ha1.emoticons = ha2.emoticons and ha1.frequency = ha2.frequency and ha2.ctid > ha1.ctid);\")\n",
    "\n",
    "conn.commit()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze parole\n",
    "cur.execute(\"select count(*),parola from PAROLEJOY group by parola\")\n",
    "result = []\n",
    "for word in cur.fetchall():\n",
    "    result.append(word) \n",
    "\n",
    "cur.execute(\"truncate table PAROLEJOY;\")\n",
    "cur.execute(\"ALTER SEQUENCE parolejoy_id_seq RESTART WITH 1;\")\n",
    "\n",
    "for couple in result:\n",
    "    cur.execute(\"INSERT INTO PAROLEJOY (PAROLA,EmoSN_joy,NRC_joy,sentisense_joy,FREQUENCY) VALUES(%s,%s,%s,%s,%s)\",(couple[1],False,False,False,couple[0]))\n",
    "\n",
    "conn.commit()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggiornamento presenza parola nelle risorse lessicali\n",
    "cur.execute(\"UPDATE PAROLEJOY SET EmoSN_joy = TRUE WHERE parola IN (select p.parola from PAROLEJOY as p LEFT OUTER JOIN RISORSEJOY as r ON p.parola = r.parola WHERE r.nome = 'EmoSN_joy')\")\n",
    "cur.execute(\"UPDATE PAROLEJOY SET NRC_joy = TRUE WHERE parola IN (select p.parola from PAROLEJOY as p LEFT OUTER JOIN RISORSEJOY as r ON p.parola = r.parola WHERE r.nome = 'NRC_joy')\")\n",
    "cur.execute(\"UPDATE PAROLEJOY SET sentisense_joy = TRUE WHERE parola IN (select p.parola from PAROLEJOY as p LEFT OUTER JOIN RISORSEJOY as r ON p.parola = r.parola WHERE r.nome = 'sentisense_joy')\")\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvataggio in un txt delle parole non presenti in nessuna risorsa lessicale\n",
    "\n",
    "wordsMongo = joyCollections.find({\"LexResources\": []}, {\"_id\": 0, \"freq\": 0, \"LexResources\": 0})\n",
    "words = []\n",
    "\n",
    "for word in wordsMongo:\n",
    "    words.append(word['word'])\n",
    "\n",
    "with open('Nuove Parole/joy.txt', 'w',encoding=\"utf8\") as f:\n",
    "    for word in words:\n",
    "        f.write(\"%s\\n\" % word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trattamento dataset Sadness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lista di righe, ogni riga una tweet sadness\n",
    "file = open(\"Dataset/dataset_dt_sadness_60k.txt\", 'r',encoding=\"utf8\")\n",
    "sadness = []\n",
    "hashtagSadness = []\n",
    "emoticonSadness = []\n",
    "emojiSadness = []\n",
    "\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "    riga = re.sub(r'URL', '',riga)\n",
    "    riga = re.sub(r'USERNAME', '',riga)\n",
    "\n",
    "    #array di hashtag singoli\n",
    "    hashTag = (re.findall('#[a-zA-Z0-9]+', riga))\n",
    "\n",
    "    #generiamo l'array di hashtag senza hashtag\n",
    "    for h in hashTag:\n",
    "        hashtagSadness.append(h.replace(\"#\",\"\"))\n",
    "\n",
    "    #Rimuoviamo glin hashtag dalla riga\n",
    "    riga = riga.replace('#',\"\")\n",
    "\n",
    "    sadness.append(riga)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sadness)):\n",
    "    #Controllo se l'emoticons c'è e in caso la rimuovo\n",
    "    for singleEmoticon in emoticonsList:\n",
    "        emoticonsFind = re.findall(re.escape(singleEmoticon),sadness[i])\n",
    "        if(len(emoticonsFind) != 0):\n",
    "            emoticonSadness.extend(emoticonsFind)\n",
    "            sadness[i] = sadness[i].replace(singleEmoticon,\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sadness)):\n",
    "    #preleviamo le emoji e le rimuoviamo dal testo\n",
    "    data = regex.findall(r'\\X', sadness[i])\n",
    "    for word in data:\n",
    "        if any(char in emoji.UNICODE_EMOJI['en'] for char in word):\n",
    "            emojiSadness.append(word)\n",
    "            sadness[i] = sadness[i].replace(word, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processamento slang prima di punteggiatura e lowercase\n",
    "with open(\"Slangs/slangs.json\") as f:\n",
    "    slangsBefore = json.load(f)\n",
    "    for i in range(len(sadness)):\n",
    "        for word in sadness[i].split():\n",
    "            indexOfWord = sadness[i].find(word)\n",
    "            if slangsBefore.get(word) is None:\n",
    "                if slangsBefore.get(word.lower()) is None:\n",
    "                    if slangsBefore.get(word.upper()) is None:\n",
    "                        continue\n",
    "                    else:\n",
    "                        sadness[i] = sadness[i][:indexOfWord] + slangsBefore.get(word.upper()) + sadness[i][indexOfWord+len(word):]\n",
    "                else:\n",
    "                    sadness[i] = sadness[i][:indexOfWord] + slangsBefore.get(word.lower()) + sadness[i][indexOfWord+len(word):]\n",
    "            else:\n",
    "                sadness[i] = sadness[i][:indexOfWord] + slangsBefore.get(word) + sadness[i][indexOfWord+len(word):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizzazione di tutte le frasi\n",
    "sadnessTokenized = []\n",
    "for linea in sadness:\n",
    "    result = word_tokenize(linea)\n",
    "    sadnessTokenized.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Esecuzione della POS tagging per ogni riga della matrice\n",
    "for i in range(len(sadnessTokenized)):\n",
    "    sadnessTokenized[i] = nltk.pos_tag(sadnessTokenized[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trattamento punteggiatura\n",
    "for line in sadnessTokenized:\n",
    "    newLine = line\n",
    "    listToRemove = []\n",
    "    for i in range(len(line)):\n",
    "        if line[i][0] in puntuaction:\n",
    "            listToRemove.append(line[i])\n",
    "    for element in listToRemove:\n",
    "        newLine.remove(element)\n",
    "    line = newLine\n",
    "\n",
    "linePuntuaction = re.compile(\"[\" + re.escape(\"\".join(puntuaction)) + \"]\")\n",
    "for line in sadnessTokenized:\n",
    "    for i in range(len(line)):\n",
    "        tupla = [(re.sub(linePuntuaction, \"\", line[i][0])),line[i][1]]\n",
    "        line[i] = tupla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming\n",
    "#ps = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for line in sadnessTokenized:\n",
    "    for i in range(len(line)):\n",
    "        tupla = [lemmatizer.lemmatize(line[i][0]),line[i][1]]\n",
    "        line[i] = tupla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trattamnento Stop word dopo Stemming\n",
    "stop_words = set(stopwords.words('english'))\n",
    "sadnessTokStoped = []\n",
    "for line in sadnessTokenized:\n",
    "    filtred_word = []\n",
    "    for row in line:\n",
    "        if  row[0] not in stop_words:\n",
    "            filtred_word.append(row) \n",
    "    sadnessTokStoped.append(filtred_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sadnessListWord = []\n",
    "sadnessListTok = []\n",
    "for line in sadnessTokStoped:\n",
    "    for row in line:\n",
    "        if row[0] != '':\n",
    "            sadnessListWord.append(row[0])\n",
    "        sadnessListTok.append(row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lavorazione risorse lessicali\n",
    "resourceSadnessNRC = []\n",
    "resourceSadnessSentisense = []\n",
    "file = open(\"Risorse lessicali\\\\Sadness\\\\NRC_sadness.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        resourceSadnessNRC.append(riga)\n",
    "\n",
    "file.close()\n",
    "\n",
    "file = open(\"Risorse lessicali\\Sadness\\sentisense_sadness.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        resourceSadnessSentisense.append(riga)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Caricare le risorse lessicali \n",
    "SadnessNRC = {\n",
    "    '_id': 'NRC_sadness',\n",
    "    'sentiment': 'sadness',  \n",
    "    'wordList': resourceSadnessNRC\n",
    "}\n",
    "\n",
    "SadnessSentisense = {\n",
    "    '_id': 'sentisense_sadness',\n",
    "    'sentiment': 'sadness',\n",
    "    'wordList': resourceSadnessSentisense\n",
    "}\n",
    "\n",
    "resourcesCollection.insert_one(SadnessNRC)\n",
    "resourcesCollection.insert_one(SadnessSentisense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione database e collezioni\n",
    "# Inserimento singoli documenti contententi i tweet sadness\n",
    "wordsString = {}\n",
    "\n",
    "for i in range(len(sadnessListWord)):\n",
    "    wordsString = {\n",
    "        \"lemma\": sadnessListWord[i],\n",
    "        \"POS\": sadnessListTok[i]\n",
    "    }\n",
    "    sadnessTwit={\"word\":wordsString}\n",
    "    sadnessCollections.insert_one(sadnessTwit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inserimento collezione Twitter con hashtags, emoticons ed emoji \n",
    "sadnessInsert = {\n",
    "    \"sentiment\":\"sadness\",\n",
    "    \"hashtags\": hashtagSadness,\n",
    "    \"emoticons\": emoticonSadness,\n",
    "    \"emoji\": emojiSadness\n",
    "}\n",
    "\n",
    "#inserimento\n",
    "x = collectionSadness.insert_one(sadnessInsert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raggruppa i documenti di input in base all'espressione _id specificata e per ciascun raggruppamento distinto genera un documento\n",
    "pipeline = [\n",
    "    {\"$group\": {\"_id\": \"$word.lemma\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"word\": \"$_id\",\n",
    "      \"freq\": \"$count\"\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"freq\": -1\n",
    "    }},\n",
    "    {\"$out\": \"sadness\"}\n",
    "]\n",
    "\n",
    "#post processing per renderlo valido \n",
    "sadnessCollections.aggregate(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creazione Word Clouds per le parole\n",
    "data = {}\n",
    "for document in sadnessCollections.find().limit(50):\n",
    "    data[document['word']] = document['freq']\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2)\n",
    "wc.fit_words(data)\n",
    "wc.to_file('Word Clouds/Sadness/sadnessWords.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conteggio degli hashtags\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$hashtags\"},\n",
    "    {\"$group\": {\"_id\": \"$hashtags\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"word\": \"$_id\",\n",
    "      \"count\": 1\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"count\": -1\n",
    "    }}\n",
    "]\n",
    "\n",
    "hashtagsCount = collectionSadness.aggregate(pipeline)\n",
    "\n",
    "data = {}\n",
    "for document in hashtagsCount:\n",
    "    data[document['word']] = document['count']\n",
    "\n",
    "collectionSadness.update_one(\n",
    "  {\"sentiment\": \"sadness\"},\n",
    "  {\"$set\": {\"hashtags\": data}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creazione Word Clouds per gli hashtags\n",
    "\n",
    "hashtags = collectionSadness.distinct(\"hashtags\")\n",
    "\n",
    "hashtagsLimited = {}\n",
    "for (word,count) in [x for x in hashtags[0].items()][:50]:\n",
    "    hashtagsLimited[word] = count\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2)\n",
    "wc.fit_words(hashtagsLimited)\n",
    "wc.to_file('Word Clouds/Sadness/sadnessHashtags.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conteggio delle emoji\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$emoji\"},\n",
    "    {\"$group\": {\"_id\": \"$emoji\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"emoji\": \"$_id\",\n",
    "      \"count\": 1\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"count\": -1\n",
    "    }}\n",
    "]\n",
    "\n",
    "emojiCount = collectionSadness.aggregate(pipeline)\n",
    "\n",
    "data = {}\n",
    "for document in emojiCount:\n",
    "    data[document['emoji']] = document['count']\n",
    "\n",
    "collectionSadness.update_one(\n",
    "  {\"sentiment\": \"sadness\"},\n",
    "  {\"$set\": {\"emoji\": data}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creazione Word Clouds per le emoji\n",
    "\n",
    "emojis = collectionSadness.distinct(\"emoji\")\n",
    "\n",
    "emojisLimited = {}\n",
    "for (word,count) in [x for x in emojis[0].items()][:50]:\n",
    "    emojisLimited[word] = count\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2, font_path='Font Emoji\\Symbola.otf')\n",
    "wc.fit_words(emojisLimited)\n",
    "wc.to_file('Word Clouds/Sadness/sadnessEmojis.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conteggio degli emoticons\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$emoticons\"},\n",
    "    {\"$group\": {\"_id\": \"$emoticons\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"emoticons\": \"$_id\",\n",
    "      \"count\": 1\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"count\": -1\n",
    "    }}\n",
    "]\n",
    "\n",
    "emoticonsCount = collectionSadness.aggregate(pipeline)\n",
    "\n",
    "data = {}\n",
    "for document in emoticonsCount:\n",
    "    data[document['emoticons']] = document['count']\n",
    "\n",
    "collectionSadness.update_one(\n",
    "  {\"sentiment\": \"sadness\"},\n",
    "  {\"$set\": {\"emoticons\": data}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creazione Word Clouds per le emoticons\n",
    "\n",
    "emoticons = collectionSadness.distinct(\"emoticons\")\n",
    "\n",
    "emoticonsLimited = {}\n",
    "for (word,count) in [x for x in emoticons[0].items()][:50]:\n",
    "    emoticonsLimited[word] = count\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2)\n",
    "wc.fit_words(emoticonsLimited)\n",
    "wc.to_file('Word Clouds/Sadness/sadnessEmoticons.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline di aggregazione per inserire la risorsa lessicale a cui appartiene la parola\n",
    "\n",
    "pipeline = [\n",
    "    {\"$lookup\": {\n",
    "        \"from\": \"Resources\",\n",
    "        \"localField\": \"word\",\n",
    "        \"foreignField\": \"wordList\",\n",
    "        \"pipeline\": [\n",
    "            {\"$match\": {\"$expr\": {\"$regexMatch\": {\"input\": \"$_id\", \"regex\": \"sadness\"}}}}\n",
    "        ],\n",
    "        \"as\": \"LexResources\"\n",
    "    }},\n",
    "    {\"$project\": {\n",
    "        \"resources.wordList\": 0,\n",
    "        \"resources.totNumberWords\": 0,\n",
    "        \"resources.sentiment\": 0,\n",
    "        \"LexResources.sentiment\": 0,\n",
    "        \"LexResources.totNumberWords\": 0,\n",
    "        \"LexResources.wordList\": 0\n",
    "    }},\n",
    "    {\"$out\": \"sadness\"}\n",
    "]\n",
    "\n",
    "sadnessCollections.aggregate(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline di aggregazione per calcolare le percentuali richieste\n",
    "\n",
    "pipelineCount = [\n",
    "    {\"$group\": {\"_id\": \"null\", \"count\": { \"$sum\": \"$freq\" }}},\n",
    "    {\"$project\": {\n",
    "        \"_id\": 0,\n",
    "        \"count\": \"$count\"\n",
    "    }}\n",
    "]\n",
    "\n",
    "results = sadnessCollections.aggregate(pipelineCount)\n",
    "\n",
    "countWords = 0\n",
    "for data in results:\n",
    "    countWords = data['count']\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$LexResources\"},\n",
    "    {\"$lookup\": {\n",
    "        \"from\": \"Resources\",\n",
    "        \"localField\": \"LexResources._id\",\n",
    "        \"foreignField\": \"_id\",\n",
    "        \"as\": \"RefResource\"\n",
    "    }},\n",
    "    {\"$group\": {\"_id\": { \"lex\": \"$LexResources._id\", \"RefResource\": \"$RefResource\" }, \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "        \"_id\": \"$_id.lex\",\n",
    "        \"wordList\": \"$_id.RefResource.wordList\",\n",
    "        \"commonWords\": \"$count\"\n",
    "    }},\n",
    "    {\"$unwind\": \"$wordList\"},\n",
    "    {\"$project\": {\n",
    "        \"_id\": 1,\n",
    "        \"commonWords\": 1,\n",
    "        \"totWordsLex\": {\"$size\": \"$wordList\"}\n",
    "    }},\n",
    "    {\"$addFields\": {\"totalWordsTwitter\": countWords}},\n",
    "    {\"$project\": {\n",
    "        \"perc_presence_lex_res\": { \"$round\": [{\"$divide\": [ \"$commonWords\", \"$totWordsLex\" ] }, 6] },\n",
    "        \"perc_presence_twitter\": { \"$round\": [{\"$divide\": [ \"$commonWords\", \"$totalWordsTwitter\" ]}, 6] }\n",
    "    }}\n",
    "]\n",
    "\n",
    "results = sadnessCollections.aggregate(pipeline)\n",
    "\n",
    "# Genero i dati per l'istogramma\n",
    "istogramSadness_lexRes = []\n",
    "istogramSadness_twitter = []\n",
    "\n",
    "for data in results:\n",
    "    print(data)\n",
    "    log =math.log(data['perc_presence_lex_res'],10)\n",
    "    istogramSadness_lexRes.append([data['_id'], data['perc_presence_lex_res']])\n",
    "    log =math.log(data['perc_presence_twitter'],10)\n",
    "    istogramSadness_twitter.append([data['_id'], data['perc_presence_twitter']])\n",
    " \n",
    "# Create the pandas DataFrame\n",
    "dfSadness_lexRes = pd.DataFrame(istogramSadness_lexRes, columns = ['Risorsa', 'Percentuale'])\n",
    "dfSadness_twitter = pd.DataFrame(istogramSadness_twitter,columns = ['Risorsa', 'Percentuale'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione istogramma per presenza parole nelle risorse lessicali\n",
    "\n",
    "dfSadness_lexRes.plot.bar(x='Risorsa')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione istogramma per presenza parole nei twitter\n",
    "\n",
    "dfSadness_twitter.plot.bar(x='Risorsa')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connessione, creazione cursore e database\n",
    "\n",
    "conn = psycopg2.connect(database = \"maadb\", user = \"postgres\", password = \"admin\", host = \"localhost\", port = \"5432\")\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione tabella risorse lessicali sadness e inserimento\n",
    "cur.execute('''CREATE TABLE RISORSESADNESS\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    NOME             TEXT       NOT NULL,\n",
    "                    PAROLA           TEXT       NOT NULL\n",
    "                );\n",
    "            '''\n",
    ")\n",
    "\n",
    "nome = \"NRC_sadness\"\n",
    "for word in resourceSadnessNRC:\n",
    "    cur.execute(\"INSERT INTO RISORSESADNESS (NOME, PAROLA) VALUES(%s, %s)\",(nome,word))\n",
    "\n",
    "nome = \"sentisense_sadness\"\n",
    "for word in resourceSadnessSentisense:\n",
    "    cur.execute(\"INSERT INTO RISORSESADNESS (NOME, PAROLA) VALUES(%s, %s)\",(nome,word))\n",
    "\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione tabelle per le parole, emoji, emoticon e hashtag\n",
    "\n",
    "cur.execute('''CREATE TABLE PAROLESADNESS\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    PAROLA           TEXT       NOT NULL,\n",
    "                    NRC_sadness      BOOL       NOT NULL DEFAULT FALSE,\n",
    "                    sentisense_sadness      BOOL       NOT NULL DEFAULT FALSE,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ")   \n",
    "\n",
    "cur.execute('''CREATE TABLE EMOJISADNESS\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    EMOJI       TEXT       NOT NULL,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ") \n",
    "\n",
    "cur.execute('''CREATE TABLE EMOTICONSSADNESS\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    EMOTICONS       TEXT       NOT NULL,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ") \n",
    "\n",
    "cur.execute('''CREATE TABLE HASHTAGSADNESS\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    HASHTAG     TEXT       NOT NULL,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ") \n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inserimento hashtag, emoticon ed emoji\n",
    "\n",
    "for hashTag in hashtagSadness:\n",
    "    cur.execute(\"INSERT INTO HASHTAGSADNESS (HASHTAG,FREQUENCY) VALUES(%s,%s)\",(hashTag,0))\n",
    "\n",
    "for emoticons in emoticonSadness:\n",
    "    cur.execute(\"INSERT INTO EMOTICONSSADNESS (EMOTICONS,FREQUENCY) VALUES(%s,%s)\",(emoticons,0))\n",
    "\n",
    "for emojis in emojiSadness:\n",
    "    cur.execute(\"INSERT INTO EMOJISADNESS (EMOJI,FREQUENCY) VALUES(%s,%s)\",(emojis,0))\n",
    "\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inserimento parole in db\n",
    "for word in sadnessListWord:\n",
    "    cur.execute(\"INSERT INTO PAROLESADNESS (PAROLA,NRC_sadness,sentisense_sadness,FREQUENCY) VALUES(%s,%s,%s,%s)\",(word,False,False,0))\n",
    "\n",
    "conn.commit()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze hashtag\n",
    "cur.execute(\"update HASHTAGSADNESS as ha1 set frequency = (select count(*) from HASHTAGSADNESS ha2 where ha1.hashtag = ha2.hashtag);\")\n",
    "cur.execute(\"delete from HASHTAGSADNESS as ha1 where exists (Select 1 From HASHTAGSADNESS as ha2 where ha1.hashtag = ha2.hashtag and ha1.frequency = ha2.frequency and ha2.ctid > ha1.ctid);\")\n",
    "\n",
    "conn.commit()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze emoji\n",
    "cur.execute(\"update EMOJISADNESS as ha1 set frequency = (select count(*) from EMOJISADNESS ha2 where ha1.emoji = ha2.emoji);\")\n",
    "cur.execute(\"delete from EMOJISADNESS as ha1 where exists (Select 1 From EMOJISADNESS as ha2 where ha1.emoji = ha2.emoji and ha1.frequency = ha2.frequency and ha2.ctid > ha1.ctid);\")\n",
    "\n",
    "conn.commit() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze emoticons\n",
    "cur.execute(\"update EMOTICONSSADNESS as ha1 set frequency = (select count(*) from EMOTICONSSADNESS ha2 where ha1.emoticons = ha2.emoticons);\")\n",
    "cur.execute(\"delete from EMOTICONSSADNESS as ha1 where exists (Select 1 From EMOTICONSSADNESS as ha2 where ha1.emoticons = ha2.emoticons and ha1.frequency = ha2.frequency and ha2.ctid > ha1.ctid);\")\n",
    "\n",
    "conn.commit()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze parole\n",
    "cur.execute(\"select count(*),parola from PAROLESADNESS group by parola\")\n",
    "result = []\n",
    "for word in cur.fetchall():\n",
    "    result.append(word) \n",
    "\n",
    "cur.execute(\"truncate table PAROLESADNESS;\")\n",
    "cur.execute(\"ALTER SEQUENCE parolesadness_id_seq RESTART WITH 1;\")\n",
    "\n",
    "for couple in result:\n",
    "    cur.execute(\"INSERT INTO PAROLESADNESS (PAROLA,NRC_sadness,sentisense_sadness,FREQUENCY) VALUES(%s,%s,%s,%s)\",(couple[1],False,False,couple[0]))\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggiornamento presenza parola nelle risorse lessicali\n",
    "cur.execute(\"UPDATE PAROLESADNESS SET NRC_sadness = TRUE WHERE parola IN (select p.parola from PAROLESADNESS as p LEFT OUTER JOIN RISORSESADNESS as r ON p.parola = r.parola WHERE r.nome = 'NRC_sadness')\")\n",
    "cur.execute(\"UPDATE PAROLESADNESS SET sentisense_sadness = TRUE WHERE parola IN (select p.parola from PAROLESADNESS as p LEFT OUTER JOIN RISORSESADNESS as r ON p.parola = r.parola WHERE r.nome = 'sentisense_sadness')\")\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvataggio in un txt delle parole non presenti in nessuna risorsa lessicale\n",
    "\n",
    "wordsMongo = sadnessCollections.find({\"LexResources\": []}, {\"_id\": 0, \"freq\": 0, \"LexResources\": 0})\n",
    "words = []\n",
    "\n",
    "for word in wordsMongo:\n",
    "    words.append(word['word'])\n",
    "\n",
    "with open('Nuove Parole/sadness.txt', 'w',encoding=\"utf8\") as f:\n",
    "    for word in words:\n",
    "        f.write(\"%s\\n\" % word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trattamento dataset Surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lista di righe, ogni riga una tweet surprise\n",
    "file = open(\"Dataset/dataset_dt_surprise_60k.txt\", 'r',encoding=\"utf8\")\n",
    "surprise = []\n",
    "hashtagSurprise = []\n",
    "emoticonSurprise = []\n",
    "emojiSurprise = []\n",
    "\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "    riga = re.sub(r'URL', '',riga)\n",
    "    riga = re.sub(r'USERNAME', '',riga)\n",
    "\n",
    "     #array di hashtag singoli\n",
    "    hashTag = (re.findall('#[a-zA-Z0-9]+', riga))\n",
    "\n",
    "    #generiamo l'array di hashtag senza hashtag\n",
    "    for h in hashTag:\n",
    "        hashtagSurprise.append(h.replace(\"#\",\"\"))\n",
    "\n",
    "    #Rimuoviamo glin hashtag dalla riga\n",
    "    riga = riga.replace('#',\"\")\n",
    "\n",
    "    surprise.append(riga)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(surprise)):\n",
    "    #Controllo se l'emoticons c'è e in caso la rimuovo\n",
    "    for singleEmoticon in emoticonsList:\n",
    "        emoticonsFind = re.findall(re.escape(singleEmoticon),surprise[i])\n",
    "        if(len(emoticonsFind) != 0):\n",
    "            emoticonSurprise.extend(emoticonsFind)\n",
    "            surprise[i] = surprise[i].replace(singleEmoticon,\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(surprise)):\n",
    "    #preleviamo le emoji e le rimuoviamo dal testo\n",
    "    data = regex.findall(r'\\X', surprise[i])\n",
    "    for word in data:\n",
    "        if any(char in emoji.UNICODE_EMOJI['en'] for char in word):\n",
    "            emojiSurprise.append(word)\n",
    "            surprise[i] = surprise[i].replace(word, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processamento slang prima di punteggiatura e lowercase\n",
    "with open(\"Slangs/slangs.json\") as f:\n",
    "    slangsBefore = json.load(f)\n",
    "    for i in range(len(surprise)):\n",
    "        for word in surprise[i].split():\n",
    "            indexOfWord = surprise[i].find(word)\n",
    "            if slangsBefore.get(word) is None:\n",
    "                if slangsBefore.get(word.lower()) is None:\n",
    "                    if slangsBefore.get(word.upper()) is None:\n",
    "                        continue\n",
    "                    else:\n",
    "                        surprise[i] = surprise[i][:indexOfWord] + slangsBefore.get(word.upper()) + surprise[i][indexOfWord+len(word):]\n",
    "                else:\n",
    "                    surprise[i] = surprise[i][:indexOfWord] + slangsBefore.get(word.lower()) + surprise[i][indexOfWord+len(word):]\n",
    "            else:\n",
    "                surprise[i] = surprise[i][:indexOfWord] + slangsBefore.get(word) + surprise[i][indexOfWord+len(word):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizzazione di tutte le frasi\n",
    "surpriseTokenized = []\n",
    "for linea in surprise:\n",
    "    result = word_tokenize(linea)\n",
    "    surpriseTokenized.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Esecuzione della POS tagging per ogni riga della matrice\n",
    "for i in range(len(surpriseTokenized)):\n",
    "    surpriseTokenized[i] = nltk.pos_tag(surpriseTokenized[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trattamento punteggiatura\n",
    "for line in surpriseTokenized:\n",
    "    newLine = line\n",
    "    listToRemove = []\n",
    "    for i in range(len(line)):\n",
    "        if line[i][0] in puntuaction:\n",
    "            listToRemove.append(line[i])\n",
    "    for element in listToRemove:\n",
    "        newLine.remove(element)\n",
    "    line = newLine\n",
    "\n",
    "linePuntuaction = re.compile(\"[\" + re.escape(\"\".join(puntuaction)) + \"]\")\n",
    "for line in surpriseTokenized:\n",
    "    for i in range(len(line)):\n",
    "        tupla = [(re.sub(linePuntuaction, \"\", line[i][0])),line[i][1]]\n",
    "        line[i] = tupla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming\n",
    "#ps = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for line in surpriseTokenized:\n",
    "    for i in range(len(line)):\n",
    "        tupla = [lemmatizer.lemmatize(line[i][0]),line[i][1]]\n",
    "        line[i] = tupla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trattamnento Stop word dopo Stemming\n",
    "stop_words = set(stopwords.words('english'))\n",
    "surpriseTokStoped = []\n",
    "for line in surpriseTokenized:\n",
    "    filtred_word = []\n",
    "    for row in line:\n",
    "        if  row[0] not in stop_words:\n",
    "            filtred_word.append(row) \n",
    "    surpriseTokStoped.append(filtred_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surpriseListWord = []\n",
    "surpriseListTok = []\n",
    "for line in surpriseTokStoped:\n",
    "    for row in line:\n",
    "        if row[0] != '':\n",
    "            surpriseListWord.append(row[0])\n",
    "        surpriseListTok.append(row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lavorazione risorse lessicali\n",
    "resourceSurpriseNRC = []\n",
    "resourceSurpriseSentisense = []\n",
    "file = open(\"Risorse lessicali\\\\Surprise\\\\NRC_surprise.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        resourceSurpriseNRC.append(riga)\n",
    "\n",
    "file.close()\n",
    "\n",
    "file = open(\"Risorse lessicali\\Surprise\\sentisense_surprise.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        resourceSurpriseSentisense.append(riga)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Caricare le risorse lessicali \n",
    "SurpriseNRC = {\n",
    "    '_id': 'NRC_surprise',\n",
    "    'sentiment': 'surprise',  \n",
    "    'wordList': resourceSurpriseNRC\n",
    "}\n",
    "\n",
    "SurpriseSentisense = {\n",
    "    '_id': 'sentisense_surprise',\n",
    "    'sentiment': 'surprise',\n",
    "    'wordList': resourceSurpriseSentisense\n",
    "}\n",
    "\n",
    "resourcesCollection.insert_one(SurpriseNRC)\n",
    "resourcesCollection.insert_one(SurpriseSentisense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione database e collezioni\n",
    "# Inserimento singoli documenti contententi i tweet surprise\n",
    "wordsString = {}\n",
    "\n",
    "for i in range(len(surpriseListWord)):\n",
    "    wordsString = {\n",
    "        \"lemma\": surpriseListWord[i],\n",
    "        \"POS\": surpriseListTok[i]\n",
    "    }\n",
    "    surpriseTwit={\"word\":wordsString}\n",
    "    surpriseCollections.insert_one(surpriseTwit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inserimento collezione Twitter con hashtags, emoticons ed emoji \n",
    "surpriseInsert = {\n",
    "    \"sentiment\":\"surprise\",\n",
    "    \"hashtags\": hashtagSurprise,\n",
    "    \"emoticons\": emoticonSurprise,\n",
    "    \"emoji\": emojiSurprise\n",
    "}\n",
    "\n",
    "#inserimento\n",
    "x = collectionSurprise.insert_one(surpriseInsert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raggruppa i documenti di input in base all'espressione _id specificata e per ciascun raggruppamento distinto genera un documento\n",
    "pipeline = [\n",
    "    {\"$group\": {\"_id\": \"$word.lemma\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"word\": \"$_id\",\n",
    "      \"freq\": \"$count\"\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"freq\": -1\n",
    "    }},\n",
    "    {\"$out\": \"surprise\"}\n",
    "]\n",
    "\n",
    "#post processing per renderlo valido \n",
    "surpriseCollections.aggregate(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creazione Word Clouds per le parole\n",
    "data = {}\n",
    "for document in surpriseCollections.find().limit(50):\n",
    "    data[document['word']] = document['freq']\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2)\n",
    "wc.fit_words(data)\n",
    "wc.to_file('Word Clouds/Surprise/surpriseWords.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conteggio degli hashtags\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$hashtags\"},\n",
    "    {\"$group\": {\"_id\": \"$hashtags\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"word\": \"$_id\",\n",
    "      \"count\": 1\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"count\": -1\n",
    "    }}\n",
    "]\n",
    "\n",
    "hashtagsCount = collectionSurprise.aggregate(pipeline)\n",
    "\n",
    "data = {}\n",
    "for document in hashtagsCount:\n",
    "    data[document['word']] = document['count']\n",
    "\n",
    "collectionSurprise.update_one(\n",
    "  {\"sentiment\": \"surprise\"},\n",
    "  {\"$set\": {\"hashtags\": data}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creazione Word Clouds per gli hashtags\n",
    "\n",
    "hashtags = collectionSurprise.distinct(\"hashtags\")\n",
    "\n",
    "hashtagsLimited = {}\n",
    "for (word,count) in [x for x in hashtags[0].items()][:50]:\n",
    "    hashtagsLimited[word] = count\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2)\n",
    "wc.fit_words(hashtagsLimited)\n",
    "wc.to_file('Word Clouds/Surprise/surpriseHashtags.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conteggio delle emoji\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$emoji\"},\n",
    "    {\"$group\": {\"_id\": \"$emoji\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"emoji\": \"$_id\",\n",
    "      \"count\": 1\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"count\": -1\n",
    "    }}\n",
    "]\n",
    "\n",
    "emojiCount = collectionSurprise.aggregate(pipeline)\n",
    "\n",
    "data = {}\n",
    "for document in emojiCount:\n",
    "    data[document['emoji']] = document['count']\n",
    "\n",
    "collectionSurprise.update_one(\n",
    "  {\"sentiment\": \"surprise\"},\n",
    "  {\"$set\": {\"emoji\": data}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creazione Word Clouds per le emoji\n",
    "\n",
    "emojis = collectionSurprise.distinct(\"emoji\")\n",
    "\n",
    "emojisLimited = {}\n",
    "for (word,count) in [x for x in emojis[0].items()][:50]:\n",
    "    emojisLimited[word] = count\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2, font_path='Font Emoji\\Symbola.otf')\n",
    "wc.fit_words(emojisLimited)\n",
    "wc.to_file('Word Clouds/Surprise/surpriseEmojis.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conteggio degli emoticons\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$emoticons\"},\n",
    "    {\"$group\": {\"_id\": \"$emoticons\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"emoticons\": \"$_id\",\n",
    "      \"count\": 1\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"count\": -1\n",
    "    }}\n",
    "]\n",
    "\n",
    "emoticonsCount = collectionSurprise.aggregate(pipeline)\n",
    "\n",
    "data = {}\n",
    "for document in emoticonsCount:\n",
    "    data[document['emoticons']] = document['count']\n",
    "\n",
    "collectionSurprise.update_one(\n",
    "  {\"sentiment\": \"surprise\"},\n",
    "  {\"$set\": {\"emoticons\": data}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creazione Word Clouds per le emoticons\n",
    "\n",
    "emoticons = collectionSurprise.distinct(\"emoticons\")\n",
    "\n",
    "emoticonsLimited = {}\n",
    "for (word,count) in [x for x in emoticons[0].items()][:50]:\n",
    "    emoticonsLimited[word] = count\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2)\n",
    "wc.fit_words(emoticonsLimited)\n",
    "wc.to_file('Word Clouds/Surprise/surpriseEmoticons.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline di aggregazione per inserire la risorsa lessicale a cui appartiene la parola\n",
    "\n",
    "pipeline = [\n",
    "    {\"$lookup\": {\n",
    "        \"from\": \"Resources\",\n",
    "        \"localField\": \"word\",\n",
    "        \"foreignField\": \"wordList\",\n",
    "        \"pipeline\": [\n",
    "            {\"$match\": {\"$expr\": {\"$regexMatch\": {\"input\": \"$_id\", \"regex\": \"surprise\"}}}}\n",
    "        ],\n",
    "        \"as\": \"LexResources\"\n",
    "    }},\n",
    "    {\"$project\": {\n",
    "        \"resources.wordList\": 0,\n",
    "        \"resources.totNumberWords\": 0,\n",
    "        \"resources.sentiment\": 0,\n",
    "        \"LexResources.sentiment\": 0,\n",
    "        \"LexResources.totNumberWords\": 0,\n",
    "        \"LexResources.wordList\": 0\n",
    "    }},\n",
    "    {\"$out\": \"surprise\"}\n",
    "]\n",
    "\n",
    "surpriseCollections.aggregate(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline di aggregazione per calcolare le percentuali richieste\n",
    "\n",
    "pipelineCount = [\n",
    "    {\"$group\": {\"_id\": \"null\", \"count\": { \"$sum\": \"$freq\" }}},\n",
    "    {\"$project\": {\n",
    "        \"_id\": 0,\n",
    "        \"count\": \"$count\"\n",
    "    }}\n",
    "]\n",
    "\n",
    "results = surpriseCollections.aggregate(pipelineCount)\n",
    "\n",
    "countWords = 0\n",
    "for data in results:\n",
    "    countWords = data['count']\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$LexResources\"},\n",
    "    {\"$lookup\": {\n",
    "        \"from\": \"Resources\",\n",
    "        \"localField\": \"LexResources._id\",\n",
    "        \"foreignField\": \"_id\",\n",
    "        \"as\": \"RefResource\"\n",
    "    }},\n",
    "    {\"$group\": {\"_id\": { \"lex\": \"$LexResources._id\", \"RefResource\": \"$RefResource\" }, \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "        \"_id\": \"$_id.lex\",\n",
    "        \"wordList\": \"$_id.RefResource.wordList\",\n",
    "        \"commonWords\": \"$count\"\n",
    "    }},\n",
    "    {\"$unwind\": \"$wordList\"},\n",
    "    {\"$project\": {\n",
    "        \"_id\": 1,\n",
    "        \"commonWords\": 1,\n",
    "        \"totWordsLex\": {\"$size\": \"$wordList\"}\n",
    "    }},\n",
    "    {\"$addFields\": {\"totalWordsTwitter\": countWords}},\n",
    "    {\"$project\": {\n",
    "        \"perc_presence_lex_res\": { \"$round\": [{\"$divide\": [ \"$commonWords\", \"$totWordsLex\" ] }, 6] },\n",
    "        \"perc_presence_twitter\": { \"$round\": [{\"$divide\": [ \"$commonWords\", \"$totalWordsTwitter\" ]}, 6] }\n",
    "    }}\n",
    "]\n",
    "\n",
    "results = surpriseCollections.aggregate(pipeline)\n",
    "\n",
    "# Genero i dati per l'istogramma\n",
    "istogramSurprise_lexRes = []\n",
    "istogramSurprise_twitter = []\n",
    "\n",
    "for data in results:\n",
    "    print(data)\n",
    "    log =math.log(data['perc_presence_lex_res'],10)\n",
    "    istogramSurprise_lexRes.append([data['_id'], data['perc_presence_lex_res']])\n",
    "    log =math.log(data['perc_presence_twitter'],10)\n",
    "    istogramSurprise_twitter.append([data['_id'], data['perc_presence_twitter']])\n",
    " \n",
    "# Create the pandas DataFrame\n",
    "dfSurprise_lexRes = pd.DataFrame(istogramSurprise_lexRes, columns = ['Risorsa', 'Percentuale'])\n",
    "dfSurprise_twitter = pd.DataFrame(istogramSurprise_twitter,columns = ['Risorsa', 'Percentuale']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione istogramma per presenza parole nelle risorse lessicali\n",
    "\n",
    "dfSurprise_lexRes.plot.bar(x='Risorsa')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione istogramma per presenza parole nei twitter\n",
    "\n",
    "dfSurprise_twitter.plot.bar(x='Risorsa')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connessione, creazione cursore e database\n",
    "\n",
    "conn = psycopg2.connect(database = \"maadb\", user = \"postgres\", password = \"admin\", host = \"localhost\", port = \"5432\")\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione tabella risorse lessicali surprise e inserimento\n",
    "cur.execute('''CREATE TABLE RISORSESURPRISE\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    NOME             TEXT       NOT NULL,\n",
    "                    PAROLA           TEXT       NOT NULL\n",
    "                );\n",
    "            '''\n",
    ")\n",
    "\n",
    "nome = \"NRC_surprise\"\n",
    "for word in resourceSurpriseNRC:\n",
    "    cur.execute(\"INSERT INTO RISORSESURPRISE (NOME, PAROLA) VALUES(%s, %s)\",(nome,word))\n",
    "\n",
    "nome = \"sentisense_surprise\"\n",
    "for word in resourceSurpriseSentisense:\n",
    "    cur.execute(\"INSERT INTO RISORSESURPRISE (NOME, PAROLA) VALUES(%s, %s)\",(nome,word))\n",
    "\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione tabelle per le parole, emoji, emoticon e hashtag\n",
    "\n",
    "cur.execute('''CREATE TABLE PAROLESURPRISE\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    PAROLA           TEXT       NOT NULL,\n",
    "                    NRC_surprise      BOOL       NOT NULL DEFAULT FALSE,\n",
    "                    sentisense_surprise      BOOL       NOT NULL DEFAULT FALSE,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ")   \n",
    "\n",
    "cur.execute('''CREATE TABLE EMOJISURPRISE\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    EMOJI       TEXT       NOT NULL,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ") \n",
    "\n",
    "cur.execute('''CREATE TABLE EMOTICONSSURPRISE\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    EMOTICONS       TEXT       NOT NULL,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ") \n",
    "\n",
    "cur.execute('''CREATE TABLE HASHTAGSURPRISE\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    HASHTAG     TEXT       NOT NULL,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ") \n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inserimento hashtag, emoticon ed emoji\n",
    "\n",
    "for hashTag in hashtagSurprise:\n",
    "    cur.execute(\"INSERT INTO HASHTAGSURPRISE (HASHTAG,FREQUENCY) VALUES(%s,%s)\",(hashTag,0))\n",
    "\n",
    "for emoticons in emoticonSurprise:\n",
    "    cur.execute(\"INSERT INTO EMOTICONSSURPRISE (EMOTICONS,FREQUENCY) VALUES(%s,%s)\",(emoticons,0))\n",
    "\n",
    "for emojis in emojiSurprise:\n",
    "    cur.execute(\"INSERT INTO EMOJISURPRISE (EMOJI,FREQUENCY) VALUES(%s,%s)\",(emojis,0))\n",
    "\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inserimento parole in db\n",
    "for word in surpriseListWord:\n",
    "    cur.execute(\"INSERT INTO PAROLESURPRISE (PAROLA,NRC_surprise,sentisense_surprise,FREQUENCY) VALUES(%s,%s,%s,%s)\",(word,False,False,0))\n",
    "\n",
    "conn.commit() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze hashtag\n",
    "cur.execute(\"update hashtagsurprise as ha1 set frequency = (select count(*) from hashtagsurprise ha2 where ha1.hashtag = ha2.hashtag);\")\n",
    "cur.execute(\"delete from hashtagsurprise as ha1 where exists (Select 1 From hashtagsurprise as ha2 where ha1.hashtag = ha2.hashtag and ha1.frequency = ha2.frequency and ha2.ctid > ha1.ctid);\")\n",
    "\n",
    "conn.commit()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze emoji\n",
    "cur.execute(\"update EMOJISURPRISE as ha1 set frequency = (select count(*) from EMOJISURPRISE ha2 where ha1.emoji = ha2.emoji);\")\n",
    "cur.execute(\"delete from EMOJISURPRISE as ha1 where exists (Select 1 From EMOJISURPRISE as ha2 where ha1.emoji = ha2.emoji and ha1.frequency = ha2.frequency and ha2.ctid > ha1.ctid);\")\n",
    "\n",
    "conn.commit()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze emoticons\n",
    "cur.execute(\"update EMOTICONSSURPRISE as ha1 set frequency = (select count(*) from EMOTICONSSURPRISE ha2 where ha1.emoticons = ha2.emoticons);\")\n",
    "cur.execute(\"delete from EMOTICONSSURPRISE as ha1 where exists (Select 1 From EMOTICONSSURPRISE as ha2 where ha1.emoticons = ha2.emoticons and ha1.frequency = ha2.frequency and ha2.ctid > ha1.ctid);\")\n",
    "\n",
    "conn.commit()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze parole\n",
    "cur.execute(\"select count(*),parola from parolesurprise group by parola\")\n",
    "result = []\n",
    "for word in cur.fetchall():\n",
    "    result.append(word) \n",
    "\n",
    "cur.execute(\"truncate table parolesurprise;\")\n",
    "cur.execute(\"ALTER SEQUENCE parolesurprise_id_seq RESTART WITH 1;\")\n",
    "\n",
    "for couple in result:\n",
    "    cur.execute(\"INSERT INTO PAROLESURPRISE (PAROLA,NRC_surprise,sentisense_surprise,FREQUENCY) VALUES(%s,%s,%s,%s)\",(couple[1],False,False,couple[0]))\n",
    "\n",
    "conn.commit()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggiornamento presenza parola nelle risorse lessicali\n",
    "cur.execute(\"UPDATE parolesurprise SET NRC_SURPRISE = TRUE WHERE parola IN (select p.parola from parolesurprise as p LEFT OUTER JOIN risorsesurprise as r ON p.parola = r.parola WHERE r.nome = 'NRC_surprise')\")\n",
    "cur.execute(\"UPDATE parolesurprise SET sentisense_surprise = TRUE WHERE parola IN (select p.parola from parolesurprise as p LEFT OUTER JOIN risorsesurprise as r ON p.parola = r.parola WHERE r.nome = 'sentisense_surprise')\")\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvataggio in un txt delle parole non presenti in nessuna risorsa lessicale\n",
    "\n",
    "wordsMongo = surpriseCollections.find({\"LexResources\": []}, {\"_id\": 0, \"freq\": 0, \"LexResources\": 0})\n",
    "words = []\n",
    "\n",
    "for word in wordsMongo:\n",
    "    words.append(word['word'])\n",
    "\n",
    "with open('Nuove Parole/surprise.txt', 'w',encoding=\"utf8\") as f:\n",
    "    for word in words:\n",
    "        f.write(\"%s\\n\" % word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trattamento dataset Trust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lista di righe, ogni riga una tweet trust\n",
    "file = open(\"Dataset/dataset_dt_trust_60k.txt\", 'r',encoding=\"utf8\")\n",
    "trust = []\n",
    "hashtagTrust = []\n",
    "emoticonTrust = []\n",
    "emojiTrust = []\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "    riga = re.sub(r'URL', '',riga)\n",
    "    riga = re.sub(r'USERNAME', '',riga)\n",
    "    \n",
    "    #array di hashtag singoli\n",
    "    hashTag = (re.findall('#[a-zA-Z0-9]+', riga))\n",
    "    \n",
    "    #generiamo l'array di hashtag senza hashtag\n",
    "    for h in hashTag:\n",
    "        hashtagTrust.append(h.replace(\"#\",\"\"))\n",
    "\n",
    "    #Rimuoviamo glin hashtag dalla riga\n",
    "    riga = riga.replace('#',\"\")\n",
    "\n",
    "    trust.append(riga)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(trust)):\n",
    "    #Controllo se l'emoticons c'è e in caso la rimuovo\n",
    "    for singleEmoticon in emoticonsList:\n",
    "        emoticonsFind = re.findall(re.escape(singleEmoticon),trust[i])\n",
    "        if(len(emoticonsFind) != 0):\n",
    "            emoticonTrust.extend(emoticonsFind)\n",
    "            trust[i] = trust[i].replace(singleEmoticon,\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(trust)):\n",
    "    #preleviamo le emoji e le rimuoviamo dal testo\n",
    "    data = regex.findall(r'\\X', trust[i])\n",
    "    for word in data:\n",
    "        if any(char in emoji.UNICODE_EMOJI['en'] for char in word):\n",
    "            emojiTrust.append(word)\n",
    "            trust[i] = trust[i].replace(word, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processamento slang prima di punteggiatura e lowercase\n",
    "with open(\"Slangs/slangs.json\") as f:\n",
    "    slangsBefore = json.load(f)\n",
    "    for i in range(len(trust)):\n",
    "        for word in trust[i].split():\n",
    "            indexOfWord = trust[i].find(word)\n",
    "            if slangsBefore.get(word) is None:\n",
    "                if slangsBefore.get(word.lower()) is None:\n",
    "                    if slangsBefore.get(word.upper()) is None:\n",
    "                        continue\n",
    "                    else:\n",
    "                        trust[i] = trust[i][:indexOfWord] + slangsBefore.get(word.upper()) + trust[i][indexOfWord+len(word):]\n",
    "                else:\n",
    "                    trust[i] = trust[i][:indexOfWord] + slangsBefore.get(word.lower()) + trust[i][indexOfWord+len(word):]\n",
    "            else:\n",
    "                trust[i] = trust[i][:indexOfWord] + slangsBefore.get(word) + trust[i][indexOfWord+len(word):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizzazione di tutte le frasi\n",
    "trustTokenized = []\n",
    "for linea in trust:\n",
    "    result = word_tokenize(linea)\n",
    "    trustTokenized.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Esecuzione della POS tagging per ogni riga della matrice\n",
    "for i in range(len(trustTokenized)):\n",
    "    trustTokenized[i] = nltk.pos_tag(trustTokenized[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trattamento punteggiatura\n",
    "for line in trustTokenized:\n",
    "    newLine = line\n",
    "    listToRemove = []\n",
    "    for i in range(len(line)):\n",
    "        if line[i][0] in puntuaction:\n",
    "            listToRemove.append(line[i])\n",
    "    for element in listToRemove:\n",
    "        newLine.remove(element)\n",
    "    line = newLine\n",
    "\n",
    "linePuntuaction = re.compile(\"[\" + re.escape(\"\".join(puntuaction)) + \"]\")\n",
    "for line in trustTokenized:\n",
    "    for i in range(len(line)):\n",
    "        tupla = [(re.sub(linePuntuaction, \"\", line[i][0])),line[i][1]]\n",
    "        line[i] = tupla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming\n",
    "#ps = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for line in trustTokenized:\n",
    "    for i in range(len(line)):\n",
    "        tupla = [lemmatizer.lemmatize(line[i][0]),line[i][1]]\n",
    "        line[i] = tupla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trattamnento Stop word dopo Stemming\n",
    "stop_words = set(stopwords.words('english'))\n",
    "trustTokStoped = []\n",
    "for line in trustTokenized:\n",
    "    filtred_word = []\n",
    "    for row in line:\n",
    "        if  row[0] not in stop_words:\n",
    "            filtred_word.append(row) \n",
    "    trustTokStoped.append(filtred_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trustListWord = []\n",
    "trustListTok = []\n",
    "for line in trustTokStoped:\n",
    "    for row in line:\n",
    "        if row[0] != '':\n",
    "            trustListWord.append(row[0])\n",
    "        trustListTok.append(row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lavorazione risorse lessicali\n",
    "resourceTrustNRC = []\n",
    "file = open(\"Risorse lessicali\\\\Trust\\\\NRC_trust.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        resourceTrustNRC.append(riga)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Caricare le risorse lessicali \n",
    "TrustNRC = {\n",
    "    '_id': 'NRC_trust',\n",
    "    'sentiment': 'trust',  \n",
    "    'wordList': resourceTrustNRC\n",
    "}\n",
    "\n",
    "resourcesCollection.insert_one(TrustNRC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione database e collezioni\n",
    "# Inserimento singoli documenti contententi i tweet trust\n",
    "wordsString = {}\n",
    "\n",
    "for i in range(len(trustListWord)):\n",
    "    wordsString = {\n",
    "        \"lemma\": trustListWord[i],\n",
    "        \"POS\": trustListTok[i]\n",
    "    }\n",
    "    trustTwit={\"word\":wordsString}\n",
    "    trustCollections.insert_one(trustTwit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inserimento collezione Twitter con hashtags, emoticons ed emoji \n",
    "trustInsert = {\n",
    "    \"sentiment\":\"trust\",\n",
    "    \"hashtags\": hashtagTrust,\n",
    "    \"emoticons\": emoticonTrust,\n",
    "    \"emoji\": emojiTrust\n",
    "}\n",
    "\n",
    "#inserimento\n",
    "x = collectionTrust.insert_one(trustInsert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raggruppa i documenti di input in base all'espressione _id specificata e per ciascun raggruppamento distinto genera un documento\n",
    "pipeline = [\n",
    "    {\"$group\": {\"_id\": \"$word.lemma\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"word\": \"$_id\",\n",
    "      \"freq\": \"$count\"\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"freq\": -1\n",
    "    }},\n",
    "    {\"$out\": \"trust\"}\n",
    "]\n",
    "\n",
    "#post processing per renderlo valido \n",
    "trustCollections.aggregate(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creazione Word Clouds per le parole\n",
    "data = {}\n",
    "for document in trustCollections.find().limit(50):\n",
    "    data[document['word']] = document['freq']\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2)\n",
    "wc.fit_words(data)\n",
    "wc.to_file('Word Clouds/Trust/trustWords.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conteggio degli hashtags\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$hashtags\"},\n",
    "    {\"$group\": {\"_id\": \"$hashtags\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"word\": \"$_id\",\n",
    "      \"count\": 1\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"count\": -1\n",
    "    }}\n",
    "]\n",
    "\n",
    "hashtagsCount = collectionTrust.aggregate(pipeline)\n",
    "\n",
    "data = {}\n",
    "for document in hashtagsCount:\n",
    "    data[document['word']] = document['count']\n",
    "\n",
    "collectionTrust.update_one(\n",
    "  {\"sentiment\": \"trust\"},\n",
    "  {\"$set\": {\"hashtags\": data}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creazione Word Clouds per gli hashtags\n",
    "\n",
    "hashtags = collectionTrust.distinct(\"hashtags\")\n",
    "\n",
    "hashtagsLimited = {}\n",
    "for (word,count) in [x for x in hashtags[0].items()][:50]:\n",
    "    hashtagsLimited[word] = count\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2)\n",
    "wc.fit_words(hashtagsLimited)\n",
    "wc.to_file('Word Clouds/Trust/trustHashtags.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conteggio delle emoji\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$emoji\"},\n",
    "    {\"$group\": {\"_id\": \"$emoji\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"emoji\": \"$_id\",\n",
    "      \"count\": 1\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"count\": -1\n",
    "    }}\n",
    "]\n",
    "\n",
    "emojiCount = collectionTrust.aggregate(pipeline)\n",
    "\n",
    "data = {}\n",
    "for document in emojiCount:\n",
    "    data[document['emoji']] = document['count']\n",
    "\n",
    "collectionTrust.update_one(\n",
    "  {\"sentiment\": \"trust\"},\n",
    "  {\"$set\": {\"emoji\": data}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creazione Word Clouds per le emoji\n",
    "\n",
    "emojis = collectionTrust.distinct(\"emoji\")\n",
    "\n",
    "emojisLimited = {}\n",
    "for (word,count) in [x for x in emojis[0].items()][:50]:\n",
    "    emojisLimited[word] = count\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2, font_path='Font Emoji\\Symbola.otf')\n",
    "wc.fit_words(emojisLimited)\n",
    "wc.to_file('Word Clouds/Trust/trustEmojis.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conteggio degli emoticons\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$emoticons\"},\n",
    "    {\"$group\": {\"_id\": \"$emoticons\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"emoticons\": \"$_id\",\n",
    "      \"count\": 1\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"count\": -1\n",
    "    }}\n",
    "]\n",
    "\n",
    "emoticonsCount = collectionTrust.aggregate(pipeline)\n",
    "\n",
    "data = {}\n",
    "for document in emoticonsCount:\n",
    "    data[document['emoticons']] = document['count']\n",
    "\n",
    "collectionTrust.update_one(\n",
    "  {\"sentiment\": \"trust\"},\n",
    "  {\"$set\": {\"emoticons\": data}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creazione Word Clouds per le emoticons\n",
    "\n",
    "emoticons = collectionTrust.distinct(\"emoticons\")\n",
    "\n",
    "emoticonsLimited = {}\n",
    "for (word,count) in [x for x in emoticons[0].items()][:50]:\n",
    "    emoticonsLimited[word] = count\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2)\n",
    "wc.fit_words(emoticonsLimited)\n",
    "wc.to_file('Word Clouds/Trust/trustEmoticons.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline di aggregazione per inserire la risorsa lessicale a cui appartiene la parola\n",
    "\n",
    "pipeline = [\n",
    "    {\"$lookup\": {\n",
    "        \"from\": \"Resources\",\n",
    "        \"localField\": \"word\",\n",
    "        \"foreignField\": \"wordList\",\n",
    "        \"pipeline\": [\n",
    "            {\"$match\": {\"$expr\": {\"$regexMatch\": {\"input\": \"$_id\", \"regex\": \"trust\"}}}}\n",
    "        ],\n",
    "        \"as\": \"LexResources\"\n",
    "    }},\n",
    "    {\"$project\": {\n",
    "        \"resources.wordList\": 0,\n",
    "        \"resources.totNumberWords\": 0,\n",
    "        \"resources.sentiment\": 0,\n",
    "        \"LexResources.sentiment\": 0,\n",
    "        \"LexResources.totNumberWords\": 0,\n",
    "        \"LexResources.wordList\": 0\n",
    "    }},\n",
    "    {\"$out\": \"trust\"}\n",
    "]\n",
    "\n",
    "trustCollections.aggregate(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline di aggregazione per calcolare le percentuali richieste\n",
    "\n",
    "pipelineCount = [\n",
    "    {\"$group\": {\"_id\": \"null\", \"count\": { \"$sum\": \"$freq\" }}},\n",
    "    {\"$project\": {\n",
    "        \"_id\": 0,\n",
    "        \"count\": \"$count\"\n",
    "    }}\n",
    "]\n",
    "\n",
    "results = trustCollections.aggregate(pipelineCount)\n",
    "\n",
    "countWords = 0\n",
    "for data in results:\n",
    "    countWords = data['count']\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$LexResources\"},\n",
    "    {\"$lookup\": {\n",
    "        \"from\": \"Resources\",\n",
    "        \"localField\": \"LexResources._id\",\n",
    "        \"foreignField\": \"_id\",\n",
    "        \"as\": \"RefResource\"\n",
    "    }},\n",
    "    {\"$group\": {\"_id\": { \"lex\": \"$LexResources._id\", \"RefResource\": \"$RefResource\" }, \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "        \"_id\": \"$_id.lex\",\n",
    "        \"wordList\": \"$_id.RefResource.wordList\",\n",
    "        \"commonWords\": \"$count\"\n",
    "    }},\n",
    "    {\"$unwind\": \"$wordList\"},\n",
    "    {\"$project\": {\n",
    "        \"_id\": 1,\n",
    "        \"commonWords\": 1,\n",
    "        \"totWordsLex\": {\"$size\": \"$wordList\"}\n",
    "    }},\n",
    "    {\"$addFields\": {\"totalWordsTwitter\": countWords}},\n",
    "    {\"$project\": {\n",
    "        \"perc_presence_lex_res\": { \"$round\": [{\"$divide\": [ \"$commonWords\", \"$totWordsLex\" ] }, 6] },\n",
    "        \"perc_presence_twitter\": { \"$round\": [{\"$divide\": [ \"$commonWords\", \"$totalWordsTwitter\" ]}, 6] }\n",
    "    }}\n",
    "]\n",
    "\n",
    "results = trustCollections.aggregate(pipeline)\n",
    "\n",
    "# Genero i dati per l'istogramma\n",
    "istogramTrust_lexRes = []\n",
    "istogramTrust_twitter = []\n",
    "\n",
    "for data in results:\n",
    "    print(data)\n",
    "    log =math.log(data['perc_presence_lex_res'],10)\n",
    "    istogramTrust_lexRes.append([data['_id'], data['perc_presence_lex_res']])\n",
    "    log =math.log(data['perc_presence_twitter'],10)\n",
    "    istogramTrust_twitter.append([data['_id'], data['perc_presence_twitter']])\n",
    " \n",
    "# Create the pandas DataFrame\n",
    "dfTrust_lexRes = pd.DataFrame(istogramTrust_lexRes, columns = ['Risorsa', 'Percentuale'])\n",
    "dfTrust_twitter = pd.DataFrame(istogramTrust_twitter,columns = ['Risorsa', 'Percentuale'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione istogramma per presenza parole nelle risorse lessicali\n",
    "\n",
    "dfTrust_lexRes.plot.bar(x='Risorsa')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione istogramma per presenza parole nei twitter\n",
    "\n",
    "dfTrust_twitter.plot.bar(x='Risorsa')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connessione, creazione cursore e database\n",
    "\n",
    "conn = psycopg2.connect(database = \"maadb\", user = \"postgres\", password = \"admin\", host = \"localhost\", port = \"5432\")\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione tabella risorse lessicali trust e inserimento\n",
    "cur.execute('''CREATE TABLE RISORSETRUST\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    NOME             TEXT       NOT NULL,\n",
    "                    PAROLA           TEXT       NOT NULL\n",
    "                );\n",
    "            '''\n",
    ")\n",
    "\n",
    "nome = \"NRC_trust\"\n",
    "for word in resourceTrustNRC:\n",
    "    cur.execute(\"INSERT INTO RISORSETRUST (NOME, PAROLA) VALUES(%s, %s)\",(nome,word))\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione tabelle per le parole, emoji, emoticon e hashtag\n",
    "\n",
    "cur.execute('''CREATE TABLE PAROLETRUST\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    PAROLA           TEXT       NOT NULL,\n",
    "                    NRC_trust      BOOL       NOT NULL DEFAULT FALSE,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ")   \n",
    "\n",
    "cur.execute('''CREATE TABLE EMOJITRUST\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    EMOJI       TEXT       NOT NULL,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ") \n",
    "\n",
    "cur.execute('''CREATE TABLE EMOTICONSTRUST\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    EMOTICONS       TEXT       NOT NULL,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ") \n",
    "\n",
    "cur.execute('''CREATE TABLE HASHTAGTRUST\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    HASHTAG     TEXT       NOT NULL,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ") \n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inserimento hashtag, emoticon ed emoji\n",
    "\n",
    "for hashTag in hashtagTrust:\n",
    "    cur.execute(\"INSERT INTO HASHTAGTRUST (HASHTAG,FREQUENCY) VALUES(%s,%s)\",(hashTag,0))\n",
    "\n",
    "for emoticons in emoticonTrust:\n",
    "    cur.execute(\"INSERT INTO EMOTICONSTRUST (EMOTICONS,FREQUENCY) VALUES(%s,%s)\",(emoticons,0))\n",
    "\n",
    "for emojis in emojiTrust:\n",
    "    cur.execute(\"INSERT INTO EMOJITRUST (EMOJI,FREQUENCY) VALUES(%s,%s)\",(emojis,0))\n",
    "\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inserimento parole in db\n",
    "for word in trustListWord:\n",
    "    cur.execute(\"INSERT INTO PAROLETRUST (PAROLA,NRC_trust,FREQUENCY) VALUES(%s,%s,%s)\",(word,False,0))\n",
    "\n",
    "conn.commit()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze hashtag\n",
    "cur.execute(\"update HASHTAGTRUST as ha1 set frequency = (select count(*) from HASHTAGTRUST ha2 where ha1.hashtag = ha2.hashtag);\")\n",
    "cur.execute(\"delete from HASHTAGTRUST as ha1 where exists (Select 1 From HASHTAGTRUST as ha2 where ha1.hashtag = ha2.hashtag and ha1.frequency = ha2.frequency and ha2.ctid > ha1.ctid);\")\n",
    "\n",
    "conn.commit()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze emoji\n",
    "cur.execute(\"update EMOJITRUST as ha1 set frequency = (select count(*) from EMOJITRUST ha2 where ha1.emoji = ha2.emoji);\")\n",
    "cur.execute(\"delete from EMOJITRUST as ha1 where exists (Select 1 From EMOJITRUST as ha2 where ha1.emoji = ha2.emoji and ha1.frequency = ha2.frequency and ha2.ctid > ha1.ctid);\")\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze emoticons\n",
    "cur.execute(\"update EMOTICONSTRUST as ha1 set frequency = (select count(*) from EMOTICONSTRUST ha2 where ha1.emoticons = ha2.emoticons);\")\n",
    "cur.execute(\"delete from EMOTICONSTRUST as ha1 where exists (Select 1 From EMOTICONSTRUST as ha2 where ha1.emoticons = ha2.emoticons and ha1.frequency = ha2.frequency and ha2.ctid > ha1.ctid);\")\n",
    "\n",
    "conn.commit()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze parole\n",
    "cur.execute(\"select count(*),parola from PAROLETRUST group by parola\")\n",
    "result = []\n",
    "for word in cur.fetchall():\n",
    "    result.append(word) \n",
    "\n",
    "cur.execute(\"truncate table PAROLETRUST;\")\n",
    "cur.execute(\"ALTER SEQUENCE paroletrust_id_seq RESTART WITH 1;\")\n",
    "\n",
    "for couple in result:\n",
    "    cur.execute(\"INSERT INTO PAROLETRUST (PAROLA,NRC_trust,FREQUENCY) VALUES(%s,%s,%s)\",(couple[1],False,couple[0]))\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggiornamento presenza parola nelle risorse lessicali\n",
    "cur.execute(\"UPDATE PAROLETRUST SET NRC_trust = TRUE WHERE parola IN (select p.parola from PAROLETRUST as p LEFT OUTER JOIN RISORSETRUST as r ON p.parola = r.parola WHERE r.nome = 'NRC_trust')\")\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvataggio in un txt delle parole non presenti in nessuna risorsa lessicale\n",
    "\n",
    "wordsMongo = trustCollections.find({\"LexResources\": []}, {\"_id\": 0, \"freq\": 0, \"LexResources\": 0})\n",
    "words = []\n",
    "\n",
    "for word in wordsMongo:\n",
    "    words.append(word['word'])\n",
    "\n",
    "with open('Nuove Parole/trust.txt', 'w',encoding=\"utf8\") as f:\n",
    "    for word in words:\n",
    "        f.write(\"%s\\n\" % word)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "727cd1b8d9e0a8fe67ccb60934071f3fe824c12f3f18648f9575d2adb7fe198a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
