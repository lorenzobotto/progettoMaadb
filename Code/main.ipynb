{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import per il resto delle celle\n",
    "import re\n",
    "import emoji\n",
    "import regex\n",
    "import nltk\n",
    "import json\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from collections import Counter\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "import pymongo\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import psycopg2\n",
    "%matplotlib inline\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "emoticonsList = [':‑)',':)',':-]',':]',':->',':>','8-)','8)',':-}',':}',':o)',':c)',':^)','=]','=)',':‑D',':D','8‑D','8D','=D','=3','B^D','c:',\n",
    "'C:','x‑D','xD','X‑D','XD',':-))',':‑(',':(',':‑c',':c',':‑<',':<',':‑[',':[',':-||','>:[',':{',':@',':('',;(',\":'‑(\",\":'(\",':=(',\":'‑)\",\":')\",\n",
    "':\"D',\"D‑':\",'D:<','D:','D8','D;','D=','DX',':‑O',':O',':‑o',':o',':-0','8‑0','>:O','=O','=o','=0',':-3',':3','=3','x3','X3','>:3',':-*',\n",
    "':*',':×',';‑)',';)','*-)','*)',';‑]',';]',';^)',';>',':‑,',';D',';3',':‑P',':P','X‑P','XP','x‑p','xp',':‑p',':p',':‑Þ',':Þ',':‑þ',':þ',':‑b',':b',\n",
    "'d:','=p','>:P',':-/',':/',':‑.',\">:/\",'=/',':L','=L',':S',':‑|',':|',':$','://)','://3',':‑X',':X',':‑#',':#',':‑&',':&','O:‑)','O:)','0:‑3'\n",
    ",'0:3','0:‑)','0:)','0;^)','>:‑)','>:)','}:‑)','}:)','3:‑)','3:)','>;‑)','>;)','>:3',';3','|;‑)','|‑O','B-)',':‑J','#‑)','%‑)','%)',':‑###..'\n",
    ",':###..','<:‑|',\"',:-|\",\"',:-l\",':E','8-X','8=X','x-3','x=3','~:>','v.v','._.','._.;','QQ  ','qq','Qq','X_X','x_x','+_+','X_x','x_X'\n",
    "'<_<','>_>','<.<','>.>','O_O','o_o','O-O','o‑o','O_o','o_O','>.<','>_<','^5','o/\\o','>_>^ ^<_<','V.v.V','V=(° °)=V','(^^^)','(::[]::)',\n",
    "'(o)(o)','( • )( • )','[̲̅$̲̅(̲̅1̲̅)̲̅$̲̅]','[̲̅$̲̅(̲̅5̲̅)̲̅$̲̅]','[̲̅$̲̅(̲̅10)̲̅$̲̅]','[̲̅$̲̅(̲̅100)̲̅$̲̅]','( ͡° ͜ʖ ͡°)','ヽ༼ຈل͜ຈ༽ﾉ','(๑ˇεˇ๑)','(◕‿◕✿)','( ༎ຶ ۝ ༎ຶ )','(=ʘᆽʘ=)∫','ʕ •ᴥ•ʔ',\n",
    "'(>_<)','(>_<)>','(>w<)',\"(';')\",'(^_^;)','(-_-;)','(~_~;)','(・.・;)','(・_・;)','^^;','(・・;)','^_^;','(#^.^#)'\n",
    ",'(^^;)','(⁄ ⁄•⁄ω⁄•⁄ ⁄)','(-.-)y-°°°','(^.^)y-.o○','(-_-)zzz','(^_-)-☆','(^_-)','((+_+))','(+o+)','(°°)','(°-°) (°.°)',\n",
    "'(°_°)','(°_°>)','(°レ°)','(o|o)','<(｀^´)>','^_^','(°o°)','(^_^)/','(^O^)／','(^o^)／','(^^)/','(≧∇≦)/','(/◕ヮ◕)/','(^o^)丿','∩(·ω·)∩',\n",
    "'(·ω·)','^ω^','_(._.)_','_(_^_)_','<(_ _)>','<m(__)m>','m(__)m','m(_ _)m','(凸ಠ益ಠ)凸',\"('_')\",'(/_;)','(T_T)','(;_;)','(;_;','(;_:)','(;O;)'\n",
    ",'(:_;)','(ToT)','(Ｔ▽Ｔ)',';_;',';-;',';n;',';;','Q.Q','T.T','TnT','QQ','Q_Q',\n",
    "'(ー_ー)!!','(-.-)','(-_-)','(一一)','(；一_一)','(=_=)','(=^・^=)','(=^・・^=)','=^_^=','(..)','(._.)',\n",
    "'.o○','○o.','_旦~~','( ^^)','_U~~','( ^^)','☆ミ','☆彡','>°)))彡','(Q))',\n",
    "'><ヨヨ (°))<<','>°))))彡','<°)))彡','>°))彡','<+','))><<','<*))','>=<','<コ:彡',\n",
    "'Ｃ:.ミ','~>°)～～～','～°·_·°～','(°°)～','●～*','￣|○',':3ミ','^m^','(・・?','(?_?)','>^_^<','<^!^>',\n",
    "'^/^','（*^_^*','§^.^§','(^<^)','(^.^)','(^ム^)','(^·^)','(^.^)','(^_^.)','(^_^)','(^^)','(^J^)',\n",
    "'(*^.^*)','^_^','(#^.^#)','（^—^）','(^^)/~~~','(^_^)/~','(;_;)/~~~','(^.^)/~~~',\n",
    "'(-_-)/~~~','($··)/~~~','(@^^)/~~~','(T_T)/~~~','(V)o￥o(V)','＼(~o~)／','＼(^o^)／','＼(-o-)／','ヽ(^。^)ノ',\n",
    "'ヽ(^o^)丿','(*^0^*)','(*_*)','(*_*;','(+_+) (@_@)','(@_@。','(＠_＠;)',\n",
    " '＼(◎o◎)／！','(*^^)v','(^^)v','(^_^)v','（’-’*) (＾ｖ＾)','(＾▽＾)','(・∀・)','(´∀`)','(⌒▽⌒）',\n",
    " '＼(^o^)／','\\(^o^)/','(~o~)','(~_~)','(^^ゞ','ˊ＿>ˋ','(p_-)', '(-_q)','((d[-_-]b))','(-\"-)','(ーー゛)',\n",
    " '(^_^メ)', '(-_-メ)','(~_~メ)','(－－〆)','(・へ・)','(｀´)'\n",
    " , '<`～´>','<`ヘ´>','(ーー;)','(^0_0^)','(＾ｖ＾)', '(＾ｕ＾)','(＾◇＾)','( ^)o(^ )','(^O^)','(^o^)','(^○^)',')^o^(',\n",
    " '(*^▽^*)','(￣ー￣)','(￣□￣;)', '°o°','°O°',':O','o_O','o_0', 'o.O','(o.o)','oO','(°◇°)','（ ﾟ Дﾟ)','(*￣m￣)',\n",
    " 'ヽ(´ー｀)┌','¯\\_(ツ)_/¯','¯\\(°_o)/¯','(´･ω･`)', '(‘A`)','(づ￣ ³￣)づ','(*^3^)/~☆','(︶｡︶✽)','(-_-) zzz'\n",
    " , 'uwu','UwU','OWO','OwO','～°·_·°～', '(°°)～',':3ミ','(´･ω･`)','(`･ω･´)','(｀-´)>', '（　´_ゝ`）'] \n",
    "\n",
    "puntuaction = ['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '.',\n",
    "           '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', \n",
    "           '`', '{', '|', '}', '~', '»', '«', '“', '”', \"-\", \"...\",\"..\",\".,\",\",.\"]\n",
    "\n",
    "\n",
    "mongoClient = pymongo.MongoClient(\"mongodb://admin:admin@localhost:27017/\")\n",
    "mydb = mongoClient[\"maadb\"]\n",
    "\n",
    "collectionTwitter = mydb[\"Twitter\"] #Questa é per la collezione dei emoji emoticon hashtag per ogni emozione\n",
    "collectionAnger = mydb[\"CollectionAnger\"]\n",
    "collectionAnticipation = mydb[\"CollectionAnticipation\"]\n",
    "collectionDisgust = mydb[\"CollectionDisgust\"]\n",
    "collectionFear = mydb[\"CollectionFear\"]\n",
    "collectionHope = mydb[\"CollectionHope\"]\n",
    "collectionAJoy = mydb[\"CollectionJoy\"]\n",
    "collectionSadness = mydb[\"CollectionSadness\"]\n",
    "collectionSurprise = mydb[\"CollectionSurprise\"]\n",
    "collectionTrust = mydb[\"CollectionTrust\"]\n",
    "\n",
    "angerCollections = mydb[\"anger\"] #Collezioni per solo i twitt di anger\n",
    "anticipationCollections = mydb[\"anticipation\"]\n",
    "disgustCollections = mydb[\"disgust\"]\n",
    "fearCollections = mydb[\"fear\"]\n",
    "joyCollections = mydb[\"joy\"]\n",
    "sadnessCollections = mydb[\"sadness\"]\n",
    "surpriseCollections = mydb[\"surprise\"]\n",
    "trustCollections = mydb[\"trust\"]\n",
    "resourcesCollection = mydb[\"Resources\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trattamento dataset Anger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lista di righe, ogni riga una tweet anger\n",
    "file = open(\"Dataset/dataset_dt_anger_60k.txt\", 'r',encoding=\"utf8\")\n",
    "anger = []\n",
    "hashtagAnger = []\n",
    "emoticonAnger = []\n",
    "emojiAnger = []\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    #Eliminazione URL e USERNAME\n",
    "    riga = re.sub(r'URL', '',riga)\n",
    "    riga = re.sub(r'USERNAME', '',riga)\n",
    "\n",
    "    #array di hashtag singoli\n",
    "    hashTag = (re.findall('#[a-zA-Z0-9]+', riga))\n",
    "\n",
    "     #generiamo l'array di hashtag senza hashtag\n",
    "    for h in hashTag:\n",
    "        hashtagAnger.append(h.replace(\"#\",\"\"))\n",
    "\n",
    "    #Rimuoviamo glin hashtag dalla riga\n",
    "    riga = riga.replace('#',\"\")\n",
    "\n",
    "    #Salvataggio frase\n",
    "    anger.append(riga)\n",
    "\n",
    "\n",
    "file.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(anger)):\n",
    "    #Controllo se l'emoticons c'è e in caso la rimuovo\n",
    "    for singleEmoticon in emoticonsList:\n",
    "        emoticonsFind = re.findall(re.escape(singleEmoticon),anger[i])\n",
    "        if(len(emoticonsFind) != 0):\n",
    "            emoticonAnger.extend(emoticonsFind)\n",
    "            anger[i] = anger[i].replace(singleEmoticon,\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(anger)):\n",
    "    #preleviamo le emoji e le rimuoviamo dal testo\n",
    "    data = regex.findall(r'\\X', anger[i])\n",
    "    for word in data:\n",
    "        if any(char in emoji.UNICODE_EMOJI['en'] for char in word):\n",
    "            emojiAnger.append(word)\n",
    "            anger[i] = anger[i].replace(word, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processamento slang \n",
    "with open(\"Slangs/slangs_before.json\") as f:\n",
    "    slangsBefore = json.load(f)\n",
    "    for i in range(len(anger)):\n",
    "        for word in anger[i].split():\n",
    "            indexOfWord = anger[i].find(word)\n",
    "            if slangsBefore.get(word) is None:\n",
    "                if slangsBefore.get(word.lower()) is None:\n",
    "                    if slangsBefore.get(word.upper()) is None:\n",
    "                        continue\n",
    "                    else:\n",
    "                        anger[i] = anger[i][:indexOfWord] + slangsBefore.get(word.upper()) + anger[i][indexOfWord+len(word):]\n",
    "                else:\n",
    "                    anger[i] = anger[i][:indexOfWord] + slangsBefore.get(word.lower()) + anger[i][indexOfWord+len(word):]\n",
    "            else:\n",
    "                anger[i] = anger[i][:indexOfWord] + slangsBefore.get(word) + anger[i][indexOfWord+len(word):]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizzazione di tutte le frasi\n",
    "angerTokenized = []\n",
    "for linea in anger:\n",
    "    result = word_tokenize(linea)\n",
    "    angerTokenized.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Esecuzione della POS tagging per ogni riga della matrice\n",
    "for i in range(len(angerTokenized)):\n",
    "    angerTokenized[i] = nltk.pos_tag(angerTokenized[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trattamento punteggiatura\n",
    "for line in angerTokenized:\n",
    "    newLine = line\n",
    "    listToRemove = []\n",
    "    for i in range(len(line)):\n",
    "        if line[i][0] in puntuaction:\n",
    "            listToRemove.append(line[i])\n",
    "    for element in listToRemove:\n",
    "        newLine.remove(element)\n",
    "    line = newLine\n",
    "\n",
    "linePuntuaction = re.compile(\"[\" + re.escape(\"\".join(puntuaction)) + \"]\")\n",
    "for line in angerTokenized:\n",
    "    for i in range(len(line)):\n",
    "        tupla = [(re.sub(linePuntuaction, \"\", line[i][0])),line[i][1]]\n",
    "        line[i] = tupla\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming\n",
    "#ps = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for line in angerTokenized:\n",
    "    for i in range(len(line)):\n",
    "        tupla = [lemmatizer.lemmatize(line[i][0]),line[i][1]]\n",
    "        line[i] = tupla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trattamnento Stop word dopo Stemming\n",
    "stop_words = set(stopwords.words('english'))\n",
    "angerTokStoped = []\n",
    "for line in angerTokenized:\n",
    "    filtred_word = []\n",
    "    for row in line:\n",
    "        if  row[0] not in stop_words:\n",
    "            filtred_word.append(row) \n",
    "    angerTokStoped.append(filtred_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#liste singole di parole e token\n",
    "angerListWord = []\n",
    "angerListTok = []\n",
    "for line in angerTokStoped:\n",
    "    for row in line:\n",
    "        if row[0] != '':\n",
    "            angerListWord.append(row[0])\n",
    "        angerListTok.append(row[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lavorazione risorse lessicali\n",
    "resourceAngerEmoSN = []\n",
    "resourceAngerNRC = []\n",
    "resourceAngerSentisense = []\n",
    "\n",
    "#Lista di righe, ogni riga una tweet anger\n",
    "file = open(\"Risorse lessicali\\Anger\\EmoSN_anger.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        resourceAngerEmoSN.append(riga)\n",
    "\n",
    "file.close()\n",
    "\n",
    "file = open(\"Risorse lessicali\\\\Anger\\\\NRC_anger.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        resourceAngerNRC.append(riga)\n",
    "\n",
    "file.close()\n",
    "\n",
    "file = open(\"Risorse lessicali\\Anger\\sentisense_anger.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        resourceAngerSentisense.append(riga)\n",
    "\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Caricare le risorse lessicali \n",
    "\n",
    "AngerEmoSN = {\n",
    "    '_id': 'EmoSN_anger',\n",
    "    'sentiment': 'anger', \n",
    "    'wordList': resourceAngerEmoSN\n",
    "}\n",
    "\n",
    "AngerNRC = {\n",
    "    '_id': 'NRC_anger',\n",
    "    'sentiment': 'anger',  \n",
    "    'wordList': resourceAngerNRC\n",
    "}\n",
    "\n",
    "AngerSentisense = {\n",
    "    '_id': 'sentisense_anger',\n",
    "    'sentiment': 'anger',\n",
    "    'wordList': resourceAngerSentisense\n",
    "}\n",
    "\n",
    "resourcesCollection.insert_one(AngerEmoSN)\n",
    "resourcesCollection.insert_one(AngerNRC)\n",
    "resourcesCollection.insert_one(AngerSentisense)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione database e collezioni\n",
    "# Inserimento singoli documenti contententi i tweet anger\n",
    "wordsString = {}\n",
    "\n",
    "for i in range(len(angerListWord)):\n",
    "    wordsString = {\n",
    "        \"lemma\": angerListWord[i],\n",
    "        \"POS\": angerListTok[i]\n",
    "    }\n",
    "    angerTwit={\"word\":wordsString}\n",
    "    angerCollections.insert_one(angerTwit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inserimento collezione Twitter con hashtags, emoticons ed emoji \n",
    "angerInsert = {\n",
    "    \"sentiment\":\"anger\",\n",
    "    \"hashtags\": hashtagAnger,\n",
    "    \"emoticons\": emoticonAnger,\n",
    "    \"emoji\": emojiAnger\n",
    "}\n",
    "\n",
    "#inserimento\n",
    "x = collectionAnger.insert_one(angerInsert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raggruppa i documenti di input in base all'espressione _id specificata e per ciascun raggruppamento distinto genera un documento\n",
    "pipeline = [\n",
    "    {\"$group\": {\"_id\": \"$word.lemma\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"word\": \"$_id\",\n",
    "      \"freq\": \"$count\"\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"freq\": -1\n",
    "    }},\n",
    "    {\"$out\": \"anger\"}\n",
    "]\n",
    "\n",
    "#post processing per renderlo valido \n",
    "angerCollections.aggregate(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creazione Word Clouds per le parole\n",
    "data = {}\n",
    "for document in angerCollections.find().limit(50):\n",
    "    data[document['word']] = document['freq']\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2)\n",
    "wc.fit_words(data)\n",
    "wc.to_file('Word Clouds/Anger/angerWords.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conteggio degli hashtags\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$hashtags\"},\n",
    "    {\"$group\": {\"_id\": \"$hashtags\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"word\": \"$_id\",\n",
    "      \"count\": 1\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"count\": -1\n",
    "    }}\n",
    "]\n",
    "\n",
    "hashtagsCount = collectionAnger.aggregate(pipeline)\n",
    "\n",
    "data = {}\n",
    "for document in hashtagsCount:\n",
    "    data[document['word']] = document['count']\n",
    "\n",
    "collectionAnger.update_one(\n",
    "  {\"sentiment\": \"anger\"},\n",
    "  {\"$set\": {\"hashtags\": data}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creazione Word Clouds per gli hashtags\n",
    "\n",
    "hashtags = collectionAnger.distinct(\"hashtags\")\n",
    "\n",
    "hashtagsLimited = {}\n",
    "for (word,count) in [x for x in hashtags[0].items()][:50]:\n",
    "    hashtagsLimited[word] = count\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2)\n",
    "wc.fit_words(hashtagsLimited)\n",
    "wc.to_file('Word Clouds/Anger/angerHashtags.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conteggio delle emoji\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$emoji\"},\n",
    "    {\"$group\": {\"_id\": \"$emoji\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"emoji\": \"$_id\",\n",
    "      \"count\": 1\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"count\": -1\n",
    "    }}\n",
    "]\n",
    "\n",
    "emojiCount = collectionAnger.aggregate(pipeline)\n",
    "\n",
    "data = {}\n",
    "for document in emojiCount:\n",
    "    data[document['emoji']] = document['count']\n",
    "\n",
    "collectionAnger.update_one(\n",
    "  {\"sentiment\": \"anger\"},\n",
    "  {\"$set\": {\"emoji\": data}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creazione Word Clouds per le emoji\n",
    "\n",
    "emojis = collectionAnger.distinct(\"emoji\")\n",
    "\n",
    "emojisLimited = {}\n",
    "for (word,count) in [x for x in emojis[0].items()][:50]:\n",
    "    emojisLimited[word] = count\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2, font_path='Font Emoji\\Symbola.otf')\n",
    "wc.fit_words(emojisLimited)\n",
    "wc.to_file('Word Clouds/Anger/angerEmojis.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conteggio degli emoticons\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$emoticons\"},\n",
    "    {\"$group\": {\"_id\": \"$emoticons\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"emoticons\": \"$_id\",\n",
    "      \"count\": 1\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"count\": -1\n",
    "    }}\n",
    "]\n",
    "\n",
    "emoticonsCount = collectionAnger.aggregate(pipeline)\n",
    "\n",
    "data = {}\n",
    "for document in emoticonsCount:\n",
    "    data[document['emoticons']] = document['count']\n",
    "\n",
    "collectionAnger.update_one(\n",
    "  {\"sentiment\": \"anger\"},\n",
    "  {\"$set\": {\"emoticons\": data}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creazione Word Clouds per le emoticons\n",
    "\n",
    "emoticons = collectionAnger.distinct(\"emoticons\")\n",
    "\n",
    "emoticonsLimited = {}\n",
    "for (word,count) in [x for x in emoticons[0].items()][:50]:\n",
    "    emoticonsLimited[word] = count\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2)\n",
    "wc.fit_words(emoticonsLimited)\n",
    "wc.to_file('Word Clouds/Anger/angerEmoticons.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline di aggregazione per inserire la risorsa lessicale a cui appartiene la parola\n",
    "\n",
    "pipeline = [\n",
    "    {\"$lookup\": {\n",
    "        \"from\": \"Resources\",\n",
    "        \"localField\": \"word\",\n",
    "        \"foreignField\": \"wordList\",\n",
    "        \"pipeline\": [\n",
    "            {\"$match\": {\"$expr\": {\"$regexMatch\": {\"input\": \"$_id\", \"regex\": \"anger\"}}}}\n",
    "        ],\n",
    "        \"as\": \"LexResources\"\n",
    "    }},\n",
    "    {\"$project\": {\n",
    "        \"resources.wordList\": 0,\n",
    "        \"resources.totNumberWords\": 0,\n",
    "        \"resources.sentiment\": 0,\n",
    "        \"LexResources.sentiment\": 0,\n",
    "        \"LexResources.totNumberWords\": 0,\n",
    "        \"LexResources.wordList\": 0\n",
    "    }},\n",
    "    {\"$out\": \"anger\"}\n",
    "]\n",
    "\n",
    "angerCollections.aggregate(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline di aggregazione per calcolare le percentuali richieste\n",
    "\n",
    "pipelineCount = [\n",
    "    {\"$group\": {\"_id\": \"null\", \"count\": { \"$sum\": \"$freq\" }}},\n",
    "    {\"$project\": {\n",
    "        \"_id\": 0,\n",
    "        \"count\": \"$count\"\n",
    "    }}\n",
    "]\n",
    "\n",
    "results = angerCollections.aggregate(pipelineCount)\n",
    "\n",
    "countWords = 0\n",
    "for data in results:\n",
    "    countWords = data['count']\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$LexResources\"},\n",
    "    {\"$lookup\": {\n",
    "        \"from\": \"Resources\",\n",
    "        \"localField\": \"LexResources._id\",\n",
    "        \"foreignField\": \"_id\",\n",
    "        \"as\": \"RefResource\"\n",
    "    }},\n",
    "    {\"$group\": {\"_id\": { \"lex\": \"$LexResources._id\", \"RefResource\": \"$RefResource\" }, \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "        \"_id\": \"$_id.lex\",\n",
    "        \"wordList\": \"$_id.RefResource.wordList\",\n",
    "        \"commonWords\": \"$count\"\n",
    "    }},\n",
    "    {\"$unwind\": \"$wordList\"},\n",
    "    {\"$project\": {\n",
    "        \"_id\": 1,\n",
    "        \"commonWords\": 1,\n",
    "        \"totWordsLex\": {\"$size\": \"$wordList\"}\n",
    "    }},\n",
    "    {\"$addFields\": {\"totalWordsTwitter\": countWords}},\n",
    "    {\"$project\": {\n",
    "        \"perc_presence_lex_res\": { \"$round\": [{\"$divide\": [ \"$commonWords\", \"$totWordsLex\" ] }, 6] },\n",
    "        \"perc_presence_twitter\": { \"$round\": [{\"$divide\": [ \"$commonWords\", \"$totalWordsTwitter\" ]}, 6] }\n",
    "    }}\n",
    "]\n",
    "\n",
    "results = angerCollections.aggregate(pipeline)\n",
    "\n",
    "# Genero i dati per l'istogramma\n",
    "istogramAnger_lexRes = []\n",
    "istogramAnger_twitter = []\n",
    "\n",
    "for data in results:\n",
    "    print(data)\n",
    "    log =math.log(data['perc_presence_lex_res'],10)\n",
    "    istogramAnger_lexRes.append([data['_id'], data['perc_presence_lex_res']])\n",
    "    log =math.log(data['perc_presence_twitter'],10)\n",
    "    istogramAnger_twitter.append([data['_id'], data['perc_presence_twitter']])\n",
    " \n",
    "# Create the pandas DataFrame\n",
    "dfAnger_lexRes = pd.DataFrame(istogramAnger_lexRes, columns = ['Risorsa', 'Percentuale'])\n",
    "dfAnger_twitter = pd.DataFrame(istogramAnger_twitter,columns = ['Risorsa', 'Percentuale'])      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione istogramma per presenza parole nelle risorse lessicali\n",
    "\n",
    "dfAnger_lexRes.plot.bar(x='Risorsa')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione istogramma per presenza parole nei twitter\n",
    "\n",
    "dfAnger_twitter.plot.bar(x='Risorsa')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connessione, creazione cursore e database\n",
    "\n",
    "conn = psycopg2.connect(database = \"maadb\", user = \"postgres\", password = \"admin\", host = \"localhost\", port = \"5432\")\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione tabella risorse lessicali anger e inserimento\n",
    "cur.execute('''CREATE TABLE RISORSEANGER\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    NOME             TEXT       NOT NULL,\n",
    "                    PAROLA           TEXT       NOT NULL\n",
    "                );\n",
    "            '''\n",
    ")\n",
    "nome = \"EmoSN_anger\"\n",
    "for word in resourceAngerEmoSN:\n",
    "    cur.execute(\"INSERT INTO RISORSEANGER (NOME, PAROLA) VALUES(%s, %s)\",(nome,word))\n",
    "\n",
    "nome = \"NRC_anger\"\n",
    "for word in resourceAngerNRC:\n",
    "    cur.execute(\"INSERT INTO RISORSEANGER (NOME, PAROLA) VALUES(%s, %s)\",(nome,word))\n",
    "\n",
    "nome = \"sentisense_anger\"\n",
    "for word in resourceAngerSentisense:\n",
    "    cur.execute(\"INSERT INTO RISORSEANGER (NOME, PAROLA) VALUES(%s, %s)\",(nome,word))\n",
    "\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione tabelle per le parole, emoji, emoticon e hashtag\n",
    "\n",
    "cur.execute('''CREATE TABLE PAROLEANGER\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    PAROLA           TEXT       NOT NULL, \n",
    "                    EmoSN_anger      BOOL       NOT NULL DEFAULT FALSE,\n",
    "                    NRC_anger      BOOL       NOT NULL DEFAULT FALSE,\n",
    "                    sentisense_anger      BOOL       NOT NULL DEFAULT FALSE,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ")   \n",
    "\n",
    "cur.execute('''CREATE TABLE EMOJIANGER\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    EMOJI       TEXT       NOT NULL,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ") \n",
    "\n",
    "cur.execute('''CREATE TABLE EMOTICONSANGER\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    EMOTICONS       TEXT       NOT NULL,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ") \n",
    "\n",
    "cur.execute('''CREATE TABLE HASHTAGANGER\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    HASHTAG     TEXT       NOT NULL,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ") \n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inserimento hashtag, emoticon ed emoji\n",
    "\n",
    "for hashTag in hashtagAnger:\n",
    "    cur.execute(\"INSERT INTO HASHTAGANGER (HASHTAG,FREQUENCY) VALUES(%s,%s)\",(hashTag,0))\n",
    "\n",
    "for emoticons in emoticonAnger:\n",
    "    cur.execute(\"INSERT INTO EMOTICONSANGER (EMOTICONS,FREQUENCY) VALUES(%s,%s)\",(emoticons,0))\n",
    "\n",
    "for emojis in emojiAnger:\n",
    "    cur.execute(\"INSERT INTO EMOJIANGER (EMOJI,FREQUENCY) VALUES(%s,%s)\",(emojis,0))\n",
    "\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inserimento parole in db\n",
    "for word in angerListWord:\n",
    "    cur.execute(\"INSERT INTO PAROLEANGER (PAROLA,EmoSN_anger,NRC_anger,sentisense_anger,FREQUENCY) VALUES(%s,%s,%s,%s,%s)\",(word,False,False,False,0))\n",
    "\n",
    "conn.commit()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze hashtag\n",
    "cur.execute(\"update hashtaganger as ha1 set frequency = (select count(*) from hashtaganger ha2 where ha1.hashtag = ha2.hashtag);\")\n",
    "cur.execute(\"delete from hashtaganger as ha1 where exists (Select 1 From hashtaganger as ha2 where ha1.hashtag = ha2.hashtag and ha1.frequency = ha2.frequency and ha2.ctid > ha1.ctid);\")\n",
    "\n",
    "conn.commit()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze emoji\n",
    "cur.execute(\"update EMOJIANGER as ha1 set frequency = (select count(*) from EMOJIANGER ha2 where ha1.emoji = ha2.emoji);\")\n",
    "cur.execute(\"delete from EMOJIANGER as ha1 where exists (Select 1 From EMOJIANGER as ha2 where ha1.emoji = ha2.emoji and ha1.frequency = ha2.frequency and ha2.ctid > ha1.ctid);\")\n",
    "\n",
    "conn.commit()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze emoticons\n",
    "cur.execute(\"update EMOTICONSANGER as ha1 set frequency = (select count(*) from EMOTICONSANGER ha2 where ha1.emoticons = ha2.emoticons);\")\n",
    "cur.execute(\"delete from EMOTICONSANGER as ha1 where exists (Select 1 From EMOTICONSANGER as ha2 where ha1.emoticons = ha2.emoticons and ha1.frequency = ha2.frequency and ha2.ctid > ha1.ctid);\")\n",
    "\n",
    "conn.commit()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze parole (ci mette più di 100 minuti)\n",
    "\"\"\"\n",
    "cur.execute(\"update PAROLEANGER as ha1 set frequency = (select count(*) from PAROLEANGER ha2 where ha1.parola = ha2.parola);\")\n",
    "cur.execute(\"delete from PAROLEANGER as ha1 where exists (Select 1 From PAROLEANGER as ha2 where ha1.parola = ha2.parola and ha1.frequency = ha2.frequency and ha2.ctid > ha1.ctid);\")\n",
    "\n",
    "conn.commit()  \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze parole\n",
    "cur.execute(\"select count(*),parola from paroleanger group by parola\")\n",
    "result = []\n",
    "for word in cur.fetchall():\n",
    "    result.append(word) \n",
    "\n",
    "cur.execute(\"truncate table paroleanger;\")\n",
    "cur.execute(\"ALTER SEQUENCE paroleanger_id_seq RESTART WITH 1;\")\n",
    "\n",
    "for couple in result:\n",
    "    cur.execute(\"INSERT INTO PAROLEANGER (PAROLA,EmoSN_anger,NRC_anger,sentisense_anger,FREQUENCY) VALUES(%s,%s,%s,%s,%s)\",(couple[1],False,False,False,couple[0]))\n",
    "\n",
    "conn.commit()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggiornamento presenza parola nelle risorse lessicali\n",
    "cur.execute(\"UPDATE paroleanger SET EmoSN_anger = TRUE WHERE parola IN (select p.parola from paroleanger as p LEFT OUTER JOIN risorseanger as r ON p.parola = r.parola WHERE r.nome = 'EmoSN_anger')\")\n",
    "cur.execute(\"UPDATE paroleanger SET NRC_ANGER = TRUE WHERE parola IN (select p.parola from paroleanger as p LEFT OUTER JOIN risorseanger as r ON p.parola = r.parola WHERE r.nome = 'NRC_anger')\")\n",
    "cur.execute(\"UPDATE paroleanger SET sentisense_anger = TRUE WHERE parola IN (select p.parola from paroleanger as p LEFT OUTER JOIN risorseanger as r ON p.parola = r.parola WHERE r.nome = 'sentisense_anger')\")\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvataggio in un txt delle parole non presenti in nessuna risorsa lessicale\n",
    "\n",
    "wordsMongo = angerCollections.find({\"LexResources\": []}, {\"_id\": 0, \"freq\": 0, \"LexResources\": 0})\n",
    "words = []\n",
    "\n",
    "for word in wordsMongo:\n",
    "    words.append(word['word'])\n",
    "\n",
    "with open('Nuove Parole/anger.txt', 'w',encoding=\"utf8\") as f:\n",
    "    for word in words:\n",
    "        f.write(\"%s\\n\" % word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trattamento dataset Anticipation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lista di righe, ogni riga una tweet anticipation\n",
    "file = open(\"Dataset/dataset_dt_anticipation_60k.txt\", 'r',encoding=\"utf8\")\n",
    "anticipation = []\n",
    "hashtagAnticipation = []\n",
    "emoticonAnticipation = []\n",
    "emojiAnticipation = []\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "    riga = re.sub(r'URL', '',riga)\n",
    "    riga = re.sub(r'USERNAME', '',riga)\n",
    "\n",
    "    #array di hashtag singoli\n",
    "    hashTag = (re.findall('#[a-zA-Z0-9]+', riga))\n",
    "\n",
    "    #generiamo l'array di hashtag senza hashtag\n",
    "    for h in hashTag:\n",
    "        hashtagAnticipation.append(h.replace(\"#\",\"\"))\n",
    "\n",
    "    #Rimuoviamo glin hashtag dalla riga\n",
    "    riga = riga.replace('#',\"\")\n",
    "    \n",
    "    anticipation.append(riga)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(anticipation)):\n",
    "    #Controllo se l'emoticons c'è e in caso la rimuovo\n",
    "    for singleEmoticon in emoticonsList:\n",
    "        emoticonsFind = re.findall(re.escape(singleEmoticon),anticipation[i])\n",
    "        if(len(emoticonsFind) != 0):\n",
    "            emoticonAnticipation.extend(emoticonsFind)\n",
    "            anticipation[i] = anticipation[i].replace(singleEmoticon,\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(anticipation)):\n",
    "    #preleviamo le emoji e le rimuoviamo dal testo\n",
    "    data = regex.findall(r'\\X', anticipation[i])\n",
    "    for word in data:\n",
    "        if any(char in emoji.UNICODE_EMOJI['en'] for char in word):\n",
    "            emojiAnticipation.append(word)\n",
    "            anticipation[i] = anticipation[i].replace(word, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processamento slang prima di punteggiatura e lowercase\n",
    "with open(\"Slangs/slangs_before.json\") as f:\n",
    "    slangsBefore = json.load(f)\n",
    "    for i in range(len(anticipation)):\n",
    "        for word in anticipation[i].split():\n",
    "            indexOfWord = anticipation[i].find(word)\n",
    "            if slangsBefore.get(word) is None:\n",
    "                if slangsBefore.get(word.lower()) is None:\n",
    "                    if slangsBefore.get(word.upper()) is None:\n",
    "                        continue\n",
    "                    else:\n",
    "                        anticipation[i] = anticipation[i][:indexOfWord] + slangsBefore.get(word.upper()) + anticipation[i][indexOfWord+len(word):]\n",
    "                else:\n",
    "                    anticipation[i] = anticipation[i][:indexOfWord] + slangsBefore.get(word.lower()) + anticipation[i][indexOfWord+len(word):]\n",
    "            else:\n",
    "                anticipation[i] = anticipation[i][:indexOfWord] + slangsBefore.get(word) + anticipation[i][indexOfWord+len(word):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizzazione di tutte le frasi\n",
    "anticipationTokenized = []\n",
    "for linea in anticipation:\n",
    "    result = word_tokenize(linea)\n",
    "    anticipationTokenized.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Esecuzione della POS tagging per ogni riga della matrice\n",
    "for i in range(len(anticipationTokenized)):\n",
    "    anticipationTokenized[i] = nltk.pos_tag(anticipationTokenized[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trattamento punteggiatura\n",
    "for line in anticipationTokenized:\n",
    "    newLine = line\n",
    "    listToRemove = []\n",
    "    for i in range(len(line)):\n",
    "        if line[i][0] in puntuaction:\n",
    "            listToRemove.append(line[i])\n",
    "    for element in listToRemove:\n",
    "        newLine.remove(element)\n",
    "    line = newLine\n",
    "\n",
    "linePuntuaction = re.compile(\"[\" + re.escape(\"\".join(puntuaction)) + \"]\")\n",
    "for line in anticipationTokenized:\n",
    "    for i in range(len(line)):\n",
    "        tupla = [(re.sub(linePuntuaction, \"\", line[i][0])),line[i][1]]\n",
    "        line[i] = tupla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming\n",
    "#ps = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for line in anticipationTokenized:\n",
    "    for i in range(len(line)):\n",
    "        tupla = [lemmatizer.lemmatize(line[i][0]),line[i][1]]\n",
    "        line[i] = tupla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trattamnento Stop word dopo Stemming\n",
    "stop_words = set(stopwords.words('english'))\n",
    "anticipationTokStoped = []\n",
    "for line in anticipationTokenized:\n",
    "    filtred_word = []\n",
    "    for row in line:\n",
    "        if  row[0] not in stop_words:\n",
    "            filtred_word.append(row) \n",
    "    anticipationTokStoped.append(filtred_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anticipationListWord = []\n",
    "anticipationListTok = []\n",
    "for line in anticipationTokStoped:\n",
    "    for row in line:\n",
    "        if row[0] != '':\n",
    "            anticipationListWord.append(row[0])\n",
    "        anticipationListTok.append(row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lavorazione risorse lessicali\n",
    "resourceNRC_anticipation = []\n",
    "resourceSentisense_anticipation = []\n",
    "#Lista di righe, ogni riga una tweet anger\n",
    "file = open(\"Risorse lessicali\\\\Anticipation\\\\NRC_anticipation.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        resourceNRC_anticipation.append(riga)\n",
    "\n",
    "file.close()\n",
    "\n",
    "file = open(\"Risorse lessicali\\Anticipation\\sentisense_anticipation.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        resourceSentisense_anticipation.append(riga)\n",
    "\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Caricare le risorse lessicali \n",
    "\n",
    "\n",
    "AnticipationNRC = {\n",
    "    '_id': 'NRC_anticipation',\n",
    "    'sentiment': 'anticipation',  \n",
    "    'wordList': resourceNRC_anticipation\n",
    "}\n",
    "\n",
    "AnticipationSentisense = {\n",
    "    '_id': 'sentisense_anticipation',\n",
    "    'sentiment': 'anticipation',\n",
    "    'wordList': resourceSentisense_anticipation\n",
    "}\n",
    "\n",
    "resourcesCollection.insert_one(AnticipationNRC)\n",
    "resourcesCollection.insert_one(AnticipationSentisense)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione database e collezioni\n",
    "# Inserimento singoli documenti contententi i tweet anger\n",
    "wordsString = {}\n",
    "\n",
    "for i in range(len(anticipationListWord)):\n",
    "    wordsString = {\n",
    "        \"lemma\": anticipationListWord[i],\n",
    "        \"POS\": anticipationListTok[i]\n",
    "    }\n",
    "    anticipationTwit={\"word\":wordsString}\n",
    "    anticipationCollections.insert_one(anticipationTwit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inserimento Documento\n",
    "anticipationInsert = {\n",
    "    \"sentiment\":\"anticipation\",\n",
    "    \"hashtags\": hashtagAnticipation,\n",
    "    \"emoticons\": emoticonAnticipation,\n",
    "    \"emoji\": emojiAnticipation\n",
    "}\n",
    "\n",
    "#inserimento\n",
    "x = collectionAnticipation.insert_one(anticipationInsert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raggruppa i documenti di input in base all'espressione _id specificata e per ciascun raggruppamento distinto genera un documento\n",
    "\n",
    "pipeline = [\n",
    "    {\"$group\": {\"_id\": \"$word.lemma\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"word\": \"$_id\",\n",
    "      \"freq\": \"$count\"\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"freq\": -1\n",
    "    }},\n",
    "    {\"$out\": \"anticipation\"}\n",
    "]\n",
    "\n",
    "\n",
    "anticipationCollections.aggregate(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creazione Word Clouds per le parole\n",
    "data = {}\n",
    "for document in anticipationCollections.find().limit(50):\n",
    "    data[document['word']] = document['freq']\n",
    "\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2)\n",
    "wc.fit_words(data)\n",
    "wc.to_file('Word Clouds/Anticipation/anticipationWords.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conteggio degli hashtags\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$hashtags\"},\n",
    "    {\"$group\": {\"_id\": \"$hashtags\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"word\": \"$_id\",\n",
    "      \"count\": 1\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"count\": -1\n",
    "    }}\n",
    "]\n",
    "\n",
    "hashtagsCount = collectionAnticipation.aggregate(pipeline)\n",
    "\n",
    "data = {}\n",
    "for document in hashtagsCount:\n",
    "    data[document['word']] = document['count']\n",
    "\n",
    "collectionAnticipation.update_one(\n",
    "  {\"sentiment\": \"anticipation\"},\n",
    "  {\"$set\": {\"hashtags\": data}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creazione Word Clouds per gli hashtag\n",
    "hashtags = collectionAnticipation.distinct(\"hashtags\")\n",
    "\n",
    "hashtagsLimited = {}\n",
    "for (word,count) in [x for x in hashtags[0].items()][:50]:\n",
    "    hashtagsLimited[word] = count\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2)\n",
    "wc.fit_words(hashtagsLimited)\n",
    "wc.to_file('Word Clouds/Anticipation/anticipationHashtags.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conteggio delle emoji\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$emoji\"},\n",
    "    {\"$group\": {\"_id\": \"$emoji\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"emoji\": \"$_id\",\n",
    "      \"count\": 1\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"count\": -1\n",
    "    }}\n",
    "]\n",
    "\n",
    "emojiCount = collectionAnticipation.aggregate(pipeline)\n",
    "\n",
    "data = {}\n",
    "for document in emojiCount:\n",
    "    data[document['emoji']] = document['count']\n",
    "\n",
    "collectionAnticipation.update_one(\n",
    "  {\"sentiment\": \"anticipation\"},\n",
    "  {\"$set\": {\"emoji\": data}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creazione Word Clouds per le emoji\n",
    "emojis = collectionAnticipation.distinct(\"emoji\")\n",
    "\n",
    "emojisLimited = {}\n",
    "for (word,count) in [x for x in emojis[0].items()][:50]:\n",
    "    emojisLimited[word] = count\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2)\n",
    "wc.fit_words(emojisLimited)\n",
    "wc.to_file('Word Clouds/Anticipation/anticipationEmojis.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conteggio degli emoticons\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$emoticons\"},\n",
    "    {\"$group\": {\"_id\": \"$emoticons\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"emoticons\": \"$_id\",\n",
    "      \"count\": 1\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"count\": -1\n",
    "    }}\n",
    "]\n",
    "\n",
    "emoticonsCount = collectionAnticipation.aggregate(pipeline)\n",
    "\n",
    "data = {}\n",
    "for document in emoticonsCount:\n",
    "    data[document['emoticons']] = document['count']\n",
    "\n",
    "collectionAnticipation.update_one(\n",
    "  {\"sentiment\": \"anticipation\"},\n",
    "  {\"$set\": {\"emoticons\": data}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creazione Word Clouds per le emoticons\n",
    "emoticons = collectionAnticipation.distinct(\"emoticons\")\n",
    "\n",
    "emoticonsLimited = {}\n",
    "for (word,count) in [x for x in emoticons[0].items()][:50]:\n",
    "    emoticonsLimited[word] = count\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2)\n",
    "wc.fit_words(emoticonsLimited)\n",
    "wc.to_file('Word Clouds/Anticipation/anticipationEmoticons.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline di aggregazione per inserire la risorsa lessicale a cui appartiene la parola\n",
    "pipeline = [\n",
    "    {\"$lookup\": {\n",
    "        \"from\": \"Resources\",\n",
    "        \"localField\": \"word\",\n",
    "        \"foreignField\": \"wordList\",\n",
    "        \"pipeline\": [\n",
    "            {\"$match\": {\"$expr\": {\"$regexMatch\": {\"input\": \"$_id\", \"regex\": \"anticipation\"}}}}\n",
    "        ],\n",
    "        \"as\": \"LexResources\"\n",
    "    }},\n",
    "    {\"$project\": {\n",
    "        \"resources.wordList\": 0,\n",
    "        \"resources.totNumberWords\": 0,\n",
    "        \"resources.sentiment\": 0,\n",
    "        \"LexResources.sentiment\": 0,\n",
    "        \"LexResources.totNumberWords\": 0,\n",
    "        \"LexResources.wordList\": 0\n",
    "    }},\n",
    "    {\"$out\": \"anticipation\"}\n",
    "]\n",
    "\n",
    "anticipationCollections.aggregate(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline di aggregazione per calcolare le percentuali richieste\n",
    "\n",
    "pipelineCount = [\n",
    "    {\"$group\": {\"_id\": \"null\", \"count\": { \"$sum\": \"$freq\" }}},\n",
    "    {\"$project\": {\n",
    "        \"_id\": 0,\n",
    "        \"count\": \"$count\"\n",
    "    }}\n",
    "]\n",
    "\n",
    "results = anticipationCollections.aggregate(pipelineCount)\n",
    "\n",
    "countWords = 0\n",
    "for data in results:\n",
    "    countWords = data['count']\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$LexResources\"},\n",
    "    {\"$lookup\": {\n",
    "        \"from\": \"Resources\",\n",
    "        \"localField\": \"LexResources._id\",\n",
    "        \"foreignField\": \"_id\",\n",
    "        \"as\": \"RefResource\"\n",
    "    }},\n",
    "    {\"$group\": {\"_id\": { \"lex\": \"$LexResources._id\", \"RefResource\": \"$RefResource\" }, \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "        \"_id\": \"$_id.lex\",\n",
    "        \"wordList\": \"$_id.RefResource.wordList\",\n",
    "        \"commonWords\": \"$count\"\n",
    "    }},\n",
    "    {\"$unwind\": \"$wordList\"},\n",
    "    {\"$project\": {\n",
    "        \"_id\": 1,\n",
    "        \"commonWords\": 1,\n",
    "        \"totWordsLex\": {\"$size\": \"$wordList\"}\n",
    "    }},\n",
    "    {\"$addFields\": {\"totalWordsTwitter\": countWords}},\n",
    "    {\"$project\": {\n",
    "        \"perc_presence_lex_res\": { \"$round\": [{\"$divide\": [ \"$commonWords\", \"$totWordsLex\" ] }, 6] },\n",
    "        \"perc_presence_twitter\": { \"$round\": [{\"$divide\": [ \"$commonWords\", \"$totalWordsTwitter\" ]}, 6] }\n",
    "    }}\n",
    "]\n",
    "\n",
    "results = anticipationCollections.aggregate(pipeline)\n",
    "\n",
    "# Genero i dati per l'istogramma\n",
    "istogramAnticipation_lexRes = []\n",
    "istogramAnticipation_twitter = []\n",
    "\n",
    "for data in results:\n",
    "    print(data)\n",
    "    log =math.log(data['perc_presence_lex_res'],10)\n",
    "    istogramAnticipation_lexRes.append([data['_id'], data['perc_presence_lex_res']])\n",
    "    log =math.log(data['perc_presence_twitter'],10)\n",
    "    istogramAnticipation_twitter.append([data['_id'], data['perc_presence_twitter']])\n",
    " \n",
    "# Create the pandas DataFrame\n",
    "dfAnticipation_lexRes = pd.DataFrame(istogramAnticipation_lexRes, columns = ['Risorsa', 'Percentuale'])\n",
    "dfAnticipation_twitter = pd.DataFrame(istogramAnticipation_twitter,columns = ['Risorsa', 'Percentuale'])      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione istogramma per presenza parole nei twitter\n",
    "\n",
    "dfAnticipation_lexRes.plot.bar(x='Risorsa')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione istogramma per presenza parole nei twitter\n",
    "\n",
    "dfAnticipation_twitter.plot.bar(x='Risorsa')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connessione, creazione cursore e database\n",
    "conn = psycopg2.connect(database = \"maadb\", user = \"postgres\", password = \"admin\", host = \"localhost\", port = \"5432\")\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione tabella risorse lessicali anticipation e inserimento\n",
    "cur.execute('''CREATE TABLE RISORSEANTICIPATION\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    NOME             TEXT       NOT NULL,\n",
    "                    PAROLA           TEXT       NOT NULL\n",
    "                );\n",
    "            '''\n",
    ")\n",
    "nome = \"NRC_anticipation\"\n",
    "for word in resourceNRC_anticipation:\n",
    "    cur.execute(\"INSERT INTO RISORSEANTICIPATION (NOME, PAROLA) VALUES(%s, %s)\",(nome,word))\n",
    "\n",
    "nome = \"sentisense_anticipation\"\n",
    "for word in resourceSentisense_anticipation:\n",
    "    cur.execute(\"INSERT INTO RISORSEANTICIPATION (NOME, PAROLA) VALUES(%s, %s)\",(nome,word))\n",
    "\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione tabelle per le parole, emoji, emoticon e hashtag\n",
    "\n",
    "cur.execute('''CREATE TABLE PAROLEANTICIPATION\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    PAROLA           TEXT       NOT NULL, \n",
    "                    NRC_anticipation      BOOL       NOT NULL DEFAULT FALSE,\n",
    "                    sentisense_anticipation      BOOL       NOT NULL DEFAULT FALSE,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ")   \n",
    "\n",
    "cur.execute('''CREATE TABLE EMOJIANTICIPATION\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    EMOJI       TEXT       NOT NULL,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ") \n",
    "\n",
    "cur.execute('''CREATE TABLE EMOTICONSANTICIPATION\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    EMOTICONS       TEXT       NOT NULL,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ") \n",
    "\n",
    "cur.execute('''CREATE TABLE HASHTAGANTICIPATION\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    HASHTAG     TEXT       NOT NULL,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ") \n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inserimento hashtag, emoticon ed emoji\n",
    "\n",
    "for hashTag in hashtagAnticipation:\n",
    "    cur.execute(\"INSERT INTO HASHTAGANTICIPATION (HASHTAG,FREQUENCY) VALUES(%s,%s)\",(hashTag,0))\n",
    "\n",
    "for emoticons in emoticonAnticipation:\n",
    "    cur.execute(\"INSERT INTO EMOTICONSANTICIPATION (EMOTICONS,FREQUENCY) VALUES(%s,%s)\",(emoticons,0))\n",
    "\n",
    "for emojis in emojiAnticipation:\n",
    "    cur.execute(\"INSERT INTO EMOJIANTICIPATION (EMOJI,FREQUENCY) VALUES(%s,%s)\",(emojis,0))\n",
    "\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inserimento parole in db\n",
    "for word in anticipationListWord:\n",
    "    cur.execute(\"INSERT INTO PAROLEANTICIPATION (PAROLA,NRC_anticipation,sentisense_anticipation,FREQUENCY) VALUES(%s,%s,%s,%s)\",(word,False,False,0))\n",
    "\n",
    "conn.commit()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze hashtag\n",
    "cur.execute(\"update HASHTAGANTICIPATION as ha1 set frequency = (select count(*) from HASHTAGANTICIPATION ha2 where ha1.hashtag = ha2.hashtag);\")\n",
    "cur.execute(\"delete from HASHTAGANTICIPATION as ha1 where exists (Select 1 From HASHTAGANTICIPATION as ha2 where ha1.hashtag = ha2.hashtag and ha1.frequency = ha2.frequency and ha2.ctid > ha1.ctid);\")\n",
    "\n",
    "conn.commit()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze emoji\n",
    "cur.execute(\"update EMOJIANTICIPATION as ha1 set frequency = (select count(*) from EMOJIANTICIPATION ha2 where ha1.emoji = ha2.emoji);\")\n",
    "cur.execute(\"delete from EMOJIANTICIPATION as ha1 where exists (Select 1 From EMOJIANTICIPATION as ha2 where ha1.emoji = ha2.emoji and ha1.frequency = ha2.frequency and ha2.ctid > ha1.ctid);\")\n",
    "\n",
    "conn.commit()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze emoticons\n",
    "cur.execute(\"update EMOTICONSANTICIPATION as ha1 set frequency = (select count(*) from EMOTICONSANTICIPATION ha2 where ha1.emoticons = ha2.emoticons);\")\n",
    "cur.execute(\"delete from EMOTICONSANTICIPATION as ha1 where exists (Select 1 From EMOTICONSANTICIPATION as ha2 where ha1.emoticons = ha2.emoticons and ha1.frequency = ha2.frequency and ha2.ctid > ha1.ctid);\")\n",
    "\n",
    "conn.commit()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze parole\n",
    "cur.execute(\"select count(*),parola from PAROLEANTICIPATION group by parola\")\n",
    "result = []\n",
    "for word in cur.fetchall():\n",
    "    result.append(word) \n",
    "\n",
    "cur.execute(\"truncate table PAROLEANTICIPATION;\")\n",
    "cur.execute(\"ALTER SEQUENCE paroleanticipation_id_seq RESTART WITH 1;\")\n",
    "\n",
    "for couple in result:\n",
    "    cur.execute(\"INSERT INTO PAROLEANTICIPATION (PAROLA,NRC_anticipation,sentisense_anticipation,FREQUENCY) VALUES(%s,%s,%s,%s)\",(couple[1],False,False,couple[0]))\n",
    "\n",
    "conn.commit()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggiornamento presenza parola nelle risorse lessicali\n",
    "cur.execute(\"UPDATE PAROLEANTICIPATION SET NRC_anticipation = TRUE WHERE parola IN (select p.parola from PAROLEANTICIPATION as p LEFT OUTER JOIN RISORSEANTICIPATION as r ON p.parola = r.parola WHERE r.nome = 'NRC_anticipation')\")\n",
    "cur.execute(\"UPDATE PAROLEANTICIPATION SET sentisense_anticipation = TRUE WHERE parola IN (select p.parola from PAROLEANTICIPATION as p LEFT OUTER JOIN RISORSEANTICIPATION as r ON p.parola = r.parola WHERE r.nome = 'sentisense_anticipation')\")\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvataggio in un txt delle parole non presenti in nessuna risorsa lessicale\n",
    "wordsMongo = anticipationCollections.find({\"LexResources\": []}, {\"_id\": 0, \"freq\": 0, \"LexResources\": 0})\n",
    "words = []\n",
    "\n",
    "for word in wordsMongo:\n",
    "    words.append(word['word'])\n",
    "\n",
    "with open('Nuove Parole/anticipation.txt', 'w',encoding=\"utf8\") as f:\n",
    "    for word in words:\n",
    "        f.write(\"%s\\n\" % word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trattamento dataset Disgust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lista di righe, ogni riga una tweet disgust\n",
    "file = open(\"Dataset/dataset_dt_disgust_60k.txt\", 'r',encoding=\"utf8\")\n",
    "disgust = []\n",
    "hashtagDisgust = []\n",
    "emoticonDisgust = []\n",
    "emojiDisgust = []\n",
    "\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "    riga = re.sub(r'URL', '',riga)\n",
    "    riga = re.sub(r'USERNAME', '',riga)\n",
    "\n",
    "    #array di hashtag singoli\n",
    "    hashTag = (re.findall('#[a-zA-Z0-9]+', riga))\n",
    "    \n",
    "    #generiamo l'array di hashtag senza hashtag\n",
    "    for h in hashTag:\n",
    "        hashtagDisgust.append(h.replace(\"#\",\"\"))\n",
    "\n",
    "    #Rimuoviamo glin hashtag dalla riga\n",
    "    riga = riga.replace('#',\"\")\n",
    "\n",
    "    disgust.append(riga)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(disgust)):\n",
    "    #Controllo se l'emoticons c'è e in caso la rimuovo\n",
    "    for singleEmoticon in emoticonsList:\n",
    "        emoticonsFind = re.findall(re.escape(singleEmoticon),disgust[i])\n",
    "        if(len(emoticonsFind) != 0):\n",
    "            emoticonDisgust.extend(emoticonsFind)\n",
    "            disgust[i] = disgust[i].replace(singleEmoticon,\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(disgust)):\n",
    "    #preleviamo le emoji e le rimuoviamo dal testo\n",
    "    data = regex.findall(r'\\X', disgust[i])\n",
    "    for word in data:\n",
    "        if any(char in emoji.UNICODE_EMOJI['en'] for char in word):\n",
    "            emojiDisgust.append(word)\n",
    "            disgust[i] = disgust[i].replace(word, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processamento slang prima di punteggiatura e lowercase\n",
    "with open(\"Slangs/slangs_before.json\") as f:\n",
    "    slangsBefore = json.load(f)\n",
    "    for i in range(len(disgust)):\n",
    "        for word in disgust[i].split():\n",
    "            indexOfWord = disgust[i].find(word)\n",
    "            if slangsBefore.get(word) is None:\n",
    "                if slangsBefore.get(word.lower()) is None:\n",
    "                    if slangsBefore.get(word.upper()) is None:\n",
    "                        continue\n",
    "                    else:\n",
    "                        disgust[i] = disgust[i][:indexOfWord] + slangsBefore.get(word.upper()) + disgust[i][indexOfWord+len(word):]\n",
    "                else:\n",
    "                    disgust[i] = disgust[i][:indexOfWord] + slangsBefore.get(word.lower()) + disgust[i][indexOfWord+len(word):]\n",
    "            else:\n",
    "                disgust[i] = disgust[i][:indexOfWord] + slangsBefore.get(word) + disgust[i][indexOfWord+len(word):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizzazione di tutte le frasi\n",
    "disgustTokenized = []\n",
    "for linea in disgust:\n",
    "    result = word_tokenize(linea)\n",
    "    disgustTokenized.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Esecuzione della POS tagging per ogni riga della matrice\n",
    "for i in range(len(disgustTokenized)):\n",
    "    disgustTokenized[i] = nltk.pos_tag(disgustTokenized[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trattamento punteggiatura\n",
    "for line in disgustTokenized:\n",
    "    newLine = line\n",
    "    listToRemove = []\n",
    "    for i in range(len(line)):\n",
    "        if line[i][0] in puntuaction:\n",
    "            listToRemove.append(line[i])\n",
    "    for element in listToRemove:\n",
    "        newLine.remove(element)\n",
    "    line = newLine\n",
    "\n",
    "linePuntuaction = re.compile(\"[\" + re.escape(\"\".join(puntuaction)) + \"]\")\n",
    "for line in disgustTokenized:\n",
    "    for i in range(len(line)):\n",
    "        tupla = [(re.sub(linePuntuaction, \"\", line[i][0])),line[i][1]]\n",
    "        line[i] = tupla\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming\n",
    "#ps = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for line in disgustTokenized:\n",
    "    for i in range(len(line)):\n",
    "        tupla = [lemmatizer.lemmatize(line[i][0]),line[i][1]]\n",
    "        line[i] = tupla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trattamnento Stop word dopo Stemming\n",
    "stop_words = set(stopwords.words('english'))\n",
    "disgustTokStoped = []\n",
    "for line in disgustTokenized:\n",
    "    filtred_word = []\n",
    "    for row in line:\n",
    "        if  row[0] not in stop_words:\n",
    "            filtred_word.append(row) \n",
    "    disgustTokStoped.append(filtred_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "disgustListWord = []\n",
    "disgustListTok = []\n",
    "for line in disgustTokStoped:\n",
    "    for row in line:\n",
    "        if row[0] != '':\n",
    "            disgustListWord.append(row[0])\n",
    "        disgustListTok.append(row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lavorazione risorse lessicali\n",
    "resourceDisgustSentisense_disgust = []\n",
    "resourceDisgustNRC_disgust = []\n",
    "resourceDisgustSentisense_hate = []\n",
    "#Lista di righe, ogni riga una tweet disgust\n",
    "file = open(\"Risorse lessicali\\Disgust-Hate\\sentisense_disgust.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        resourceDisgustSentisense_disgust.append(riga)\n",
    "\n",
    "file.close()\n",
    "\n",
    "file = open(\"C:Risorse lessicali\\\\Disgust-Hate\\\\NRC_disgust.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        resourceDisgustNRC_disgust.append(riga)\n",
    "\n",
    "file.close()\n",
    "\n",
    "file = open(\"Risorse lessicali\\Disgust-Hate\\sentisense_hate.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        resourceDisgustSentisense_hate.append(riga)\n",
    "\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.InsertOneResult at 0x1a700023ee0>"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Caricare le risorse lessicali \n",
    "\n",
    "DisgustSentisense = {\n",
    "    '_id': 'Sentisense_disgust',\n",
    "    'sentiment': 'disgust', \n",
    "    'wordList': resourceDisgustSentisense_disgust\n",
    "}\n",
    "\n",
    "DisgustNRC_disgust = {\n",
    "    '_id': 'NRC_disgust',\n",
    "    'sentiment': 'disgust',  \n",
    "    'wordList': resourceDisgustNRC_disgust\n",
    "}\n",
    "\n",
    "DisgustSentisense_hate = {\n",
    "    '_id': 'Sentisense_hatesense',\n",
    "    'sentiment': 'disgust',\n",
    "    'wordList': resourceDisgustSentisense_hate\n",
    "}\n",
    "\n",
    "resourcesCollection.insert_one(DisgustSentisense)\n",
    "resourcesCollection.insert_one(DisgustNRC_disgust)\n",
    "resourcesCollection.insert_one(DisgustSentisense_hate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione database e collezioni\n",
    "# Inserimento singoli documenti contententi i tweet disgust\n",
    "wordsString = {}\n",
    "\n",
    "for i in range(len(disgustListWord)):\n",
    "    wordsString = {\n",
    "        \"lemma\": disgustListWord[i],\n",
    "        \"POS\": disgustListTok[i]\n",
    "    }\n",
    "    disgustTwit={\"word\":wordsString}\n",
    "    disgustCollections.insert_one(disgustTwit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.InsertOneResult at 0x1a700023eb0>"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Inserimento collezione Twitter con hashtags, emoticons ed emoji \n",
    "disgustInsert = {\n",
    "    \"sentiment\":\"disgust\",\n",
    "    \"hashtags\": hashtagDisgust,\n",
    "    \"emoticons\": emoticonDisgust,\n",
    "    \"emoji\": emojiDisgust\n",
    "}\n",
    "\n",
    "#inserimento\n",
    "collectionDisgust.insert_one(disgustInsert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.command_cursor.CommandCursor at 0x1a7747447f0>"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Raggruppa i documenti di input in base all'espressione _id specificata e per ciascun raggruppamento distinto genera un documento\n",
    "pipeline = [\n",
    "    {\"$group\": {\"_id\": \"$word.lemma\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"word\": \"$_id\",\n",
    "      \"freq\": \"$count\"\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"freq\": -1\n",
    "    }},\n",
    "    {\"$out\": \"disgust\"}\n",
    "]\n",
    "\n",
    "#post processing per renderlo valido \n",
    "disgustCollections.aggregate(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<wordcloud.wordcloud.WordCloud at 0x1a7747440d0>"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creazione Word Clouds per le parole\n",
    "data = {}\n",
    "for document in disgustCollections.find().limit(50):\n",
    "    data[document['word']] = document['freq']\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2)\n",
    "wc.fit_words(data)\n",
    "wc.to_file('Word Clouds/Disgust/disgustWords.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.UpdateResult at 0x1a772c4ec40>"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Conteggio degli hashtags\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$hashtags\"},\n",
    "    {\"$group\": {\"_id\": \"$hashtags\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"word\": \"$_id\",\n",
    "      \"count\": 1\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"count\": -1\n",
    "    }}\n",
    "]\n",
    "\n",
    "hashtagsCount = collectionDisgust.aggregate(pipeline)\n",
    "\n",
    "data = {}\n",
    "for document in hashtagsCount:\n",
    "    data[document['word']] = document['count']\n",
    "\n",
    "collectionDisgust.update_one(\n",
    "  {\"sentiment\": \"disgust\"},\n",
    "  {\"$set\": {\"hashtags\": data}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<wordcloud.wordcloud.WordCloud at 0x1a774744c40>"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creazione Word Clouds per gli hashtags\n",
    "\n",
    "hashtags = collectionDisgust.distinct(\"hashtags\")\n",
    "\n",
    "hashtagsLimited = {}\n",
    "for (word,count) in [x for x in hashtags[0].items()][:50]:\n",
    "    hashtagsLimited[word] = count\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2)\n",
    "wc.fit_words(hashtagsLimited)\n",
    "wc.to_file('Word Clouds/Disgust/disgustHashtags.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.UpdateResult at 0x1a772c4e4c0>"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Conteggio delle emoji\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$emoji\"},\n",
    "    {\"$group\": {\"_id\": \"$emoji\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"emoji\": \"$_id\",\n",
    "      \"count\": 1\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"count\": -1\n",
    "    }}\n",
    "]\n",
    "\n",
    "emojiCount = collectionDisgust.aggregate(pipeline)\n",
    "\n",
    "data = {}\n",
    "for document in emojiCount:\n",
    "    data[document['emoji']] = document['count']\n",
    "\n",
    "collectionDisgust.update_one(\n",
    "  {\"sentiment\": \"disgust\"},\n",
    "  {\"$set\": {\"emoji\": data}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<wordcloud.wordcloud.WordCloud at 0x1a7748c2a60>"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creazione Word Clouds per le emoji\n",
    "\n",
    "emojis = collectionDisgust.distinct(\"emoji\")\n",
    "\n",
    "emojisLimited = {}\n",
    "for (word,count) in [x for x in emojis[0].items()][:50]:\n",
    "    emojisLimited[word] = count\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2, font_path='Font Emoji\\Symbola.otf')\n",
    "wc.fit_words(emojisLimited)\n",
    "wc.to_file('Word Clouds/Disgust/disgustEmojis.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.UpdateResult at 0x1a7748c2b50>"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Conteggio degli emoticons\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$emoticons\"},\n",
    "    {\"$group\": {\"_id\": \"$emoticons\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"emoticons\": \"$_id\",\n",
    "      \"count\": 1\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"count\": -1\n",
    "    }}\n",
    "]\n",
    "\n",
    "emoticonsCount = collectionDisgust.aggregate(pipeline)\n",
    "\n",
    "data = {}\n",
    "for document in emoticonsCount:\n",
    "  data = document['emoticons']\n",
    "\n",
    "collectionDisgust.update_one(\n",
    "  {\"sentiment\": \"disgust\"},\n",
    "  {\"$set\": {\"emoticons\": data}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\lores\\Desktop\\progettoMaadb\\Code\\main.ipynb Cell 101'\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/lores/Desktop/progettoMaadb/Code/main.ipynb#ch0000100?line=2'>3</a>\u001b[0m emoticons \u001b[39m=\u001b[39m collectionDisgust\u001b[39m.\u001b[39mdistinct(\u001b[39m\"\u001b[39m\u001b[39memoticons\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/lores/Desktop/progettoMaadb/Code/main.ipynb#ch0000100?line=4'>5</a>\u001b[0m emoticonsLimited \u001b[39m=\u001b[39m {}\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/lores/Desktop/progettoMaadb/Code/main.ipynb#ch0000100?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;00m (word,count) \u001b[39min\u001b[39;00m [x \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m emoticons[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mitems()][:\u001b[39m50\u001b[39m]:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/lores/Desktop/progettoMaadb/Code/main.ipynb#ch0000100?line=6'>7</a>\u001b[0m     emoticonsLimited[word] \u001b[39m=\u001b[39m count\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/lores/Desktop/progettoMaadb/Code/main.ipynb#ch0000100?line=8'>9</a>\u001b[0m wc \u001b[39m=\u001b[39m WordCloud(background_color\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mwhite\u001b[39m\u001b[39m'\u001b[39m, width \u001b[39m=\u001b[39m \u001b[39m1920\u001b[39m, height\u001b[39m=\u001b[39m\u001b[39m1080\u001b[39m, margin\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'items'"
     ]
    }
   ],
   "source": [
    "#Creazione Word Clouds per le emoticons\n",
    "\n",
    "emoticons = collectionDisgust.distinct(\"emoticons\")\n",
    "\n",
    "emoticonsLimited = {}\n",
    "for (word,count) in [x for x in emoticons[0].items()][:50]:\n",
    "    emoticonsLimited[word] = count\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2)\n",
    "wc.fit_words(emoticonsLimited)\n",
    "wc.to_file('Word Clouds/Disgust/disgustEmoticons.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline di aggregazione per inserire la risorsa lessicale a cui appartiene la parola\n",
    "\n",
    "pipeline = [\n",
    "    {\"$lookup\": {\n",
    "        \"from\": \"Resources\",\n",
    "        \"localField\": \"word\",\n",
    "        \"foreignField\": \"wordList\",\n",
    "        \"pipeline\": [\n",
    "            {\"$match\": {\"$expr\": {\"$regexMatch\": {\"input\": \"$_id\", \"regex\": \"disgust\"}}}}\n",
    "        ],\n",
    "        \"as\": \"LexResources\"\n",
    "    }},\n",
    "    {\"$project\": {\n",
    "        \"resources.wordList\": 0,\n",
    "        \"resources.totNumberWords\": 0,\n",
    "        \"resources.sentiment\": 0,\n",
    "        \"LexResources.sentiment\": 0,\n",
    "        \"LexResources.totNumberWords\": 0,\n",
    "        \"LexResources.wordList\": 0\n",
    "    }},\n",
    "    {\"$out\": \"disgust\"}\n",
    "]\n",
    "\n",
    "disgustCollections.aggregate(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline di aggregazione per calcolare le percentuali richieste\n",
    "\n",
    "pipelineCount = [\n",
    "    {\"$group\": {\"_id\": \"null\", \"count\": { \"$sum\": \"$freq\" }}},\n",
    "    {\"$project\": {\n",
    "        \"_id\": 0,\n",
    "        \"count\": \"$count\"\n",
    "    }}\n",
    "]\n",
    "\n",
    "results = disgustCollections.aggregate(pipelineCount)\n",
    "\n",
    "countWords = 0\n",
    "for data in results:\n",
    "    countWords = data['count']\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$LexResources\"},\n",
    "    {\"$lookup\": {\n",
    "        \"from\": \"Resources\",\n",
    "        \"localField\": \"LexResources._id\",\n",
    "        \"foreignField\": \"_id\",\n",
    "        \"as\": \"RefResource\"\n",
    "    }},\n",
    "    {\"$group\": {\"_id\": { \"lex\": \"$LexResources._id\", \"RefResource\": \"$RefResource\" }, \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "        \"_id\": \"$_id.lex\",\n",
    "        \"wordList\": \"$_id.RefResource.wordList\",\n",
    "        \"commonWords\": \"$count\"\n",
    "    }},\n",
    "    {\"$unwind\": \"$wordList\"},\n",
    "    {\"$project\": {\n",
    "        \"_id\": 1,\n",
    "        \"commonWords\": 1,\n",
    "        \"totWordsLex\": {\"$size\": \"$wordList\"}\n",
    "    }},\n",
    "    {\"$addFields\": {\"totalWordsTwitter\": countWords}},\n",
    "    {\"$project\": {\n",
    "        \"perc_presence_lex_res\": { \"$round\": [{\"$divide\": [ \"$commonWords\", \"$totWordsLex\" ] }, 6] },\n",
    "        \"perc_presence_twitter\": { \"$round\": [{\"$divide\": [ \"$commonWords\", \"$totalWordsTwitter\" ]}, 6] }\n",
    "    }}\n",
    "]\n",
    "\n",
    "results = disgustCollections.aggregate(pipeline)\n",
    "\n",
    "# Genero i dati per l'istogramma\n",
    "istogramDisgust_lexRes = []\n",
    "istogramDisgust_twitter = []\n",
    "\n",
    "for data in results:\n",
    "    print(data)\n",
    "    log =math.log(data['perc_presence_lex_res'],10)\n",
    "    istogramDisgust_lexRes.append([data['_id'], data['perc_presence_lex_res']])\n",
    "    log =math.log(data['perc_presence_twitter'],10)\n",
    "    istogramDisgust_twitter.append([data['_id'], data['perc_presence_twitter']])\n",
    " \n",
    "# Create the pandas DataFrame\n",
    "dfDisgust_lexRes = pd.DataFrame(istogramDisgust_lexRes, columns = ['Risorsa', 'Percentuale'])\n",
    "dfDisgust_twitter = pd.DataFrame(istogramDisgust_twitter,columns = ['Risorsa', 'Percentuale'])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione istogramma per presenza parole nelle risorse lessicali\n",
    "\n",
    "dfDisgust_lexRes.plot.bar(x='Risorsa')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione istogramma per presenza parole nei twitter\n",
    "\n",
    "dfDisgust_twitter.plot.bar(x='Risorsa')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connessione, creazione cursore e database\n",
    "\n",
    "conn = psycopg2.connect(database = \"maadb\", user = \"postgres\", password = \"admin\", host = \"localhost\", port = \"5432\")\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione tabella risorse lessicali disgust e inserimento\n",
    "cur.execute('''CREATE TABLE RISORSEDISGUST\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    NOME             TEXT       NOT NULL,\n",
    "                    PAROLA           TEXT       NOT NULL\n",
    "                );\n",
    "            '''\n",
    ")\n",
    "nome = \"NRC_disgust\"\n",
    "for word in resourceDisgustNRC_disgust:\n",
    "    cur.execute(\"INSERT INTO RISORSEDISGUST (NOME, PAROLA) VALUES(%s, %s)\",(nome,word))\n",
    "\n",
    "nome = \"sentisense_disgust\"\n",
    "for word in resourceDisgustSentisense_disgust:\n",
    "    cur.execute(\"INSERT INTO RISORSEDISGUST (NOME, PAROLA) VALUES(%s, %s)\",(nome,word))\n",
    "\n",
    "nome = \"sentisense_hate\"\n",
    "for word in resourceDisgustSentisense_hate:\n",
    "    cur.execute(\"INSERT INTO RISORSEDISGUST (NOME, PAROLA) VALUES(%s, %s)\",(nome,word))\n",
    "\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione tabelle per le parole, emoji, emoticon e hashtag\n",
    "\n",
    "cur.execute('''CREATE TABLE PAROLEDISGUST\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    PAROLA           TEXT       NOT NULL, \n",
    "                    NRC_disgust      BOOL       NOT NULL DEFAULT FALSE,\n",
    "                    sentisense_disgust      BOOL       NOT NULL DEFAULT FALSE,\n",
    "                    sentisense_hate      BOOL       NOT NULL DEFAULT FALSE,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ")   \n",
    "\n",
    "cur.execute('''CREATE TABLE EMOJIDISGUST\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    EMOJI       TEXT       NOT NULL,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ") \n",
    "\n",
    "cur.execute('''CREATE TABLE EMOTICONSDISGUST\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    EMOTICONS       TEXT       NOT NULL,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ") \n",
    "\n",
    "cur.execute('''CREATE TABLE HASHTAGDISGUST\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    HASHTAG     TEXT       NOT NULL,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ") \n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inserimento hashtag, emoticon ed emoji\n",
    "\n",
    "for hashTag in hashtagDisgust:\n",
    "    cur.execute(\"INSERT INTO HASHTAGDISGUST (HASHTAG,FREQUENCY) VALUES(%s,%s)\",(hashTag,0))\n",
    "\n",
    "for emoticons in emoticonDisgust:\n",
    "    cur.execute(\"INSERT INTO EMOTICONSDISGUST (EMOTICONS,FREQUENCY) VALUES(%s,%s)\",(emoticons,0))\n",
    "\n",
    "for emojis in emojiDisgust:\n",
    "    cur.execute(\"INSERT INTO EMOJIDISGUST (EMOJI,FREQUENCY) VALUES(%s,%s)\",(emojis,0))\n",
    "\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inserimento parole in db\n",
    "for word in disgustListWord:\n",
    "    cur.execute(\"INSERT INTO PAROLEDISGUST (PAROLA,NRC_disgust,sentisense_disgust,sentisense_hate,FREQUENCY) VALUES(%s,%s,%s,%s,%s)\",(word,False,False,False,0))\n",
    "\n",
    "conn.commit()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze hashtag\n",
    "cur.execute(\"update HASHTAGDISGUST as ha1 set frequency = (select count(*) from HASHTAGDISGUST ha2 where ha1.hashtag = ha2.hashtag);\")\n",
    "cur.execute(\"delete from HASHTAGDISGUST as ha1 where exists (Select 1 From HASHTAGDISGUST as ha2 where ha1.hashtag = ha2.hashtag and ha1.frequency = ha2.frequency and ha2.ctid > ha1.ctid);\")\n",
    "\n",
    "conn.commit()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze emoji\n",
    "cur.execute(\"update EMOJIDISGUST as ha1 set frequency = (select count(*) from EMOJIDISGUST ha2 where ha1.emoji = ha2.emoji);\")\n",
    "cur.execute(\"delete from EMOJIDISGUST as ha1 where exists (Select 1 From EMOJIDISGUST as ha2 where ha1.emoji = ha2.emoji and ha1.frequency = ha2.frequency and ha2.ctid > ha1.ctid);\")\n",
    "\n",
    "conn.commit()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze emoticons\n",
    "cur.execute(\"update EMOTICONSDISGUST as ha1 set frequency = (select count(*) from EMOTICONSDISGUST ha2 where ha1.emoticons = ha2.emoticons);\")\n",
    "cur.execute(\"delete from EMOTICONSDISGUST as ha1 where exists (Select 1 From EMOTICONSDISGUST as ha2 where ha1.emoticons = ha2.emoticons and ha1.frequency = ha2.frequency and ha2.ctid > ha1.ctid);\")\n",
    "\n",
    "conn.commit()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze parole\n",
    "cur.execute(\"select count(*),parola from PAROLEDISGUST group by parola\")\n",
    "result = []\n",
    "for word in cur.fetchall():\n",
    "    result.append(word) \n",
    "\n",
    "cur.execute(\"truncate table PAROLEDISGUST;\")\n",
    "cur.execute(\"ALTER SEQUENCE paroledisgust_id_seq RESTART WITH 1;\")\n",
    "\n",
    "for couple in result:\n",
    "    cur.execute(\"INSERT INTO PAROLEDISGUST (PAROLA,NRC_disgust,sentisense_disgust,sentisense_hate,FREQUENCY) VALUES(%s,%s,%s,%s,%s)\",(couple[1],False,False,False,couple[0]))\n",
    "\n",
    "conn.commit()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggiornamento presenza parola nelle risorse lessicali\n",
    "cur.execute(\"UPDATE PAROLEDISGUST SET NRC_disgust = TRUE WHERE parola IN (select p.parola from PAROLEDISGUST as p LEFT OUTER JOIN RISORSEDISGUST as r ON p.parola = r.parola WHERE r.nome = 'NRC_disgust')\")\n",
    "cur.execute(\"UPDATE PAROLEDISGUST SET sentisense_disgust = TRUE WHERE parola IN (select p.parola from PAROLEDISGUST as p LEFT OUTER JOIN RISORSEDISGUST as r ON p.parola = r.parola WHERE r.nome = 'sentisense_disgust')\")\n",
    "cur.execute(\"UPDATE PAROLEDISGUST SET sentisense_hate = TRUE WHERE parola IN (select p.parola from PAROLEDISGUST as p LEFT OUTER JOIN RISORSEDISGUST as r ON p.parola = r.parola WHERE r.nome = 'sentisense_hate')\")\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvataggio in un txt delle parole non presenti in nessuna risorsa lessicale\n",
    "\n",
    "wordsMongo = disgustCollections.find({\"LexResources\": []}, {\"_id\": 0, \"freq\": 0, \"LexResources\": 0})\n",
    "words = []\n",
    "\n",
    "for word in wordsMongo:\n",
    "    words.append(word['word'])\n",
    "\n",
    "with open('Nuove Parole/disgust.txt', 'w',encoding=\"utf8\") as f:\n",
    "    for word in words:\n",
    "        f.write(\"%s\\n\" % word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trattamento dataset Fear "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lista di righe, ogni riga una tweet fear\n",
    "file = open(\"Dataset/dataset_dt_fear_60k.txt\", 'r',encoding=\"utf8\")\n",
    "fear = []\n",
    "hashtagFear = []\n",
    "emoticonFear = []\n",
    "emojiFear = []\n",
    "\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "    riga = re.sub(r'URL', '',riga)\n",
    "    riga = re.sub(r'USERNAME', '',riga)\n",
    "\n",
    "     #array di hashtag singoli\n",
    "    hashTag = (re.findall('#[a-zA-Z0-9]+', riga))\n",
    "\n",
    "    #generiamo l'array di hashtag senza hashtag\n",
    "    for h in hashTag:\n",
    "        hashtagFear.append(h.replace(\"#\",\"\"))\n",
    "\n",
    "    #Rimuoviamo glin hashtag dalla riga\n",
    "    riga = riga.replace('#',\"\")\n",
    "\n",
    "    fear.append(riga)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(fear)):\n",
    "    #Controllo se l'emoticons c'è e in caso la rimuovo\n",
    "    for singleEmoticon in emoticonsList:\n",
    "        emoticonsFind = re.findall(re.escape(singleEmoticon),fear[i])\n",
    "        if(len(emoticonsFind) != 0):\n",
    "            emoticonFear.extend(emoticonsFind)\n",
    "            fear[i] = fear[i].replace(singleEmoticon,\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(fear)):\n",
    "    #preleviamo le emoji e le rimuoviamo dal testo\n",
    "    data = regex.findall(r'\\X', fear[i])\n",
    "    for word in data:\n",
    "        if any(char in emoji.UNICODE_EMOJI['en'] for char in word):\n",
    "            emojiFear.append(word)\n",
    "            fear[i] = fear[i].replace(word, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processamento slang prima di punteggiatura e lowercase\n",
    "with open(\"Slangs/slangs_before.json\") as f:\n",
    "    slangsBefore = json.load(f)\n",
    "    for i in range(len(fear)):\n",
    "        for word in fear[i].split():\n",
    "            indexOfWord = fear[i].find(word)\n",
    "            if slangsBefore.get(word) is None:\n",
    "                if slangsBefore.get(word.lower()) is None:\n",
    "                    if slangsBefore.get(word.upper()) is None:\n",
    "                        continue\n",
    "                    else:\n",
    "                        fear[i] = fear[i][:indexOfWord] + slangsBefore.get(word.upper()) + fear[i][indexOfWord+len(word):]\n",
    "                else:\n",
    "                    fear[i] = fear[i][:indexOfWord] + slangsBefore.get(word.lower()) + fear[i][indexOfWord+len(word):]\n",
    "            else:\n",
    "                fear[i] = fear[i][:indexOfWord] + slangsBefore.get(word) + fear[i][indexOfWord+len(word):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizzazione di tutte le frasi\n",
    "fearTokenized = []\n",
    "for linea in fear:\n",
    "    result = word_tokenize(linea)\n",
    "    fearTokenized.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Esecuzione della POS tagging per ogni riga della matrice\n",
    "for i in range(len(fearTokenized)):\n",
    "    fearTokenized[i] = nltk.pos_tag(fearTokenized[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trattamento punteggiatura\n",
    "for line in fearTokenized:\n",
    "    newLine = line\n",
    "    listToRemove = []\n",
    "    for i in range(len(line)):\n",
    "        if line[i][0] in puntuaction:\n",
    "            listToRemove.append(line[i])\n",
    "    for element in listToRemove:\n",
    "        newLine.remove(element)\n",
    "    line = newLine\n",
    "\n",
    "linePuntuaction = re.compile(\"[\" + re.escape(\"\".join(puntuaction)) + \"]\")\n",
    "for line in fearTokenized:\n",
    "    for i in range(len(line)):\n",
    "        tupla = [(re.sub(linePuntuaction, \"\", line[i][0])),line[i][1]]\n",
    "        line[i] = tupla\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming\n",
    "#ps = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for line in fearTokenized:\n",
    "    for i in range(len(line)):\n",
    "        tupla = [lemmatizer.lemmatize(line[i][0]),line[i][1]]\n",
    "        line[i] = tupla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trattamnento Stop word dopo Stemming\n",
    "stop_words = set(stopwords.words('english'))\n",
    "fearTokStoped = []\n",
    "for line in fearTokenized:\n",
    "    filtred_word = []\n",
    "    for row in line:\n",
    "        if  row[0] not in stop_words:\n",
    "            filtred_word.append(row) \n",
    "    fearTokStoped.append(filtred_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fearListWord = []\n",
    "fearListTok = []\n",
    "for line in fearTokStoped:\n",
    "    for row in line:\n",
    "        if row[0] != '':\n",
    "            fearListWord.append(row[0])\n",
    "        fearListTok.append(row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lavorazione risorse lessicali\n",
    "resourceFearNRC = []\n",
    "resourceFearSentisense = []\n",
    "#Lista di righe, ogni riga una tweet fear\n",
    "file = open(\"Risorse lessicali\\\\Fear\\\\NRC_fear.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        resourceFearNRC.append(riga)\n",
    "\n",
    "file.close()\n",
    "\n",
    "file = open(\"Risorse lessicali\\Fear\\sentisense_fear.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        resourceFearSentisense.append(riga)\n",
    "\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Caricare le risorse lessicali \n",
    "FearNRC = {\n",
    "    '_id': 'NRC_fear',\n",
    "    'sentiment': 'fear',  \n",
    "    'wordList': resourceFearNRC\n",
    "}\n",
    "\n",
    "FearSentisense = {\n",
    "    '_id': 'sentisense_fear',\n",
    "    'sentiment': 'fear',\n",
    "    'wordList': resourceFearSentisense\n",
    "}\n",
    "\n",
    "resourcesCollection.insert_one(FearNRC)\n",
    "resourcesCollection.insert_one(FearSentisense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione database e collezioni\n",
    "# Inserimento singoli documenti contententi i tweet fear\n",
    "wordsString = {}\n",
    "\n",
    "for i in range(len(fearListWord)):\n",
    "    wordsString = {\n",
    "        \"lemma\": fearListWord[i],\n",
    "        \"POS\": fearListTok[i]\n",
    "    }\n",
    "    fearTwit={\"word\":wordsString}\n",
    "    fearCollections.insert_one(fearTwit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inserimento collezione Twitter con hashtags, emoticons ed emoji \n",
    "fearInsert = {\n",
    "    \"sentiment\":\"fear\",\n",
    "    \"hashtags\": hashtagFear,\n",
    "    \"emoticons\": emoticonFear,\n",
    "    \"emoji\": emojiFear\n",
    "}\n",
    "\n",
    "#inserimento\n",
    "x = collectionFear.insert_one(fearInsert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raggruppa i documenti di input in base all'espressione _id specificata e per ciascun raggruppamento distinto genera un documento\n",
    "pipeline = [\n",
    "    {\"$group\": {\"_id\": \"$word.lemma\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"word\": \"$_id\",\n",
    "      \"freq\": \"$count\"\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"freq\": -1\n",
    "    }},\n",
    "    {\"$out\": \"fear\"}\n",
    "]\n",
    "\n",
    "#post processing per renderlo valido \n",
    "fearCollections.aggregate(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creazione Word Clouds per le parole\n",
    "data = {}\n",
    "for document in fearCollections.find().limit(50):\n",
    "    data[document['word']] = document['freq']\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2)\n",
    "wc.fit_words(data)\n",
    "wc.to_file('Word Clouds/Fear/fearWords.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conteggio degli hashtags\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$hashtags\"},\n",
    "    {\"$group\": {\"_id\": \"$hashtags\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"word\": \"$_id\",\n",
    "      \"count\": 1\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"count\": -1\n",
    "    }}\n",
    "]\n",
    "\n",
    "hashtagsCount = collectionFear.aggregate(pipeline)\n",
    "\n",
    "data = {}\n",
    "for document in hashtagsCount:\n",
    "    data[document['word']] = document['count']\n",
    "\n",
    "collectionFear.update_one(\n",
    "  {\"sentiment\": \"fear\"},\n",
    "  {\"$set\": {\"hashtags\": data}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creazione Word Clouds per gli hashtags\n",
    "\n",
    "hashtags = collectionFear.distinct(\"hashtags\")\n",
    "\n",
    "hashtagsLimited = {}\n",
    "for (word,count) in [x for x in hashtags[0].items()][:50]:\n",
    "    hashtagsLimited[word] = count\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2)\n",
    "wc.fit_words(hashtagsLimited)\n",
    "wc.to_file('Word Clouds/Fear/fearHashtags.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conteggio delle emoji\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$emoji\"},\n",
    "    {\"$group\": {\"_id\": \"$emoji\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"emoji\": \"$_id\",\n",
    "      \"count\": 1\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"count\": -1\n",
    "    }}\n",
    "]\n",
    "\n",
    "emojiCount = collectionFear.aggregate(pipeline)\n",
    "\n",
    "data = {}\n",
    "for document in emojiCount:\n",
    "    data[document['emoji']] = document['count']\n",
    "\n",
    "collectionFear.update_one(\n",
    "  {\"sentiment\": \"fear\"},\n",
    "  {\"$set\": {\"emoji\": data}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creazione Word Clouds per le emoji\n",
    "\n",
    "emojis = collectionFear.distinct(\"emoji\")\n",
    "\n",
    "emojisLimited = {}\n",
    "for (word,count) in [x for x in emojis[0].items()][:50]:\n",
    "    emojisLimited[word] = count\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2, font_path='Font Emoji\\Symbola.otf')\n",
    "wc.fit_words(emojisLimited)\n",
    "wc.to_file('Word Clouds/Fear/fearEmojis.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conteggio degli emoticons\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$emoticons\"},\n",
    "    {\"$group\": {\"_id\": \"$emoticons\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"emoticons\": \"$_id\",\n",
    "      \"count\": 1\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"count\": -1\n",
    "    }}\n",
    "]\n",
    "\n",
    "emoticonsCount = collectionFear.aggregate(pipeline)\n",
    "\n",
    "data = {}\n",
    "for document in emoticonsCount:\n",
    "    data[document['emoticons']] = document['count']\n",
    "\n",
    "collectionFear.update_one(\n",
    "  {\"sentiment\": \"fear\"},\n",
    "  {\"$set\": {\"emoticons\": data}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creazione Word Clouds per le emoticons\n",
    "\n",
    "emoticons = collectionFear.distinct(\"emoticons\")\n",
    "\n",
    "emoticonsLimited = {}\n",
    "for (word,count) in [x for x in emoticons[0].items()][:50]:\n",
    "    emoticonsLimited[word] = count\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2)\n",
    "wc.fit_words(emoticonsLimited)\n",
    "wc.to_file('Word Clouds/Fear/fearEmoticons.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline di aggregazione per inserire la risorsa lessicale a cui appartiene la parola\n",
    "pipeline = [\n",
    "    {\"$lookup\": {\n",
    "        \"from\": \"Resources\",\n",
    "        \"localField\": \"word\",\n",
    "        \"foreignField\": \"wordList\",\n",
    "        \"pipeline\": [\n",
    "            {\"$match\": {\"$expr\": {\"$regexMatch\": {\"input\": \"$_id\", \"regex\": \"fear\"}}}}\n",
    "        ],\n",
    "        \"as\": \"LexResources\"\n",
    "    }},\n",
    "    {\"$project\": {\n",
    "        \"resources.wordList\": 0,\n",
    "        \"resources.totNumberWords\": 0,\n",
    "        \"resources.sentiment\": 0,\n",
    "        \"LexResources.sentiment\": 0,\n",
    "        \"LexResources.totNumberWords\": 0,\n",
    "        \"LexResources.wordList\": 0\n",
    "    }},\n",
    "    {\"$out\": \"fear\"}\n",
    "]\n",
    "\n",
    "fearCollections.aggregate(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline di aggregazione per calcolare le percentuali richieste\n",
    "\n",
    "pipelineCount = [\n",
    "    {\"$group\": {\"_id\": \"null\", \"count\": { \"$sum\": \"$freq\" }}},\n",
    "    {\"$project\": {\n",
    "        \"_id\": 0,\n",
    "        \"count\": \"$count\"\n",
    "    }}\n",
    "]\n",
    "\n",
    "results = fearCollections.aggregate(pipelineCount)\n",
    "\n",
    "countWords = 0\n",
    "for data in results:\n",
    "    countWords = data['count']\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$LexResources\"},\n",
    "    {\"$lookup\": {\n",
    "        \"from\": \"Resources\",\n",
    "        \"localField\": \"LexResources._id\",\n",
    "        \"foreignField\": \"_id\",\n",
    "        \"as\": \"RefResource\"\n",
    "    }},\n",
    "    {\"$group\": {\"_id\": { \"lex\": \"$LexResources._id\", \"RefResource\": \"$RefResource\" }, \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "        \"_id\": \"$_id.lex\",\n",
    "        \"wordList\": \"$_id.RefResource.wordList\",\n",
    "        \"commonWords\": \"$count\"\n",
    "    }},\n",
    "    {\"$unwind\": \"$wordList\"},\n",
    "    {\"$project\": {\n",
    "        \"_id\": 1,\n",
    "        \"commonWords\": 1,\n",
    "        \"totWordsLex\": {\"$size\": \"$wordList\"}\n",
    "    }},\n",
    "    {\"$addFields\": {\"totalWordsTwitter\": countWords}},\n",
    "    {\"$project\": {\n",
    "        \"perc_presence_lex_res\": { \"$round\": [{\"$divide\": [ \"$commonWords\", \"$totWordsLex\" ] }, 6] },\n",
    "        \"perc_presence_twitter\": { \"$round\": [{\"$divide\": [ \"$commonWords\", \"$totalWordsTwitter\" ]}, 6] }\n",
    "    }}\n",
    "]\n",
    "\n",
    "results = fearCollections.aggregate(pipeline)\n",
    "\n",
    "# Genero i dati per l'istogramma\n",
    "istogramFear_lexRes = []\n",
    "istogramFear_twitter = []\n",
    "\n",
    "for data in results:\n",
    "    print(data)\n",
    "    log =math.log(data['perc_presence_lex_res'],10)\n",
    "    istogramFear_lexRes.append([data['_id'], data['perc_presence_lex_res']])\n",
    "    log =math.log(data['perc_presence_twitter'],10)\n",
    "    istogramFear_twitter.append([data['_id'], data['perc_presence_twitter']])\n",
    "\n",
    "# Create the pandas DataFrame\n",
    "dfFear_lexRes = pd.DataFrame(istogramFear_lexRes, columns = ['Risorsa', 'Percentuale'])\n",
    "dfFear_twitter = pd.DataFrame(istogramFear_twitter,columns = ['Risorsa', 'Percentuale'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione istogramma per presenza parole nei twitter\n",
    "\n",
    "dfFear_lexRes.plot.bar(x='Risorsa')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione istogramma per presenza parole nei twitter\n",
    "\n",
    "dfFear_twitter.plot.bar(x='Risorsa')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connessione, creazione cursore e database\n",
    "conn = psycopg2.connect(database = \"maadb\", user = \"postgres\", password = \"admin\", host = \"localhost\", port = \"5432\")\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione tabella risorse lessicali fear e inserimento\n",
    "cur.execute('''CREATE TABLE RISORSEFEAR\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    NOME             TEXT       NOT NULL,\n",
    "                    PAROLA           TEXT       NOT NULL\n",
    "                );\n",
    "            '''\n",
    ")\n",
    "nome = \"NRC_fear\"\n",
    "for word in resourceFearNRC:\n",
    "    cur.execute(\"INSERT INTO RISORSEFEAR (NOME, PAROLA) VALUES(%s, %s)\",(nome,word))\n",
    "\n",
    "nome = \"sentisense_fear\"\n",
    "for word in resourceFearSentisense:\n",
    "    cur.execute(\"INSERT INTO RISORSEFEAR (NOME, PAROLA) VALUES(%s, %s)\",(nome,word))\n",
    "\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione tabelle per le parole, emoji, emoticon e hashtag\n",
    "\n",
    "cur.execute('''CREATE TABLE PAROLEFEAR\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    PAROLA           TEXT       NOT NULL, \n",
    "                    NRC_fear      BOOL       NOT NULL DEFAULT FALSE,\n",
    "                    sentisense_fear      BOOL       NOT NULL DEFAULT FALSE,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ")   \n",
    "\n",
    "cur.execute('''CREATE TABLE EMOJIFEAR\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    EMOJI       TEXT       NOT NULL,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ") \n",
    "\n",
    "cur.execute('''CREATE TABLE EMOTICONSFEAR\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    EMOTICONS       TEXT       NOT NULL,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ") \n",
    "\n",
    "cur.execute('''CREATE TABLE HASHTAGFEAR\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    HASHTAG     TEXT       NOT NULL,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ") \n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inserimento hashtag, emoticon ed emoji\n",
    "\n",
    "for hashTag in hashtagFear:\n",
    "    cur.execute(\"INSERT INTO HASHTAGFEAR (HASHTAG,FREQUENCY) VALUES(%s,%s)\",(hashTag,0))\n",
    "\n",
    "for emoticons in emoticonFear:\n",
    "    cur.execute(\"INSERT INTO EMOTICONSFEAR (EMOTICONS,FREQUENCY) VALUES(%s,%s)\",(emoticons,0))\n",
    "\n",
    "for emojis in emojiFear:\n",
    "    cur.execute(\"INSERT INTO EMOJIFEAR (EMOJI,FREQUENCY) VALUES(%s,%s)\",(emojis,0))\n",
    "\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inserimento parole in db\n",
    "for word in fearListWord:\n",
    "    cur.execute(\"INSERT INTO PAROLEFEAR (PAROLA,NRC_fear,sentisense_fear,FREQUENCY) VALUES(%s,%s,%s,%s)\",(word,False,False,0))\n",
    "\n",
    "conn.commit()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze hashtag\n",
    "cur.execute(\"update HASHTAGFEAR as ha1 set frequency = (select count(*) from HASHTAGFEAR ha2 where ha1.hashtag = ha2.hashtag);\")\n",
    "cur.execute(\"delete from HASHTAGFEAR as ha1 where exists (Select 1 From HASHTAGFEAR as ha2 where ha1.hashtag = ha2.hashtag and ha1.frequency = ha2.frequency and ha2.ctid > ha1.ctid);\")\n",
    "\n",
    "conn.commit()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze emoji\n",
    "cur.execute(\"update EMOJIFEAR as ha1 set frequency = (select count(*) from EMOJIFEAR ha2 where ha1.emoji = ha2.emoji);\")\n",
    "cur.execute(\"delete from EMOJIFEAR as ha1 where exists (Select 1 From EMOJIFEAR as ha2 where ha1.emoji = ha2.emoji and ha1.frequency = ha2.frequency and ha2.ctid > ha1.ctid);\")\n",
    "\n",
    "conn.commit()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze emoticons\n",
    "cur.execute(\"update EMOTICONSFEAR as ha1 set frequency = (select count(*) from EMOTICONSFEAR ha2 where ha1.emoticons = ha2.emoticons);\")\n",
    "cur.execute(\"delete from EMOTICONSFEAR as ha1 where exists (Select 1 From EMOTICONSFEAR as ha2 where ha1.emoticons = ha2.emoticons and ha1.frequency = ha2.frequency and ha2.ctid > ha1.ctid);\")\n",
    "\n",
    "conn.commit() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze parole\n",
    "cur.execute(\"select count(*),parola from PAROLEFEAR group by parola\")\n",
    "result = []\n",
    "for word in cur.fetchall():\n",
    "    result.append(word) \n",
    "\n",
    "cur.execute(\"truncate table PAROLEFEAR;\")\n",
    "cur.execute(\"ALTER SEQUENCE parolefear_id_seq RESTART WITH 1;\")\n",
    "\n",
    "for couple in result:\n",
    "    cur.execute(\"INSERT INTO PAROLEFEAR (PAROLA,NRC_fear,sentisense_fear,FREQUENCY) VALUES(%s,%s,%s,%s)\",(couple[1],False,False,couple[0]))\n",
    "\n",
    "conn.commit()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggiornamento presenza parola nelle risorse lessicali\n",
    "cur.execute(\"UPDATE PAROLEFEAR SET NRC_fear = TRUE WHERE parola IN (select p.parola from PAROLEFEAR as p LEFT OUTER JOIN RISORSEFEAR as r ON p.parola = r.parola WHERE r.nome = 'NRC_fear')\")\n",
    "cur.execute(\"UPDATE PAROLEFEAR SET sentisense_fear = TRUE WHERE parola IN (select p.parola from PAROLEFEAR as p LEFT OUTER JOIN RISORSEFEAR as r ON p.parola = r.parola WHERE r.nome = 'sentisense_fear')\")\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvataggio in un txt delle parole non presenti in nessuna risorsa lessicale\n",
    "wordsMongo = fearCollections.find({\"LexResources\": []}, {\"_id\": 0, \"freq\": 0, \"LexResources\": 0})\n",
    "words = []\n",
    "\n",
    "for word in wordsMongo:\n",
    "    words.append(word['word'])\n",
    "\n",
    "with open('Nuove Parole/fear.txt', 'w',encoding=\"utf8\") as f:\n",
    "    for word in words:\n",
    "        f.write(\"%s\\n\" % word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trattamento dataset Joy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lista di righe, ogni riga una tweet joy\n",
    "file = open(\"Dataset/dataset_dt_joy_60k.txt\", 'r',encoding=\"utf8\")\n",
    "joy = []\n",
    "hashtagJoy = []\n",
    "emoticonJoy = []\n",
    "emojiJoy = []\n",
    "\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "    riga = re.sub(r'URL', '',riga)\n",
    "    riga = re.sub(r'USERNAME', '',riga)\n",
    "    \n",
    "    #array di hashtag singoli\n",
    "    hashTag = (re.findall('#[a-zA-Z0-9]+', riga))\n",
    "\n",
    "    #generiamo l'array di hashtag senza hashtag\n",
    "    for h in hashTag:\n",
    "        hashtagJoy.append(h.replace(\"#\",\"\"))\n",
    "\n",
    "    #Rimuoviamo glin hashtag dalla riga\n",
    "    riga = riga.replace('#',\"\")\n",
    "\n",
    "    joy.append(riga)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(joy)):\n",
    "    #Controllo se l'emoticons c'è e in caso la rimuovo\n",
    "    for singleEmoticon in emoticonsList:\n",
    "        emoticonsFind = re.findall(re.escape(singleEmoticon),joy[i])\n",
    "        if(len(emoticonsFind) != 0):\n",
    "            emoticonJoy.extend(emoticonsFind)\n",
    "            joy[i] = joy[i].replace(singleEmoticon,\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(joy)):\n",
    "    #preleviamo le emoji e le rimuoviamo dal testo\n",
    "    data = regex.findall(r'\\X', joy[i])\n",
    "    for word in data:\n",
    "        if any(char in emoji.UNICODE_EMOJI['en'] for char in word):\n",
    "            emojiJoy.append(word)\n",
    "            joy[i] = joy[i].replace(word, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processamento slang prima di punteggiatura e lowercase\n",
    "with open(\"Slangs/slangs_before.json\") as f:\n",
    "    slangsBefore = json.load(f)\n",
    "    for i in range(len(joy)):\n",
    "        for word in joy[i].split():\n",
    "            indexOfWord = joy[i].find(word)\n",
    "            if slangsBefore.get(word) is None:\n",
    "                if slangsBefore.get(word.lower()) is None:\n",
    "                    if slangsBefore.get(word.upper()) is None:\n",
    "                        continue\n",
    "                    else:\n",
    "                        joy[i] = joy[i][:indexOfWord] + slangsBefore.get(word.upper()) + joy[i][indexOfWord+len(word):]\n",
    "                else:\n",
    "                    joy[i] = joy[i][:indexOfWord] + slangsBefore.get(word.lower()) + joy[i][indexOfWord+len(word):]\n",
    "            else:\n",
    "                joy[i] = joy[i][:indexOfWord] + slangsBefore.get(word) + joy[i][indexOfWord+len(word):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizzazione di tutte le frasi\n",
    "joyTokenized = []\n",
    "for linea in joy:\n",
    "    result = word_tokenize(linea)\n",
    "    joyTokenized.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Esecuzione della POS tagging per ogni riga della matrice\n",
    "for i in range(len(joyTokenized)):\n",
    "    joyTokenized[i] = nltk.pos_tag(joyTokenized[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trattamento punteggiatura\n",
    "for line in joyTokenized:\n",
    "    newLine = line\n",
    "    listToRemove = []\n",
    "    for i in range(len(line)):\n",
    "        if line[i][0] in puntuaction:\n",
    "            listToRemove.append(line[i])\n",
    "    for element in listToRemove:\n",
    "        newLine.remove(element)\n",
    "    line = newLine\n",
    "\n",
    "linePuntuaction = re.compile(\"[\" + re.escape(\"\".join(puntuaction)) + \"]\")\n",
    "for line in joyTokenized:\n",
    "    for i in range(len(line)):\n",
    "        tupla = [(re.sub(linePuntuaction, \"\", line[i][0])),line[i][1]]\n",
    "        line[i] = tupla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming\n",
    "#ps = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for line in joyTokenized:\n",
    "    for i in range(len(line)):\n",
    "        tupla = [lemmatizer.lemmatize(line[i][0]),line[i][1]]\n",
    "        line[i] = tupla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trattamnento Stop word dopo Stemming\n",
    "stop_words = set(stopwords.words('english'))\n",
    "joyTokStoped = []\n",
    "for line in joyTokenized:\n",
    "    filtred_word = []\n",
    "    for row in line:\n",
    "        if  row[0] not in stop_words:\n",
    "            filtred_word.append(row) \n",
    "    joyTokStoped.append(filtred_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joyListWord = []\n",
    "joyListTok = []\n",
    "for line in joyTokStoped:\n",
    "    for row in line:\n",
    "        if row[0] != '':\n",
    "            joyListWord.append(row[0])\n",
    "        joyListTok.append(row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lavorazione risorse lessicali\n",
    "resourceJoyEmoSN = []\n",
    "resourceJoyNRC = []\n",
    "resourceJoySentisence = []\n",
    "#Lista di righe, ogni riga una tweet anger\n",
    "file = open(\"Risorse lessicali\\Joy\\EmoSN_joy.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        resourceJoyEmoSN.append(riga)\n",
    "\n",
    "file.close()\n",
    "\n",
    "file = open(\"Risorse lessicali\\\\Joy\\\\NRC_joy.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        resourceJoyNRC.append(riga)\n",
    "\n",
    "file.close()\n",
    "\n",
    "file = open(\"Risorse lessicali\\Joy\\sentisense_joy.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        resourceJoySentisence.append(riga)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Caricare le risorse lessicali \n",
    "\n",
    "JoyEmoSN = {\n",
    "    '_id': 'EmoSN_joy',\n",
    "    'sentiment': 'joy', \n",
    "    'wordList': resourceJoyEmoSN\n",
    "}\n",
    "\n",
    "JoyNRC = {\n",
    "    '_id': 'NRC_joy',\n",
    "    'sentiment': 'joy',  \n",
    "    'wordList': resourceJoyNRC\n",
    "}\n",
    "\n",
    "JoySentisense = {\n",
    "    '_id': 'sentisense_joy',\n",
    "    'sentiment': 'joy',\n",
    "    'wordList': resourceJoySentisence\n",
    "}\n",
    "\n",
    "resourcesCollection.insert_one(JoyEmoSN)\n",
    "resourcesCollection.insert_one(JoyNRC)\n",
    "resourcesCollection.insert_one(JoySentisense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione database e collezioni\n",
    "# Inserimento singoli documenti contententi i tweet joy\n",
    "wordsString = {}\n",
    "\n",
    "for i in range(len(joyListWord)):\n",
    "    wordsString = {\n",
    "        \"lemma\": joyListWord[i],\n",
    "        \"POS\": joyListTok[i]\n",
    "    }\n",
    "    joyTwit={\"word\":wordsString}\n",
    "    joyCollections.insert_one(joyTwit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inserimento collezione Twitter con hashtags, emoticons ed emoji \n",
    "joyInsert = {\n",
    "    \"sentiment\":\"joy\",\n",
    "    \"hashtags\": hashtagJoy,\n",
    "    \"emoticons\": emoticonJoy,\n",
    "    \"emoji\": emojiJoy\n",
    "}\n",
    "\n",
    "#inserimento\n",
    "x = collectionAJoy.insert_one(joyInsert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raggruppa i documenti di input in base all'espressione _id specificata e per ciascun raggruppamento distinto genera un documento\n",
    "pipeline = [\n",
    "    {\"$group\": {\"_id\": \"$word.lemma\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"word\": \"$_id\",\n",
    "      \"freq\": \"$count\"\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"freq\": -1\n",
    "    }},\n",
    "    {\"$out\": \"joy\"}\n",
    "]\n",
    "\n",
    "#post processing per renderlo valido \n",
    "joyCollections.aggregate(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creazione Word Clouds per le parole\n",
    "data = {}\n",
    "for document in joyCollections.find().limit(50):\n",
    "    data[document['word']] = document['freq']\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2)\n",
    "wc.fit_words(data)\n",
    "wc.to_file('Word Clouds/Joy/joyWords.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conteggio degli hashtags\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$hashtags\"},\n",
    "    {\"$group\": {\"_id\": \"$hashtags\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"word\": \"$_id\",\n",
    "      \"count\": 1\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"count\": -1\n",
    "    }}\n",
    "]\n",
    "\n",
    "hashtagsCount = collectionAJoy.aggregate(pipeline)\n",
    "\n",
    "data = {}\n",
    "for document in hashtagsCount:\n",
    "    data[document['word']] = document['count']\n",
    "\n",
    "collectionAJoy.update_one(\n",
    "  {\"sentiment\": \"joy\"},\n",
    "  {\"$set\": {\"hashtags\": data}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creazione Word Clouds per gli hashtags\n",
    "\n",
    "hashtags = collectionAJoy.distinct(\"hashtags\")\n",
    "\n",
    "hashtagsLimited = {}\n",
    "for (word,count) in [x for x in hashtags[0].items()][:50]:\n",
    "    hashtagsLimited[word] = count\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2)\n",
    "wc.fit_words(hashtagsLimited)\n",
    "wc.to_file('Word Clouds/Joy/joyHashtags.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conteggio delle emoji\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$emoji\"},\n",
    "    {\"$group\": {\"_id\": \"$emoji\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"emoji\": \"$_id\",\n",
    "      \"count\": 1\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"count\": -1\n",
    "    }}\n",
    "]\n",
    "\n",
    "emojiCount = collectionAJoy.aggregate(pipeline)\n",
    "\n",
    "data = {}\n",
    "for document in emojiCount:\n",
    "    data[document['emoji']] = document['count']\n",
    "\n",
    "collectionAJoy.update_one(\n",
    "  {\"sentiment\": \"joy\"},\n",
    "  {\"$set\": {\"emoji\": data}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creazione Word Clouds per le emoji\n",
    "\n",
    "emojis = collectionAJoy.distinct(\"emoji\")\n",
    "\n",
    "emojisLimited = {}\n",
    "for (word,count) in [x for x in emojis[0].items()][:50]:\n",
    "    emojisLimited[word] = count\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2, font_path='Font Emoji\\Symbola.otf')\n",
    "wc.fit_words(emojisLimited)\n",
    "wc.to_file('Word Clouds/Joy/joyEmojis.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conteggio degli emoticons\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$emoticons\"},\n",
    "    {\"$group\": {\"_id\": \"$emoticons\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"emoticons\": \"$_id\",\n",
    "      \"count\": 1\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"count\": -1\n",
    "    }}\n",
    "]\n",
    "\n",
    "emoticonsCount = collectionAJoy.aggregate(pipeline)\n",
    "\n",
    "data = {}\n",
    "for document in emoticonsCount:\n",
    "    data[document['emoticons']] = document['count']\n",
    "\n",
    "collectionAJoy.update_one(\n",
    "  {\"sentiment\": \"joy\"},\n",
    "  {\"$set\": {\"emoticons\": data}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creazione Word Clouds per le emoticons\n",
    "\n",
    "emoticons = collectionAJoy.distinct(\"emoticons\")\n",
    "\n",
    "emoticonsLimited = {}\n",
    "for (word,count) in [x for x in emoticons[0].items()][:50]:\n",
    "    emoticonsLimited[word] = count\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2)\n",
    "wc.fit_words(emoticonsLimited)\n",
    "wc.to_file('Word Clouds/Joy/joyEmoticons.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline di aggregazione per inserire la risorsa lessicale a cui appartiene la parola\n",
    "\n",
    "pipeline = [\n",
    "    {\"$lookup\": {\n",
    "        \"from\": \"Resources\",\n",
    "        \"localField\": \"word\",\n",
    "        \"foreignField\": \"wordList\",\n",
    "        \"pipeline\": [\n",
    "            {\"$match\": {\"$expr\": {\"$regexMatch\": {\"input\": \"$_id\", \"regex\": \"joy\"}}}}\n",
    "        ],\n",
    "        \"as\": \"LexResources\"\n",
    "    }},\n",
    "    {\"$project\": {\n",
    "        \"resources.wordList\": 0,\n",
    "        \"resources.totNumberWords\": 0,\n",
    "        \"resources.sentiment\": 0,\n",
    "        \"LexResources.sentiment\": 0,\n",
    "        \"LexResources.totNumberWords\": 0,\n",
    "        \"LexResources.wordList\": 0\n",
    "    }},\n",
    "    {\"$out\": \"joy\"}\n",
    "]\n",
    "\n",
    "joyCollections.aggregate(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline di aggregazione per calcolare le percentuali richieste\n",
    "\n",
    "pipelineCount = [\n",
    "    {\"$group\": {\"_id\": \"null\", \"count\": { \"$sum\": \"$freq\" }}},\n",
    "    {\"$project\": {\n",
    "        \"_id\": 0,\n",
    "        \"count\": \"$count\"\n",
    "    }}\n",
    "]\n",
    "\n",
    "results = joyCollections.aggregate(pipelineCount)\n",
    "\n",
    "countWords = 0\n",
    "for data in results:\n",
    "    countWords = data['count']\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$LexResources\"},\n",
    "    {\"$lookup\": {\n",
    "        \"from\": \"Resources\",\n",
    "        \"localField\": \"LexResources._id\",\n",
    "        \"foreignField\": \"_id\",\n",
    "        \"as\": \"RefResource\"\n",
    "    }},\n",
    "    {\"$group\": {\"_id\": { \"lex\": \"$LexResources._id\", \"RefResource\": \"$RefResource\" }, \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "        \"_id\": \"$_id.lex\",\n",
    "        \"wordList\": \"$_id.RefResource.wordList\",\n",
    "        \"commonWords\": \"$count\"\n",
    "    }},\n",
    "    {\"$unwind\": \"$wordList\"},\n",
    "    {\"$project\": {\n",
    "        \"_id\": 1,\n",
    "        \"commonWords\": 1,\n",
    "        \"totWordsLex\": {\"$size\": \"$wordList\"}\n",
    "    }},\n",
    "    {\"$addFields\": {\"totalWordsTwitter\": countWords}},\n",
    "    {\"$project\": {\n",
    "        \"perc_presence_lex_res\": { \"$round\": [{\"$divide\": [ \"$commonWords\", \"$totWordsLex\" ] }, 6] },\n",
    "        \"perc_presence_twitter\": { \"$round\": [{\"$divide\": [ \"$commonWords\", \"$totalWordsTwitter\" ]}, 6] }\n",
    "    }}\n",
    "]\n",
    "\n",
    "results = joyCollections.aggregate(pipeline)\n",
    "\n",
    "# Genero i dati per l'istogramma\n",
    "istogramJoy_lexRes = []\n",
    "istogramJoy_twitter = []\n",
    "\n",
    "for data in results:\n",
    "    print(data)\n",
    "    log =math.log(data['perc_presence_lex_res'],10)\n",
    "    istogramJoy_lexRes.append([data['_id'], data['perc_presence_lex_res']])\n",
    "    log =math.log(data['perc_presence_twitter'],10)\n",
    "    istogramJoy_twitter.append([data['_id'], data['perc_presence_twitter']])\n",
    " \n",
    "# Create the pandas DataFrame\n",
    "dfJoy_lexRes = pd.DataFrame(istogramJoy_lexRes, columns = ['Risorsa', 'Percentuale'])\n",
    "dfJoy_twitter = pd.DataFrame(istogramJoy_twitter,columns = ['Risorsa', 'Percentuale'])      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione istogramma per presenza parole nelle risorse lessicali\n",
    "\n",
    "dfJoy_lexRes.plot.bar(x='Risorsa')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione istogramma per presenza parole nei twitter\n",
    "\n",
    "dfJoy_twitter.plot.bar(x='Risorsa')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connessione, creazione cursore e database\n",
    "\n",
    "conn = psycopg2.connect(database = \"maadb\", user = \"postgres\", password = \"admin\", host = \"localhost\", port = \"5432\")\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione tabella risorse lessicali joy e inserimento\n",
    "cur.execute('''CREATE TABLE RISORSEJOY\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    NOME             TEXT       NOT NULL,\n",
    "                    PAROLA           TEXT       NOT NULL\n",
    "                );\n",
    "            '''\n",
    ")\n",
    "nome = \"EmoSN_joy\"\n",
    "for word in resourceJoyEmoSN:\n",
    "    cur.execute(\"INSERT INTO RISORSEJOY (NOME, PAROLA) VALUES(%s, %s)\",(nome,word))\n",
    "\n",
    "nome = \"NRC_joy\"\n",
    "for word in resourceJoyNRC:\n",
    "    cur.execute(\"INSERT INTO RISORSEJOY (NOME, PAROLA) VALUES(%s, %s)\",(nome,word))\n",
    "\n",
    "nome = \"sentisense_joy\"\n",
    "for word in resourceJoySentisence:\n",
    "    cur.execute(\"INSERT INTO RISORSEJOY (NOME, PAROLA) VALUES(%s, %s)\",(nome,word))\n",
    "\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione tabelle per le parole, emoji, emoticon e hashtag\n",
    "\n",
    "cur.execute('''CREATE TABLE PAROLEJOY\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    PAROLA           TEXT       NOT NULL, \n",
    "                    EmoSN_joy      BOOL       NOT NULL DEFAULT FALSE,\n",
    "                    NRC_joy      BOOL       NOT NULL DEFAULT FALSE,\n",
    "                    sentisense_joy      BOOL       NOT NULL DEFAULT FALSE,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ")   \n",
    "\n",
    "cur.execute('''CREATE TABLE EMOJIJOY\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    EMOJI       TEXT       NOT NULL,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ") \n",
    "\n",
    "cur.execute('''CREATE TABLE EMOTICONSJOY\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    EMOTICONS       TEXT       NOT NULL,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ") \n",
    "\n",
    "cur.execute('''CREATE TABLE HASHTAGJOY\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    HASHTAG     TEXT       NOT NULL,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ") \n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inserimento hashtag, emoticon ed emoji\n",
    "\n",
    "for hashTag in hashtagJoy:\n",
    "    cur.execute(\"INSERT INTO HASHTAGJOY (HASHTAG,FREQUENCY) VALUES(%s,%s)\",(hashTag,0))\n",
    "\n",
    "for emoticons in emoticonJoy:\n",
    "    cur.execute(\"INSERT INTO EMOTICONSJOY (EMOTICONS,FREQUENCY) VALUES(%s,%s)\",(emoticons,0))\n",
    "\n",
    "for emojis in emojiJoy:\n",
    "    cur.execute(\"INSERT INTO EMOJIJOY (EMOJI,FREQUENCY) VALUES(%s,%s)\",(emojis,0))\n",
    "\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inserimento parole in db\n",
    "for word in joyListWord:\n",
    "    cur.execute(\"INSERT INTO PAROLEJOY (PAROLA,EmoSN_joy,NRC_joy,sentisense_joy,FREQUENCY) VALUES(%s,%s,%s,%s,%s)\",(word,False,False,False,0))\n",
    "\n",
    "conn.commit()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze hashtag\n",
    "cur.execute(\"update HASHTAGJOY as ha1 set frequency = (select count(*) from HASHTAGJOY ha2 where ha1.hashtag = ha2.hashtag);\")\n",
    "cur.execute(\"delete from HASHTAGJOY as ha1 where exists (Select 1 From HASHTAGJOY as ha2 where ha1.hashtag = ha2.hashtag and ha1.frequency = ha2.frequency and ha2.ctid > ha1.ctid);\")\n",
    "\n",
    "conn.commit()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze emoji\n",
    "cur.execute(\"update EMOJIJOY as ha1 set frequency = (select count(*) from EMOJIJOY ha2 where ha1.emoji = ha2.emoji);\")\n",
    "cur.execute(\"delete from EMOJIJOY as ha1 where exists (Select 1 From EMOJIJOY as ha2 where ha1.emoji = ha2.emoji and ha1.frequency = ha2.frequency and ha2.ctid > ha1.ctid);\")\n",
    "\n",
    "conn.commit() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze emoticons\n",
    "cur.execute(\"update EMOTICONSJOY as ha1 set frequency = (select count(*) from EMOTICONSJOY ha2 where ha1.emoticons = ha2.emoticons);\")\n",
    "cur.execute(\"delete from EMOTICONSJOY as ha1 where exists (Select 1 From EMOTICONSJOY as ha2 where ha1.emoticons = ha2.emoticons and ha1.frequency = ha2.frequency and ha2.ctid > ha1.ctid);\")\n",
    "\n",
    "conn.commit()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze parole\n",
    "cur.execute(\"select count(*),parola from PAROLEJOY group by parola\")\n",
    "result = []\n",
    "for word in cur.fetchall():\n",
    "    result.append(word) \n",
    "\n",
    "cur.execute(\"truncate table PAROLEJOY;\")\n",
    "cur.execute(\"ALTER SEQUENCE parolejoy_id_seq RESTART WITH 1;\")\n",
    "\n",
    "for couple in result:\n",
    "    cur.execute(\"INSERT INTO PAROLEJOY (PAROLA,EmoSN_joy,NRC_joy,sentisense_joy,FREQUENCY) VALUES(%s,%s,%s,%s,%s)\",(couple[1],False,False,False,couple[0]))\n",
    "\n",
    "conn.commit()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggiornamento presenza parola nelle risorse lessicali\n",
    "cur.execute(\"UPDATE PAROLEJOY SET EmoSN_joy = TRUE WHERE parola IN (select p.parola from PAROLEJOY as p LEFT OUTER JOIN RISORSEJOY as r ON p.parola = r.parola WHERE r.nome = 'EmoSN_joy')\")\n",
    "cur.execute(\"UPDATE PAROLEJOY SET NRC_joy = TRUE WHERE parola IN (select p.parola from PAROLEJOY as p LEFT OUTER JOIN RISORSEJOY as r ON p.parola = r.parola WHERE r.nome = 'NRC_joy')\")\n",
    "cur.execute(\"UPDATE PAROLEJOY SET sentisense_joy = TRUE WHERE parola IN (select p.parola from PAROLEJOY as p LEFT OUTER JOIN RISORSEJOY as r ON p.parola = r.parola WHERE r.nome = 'sentisense_joy')\")\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvataggio in un txt delle parole non presenti in nessuna risorsa lessicale\n",
    "\n",
    "wordsMongo = joyCollections.find({\"LexResources\": []}, {\"_id\": 0, \"freq\": 0, \"LexResources\": 0})\n",
    "words = []\n",
    "\n",
    "for word in wordsMongo:\n",
    "    words.append(word['word'])\n",
    "\n",
    "with open('Nuove Parole/joy.txt', 'w',encoding=\"utf8\") as f:\n",
    "    for word in words:\n",
    "        f.write(\"%s\\n\" % word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trattamento dataset Sadness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lista di righe, ogni riga una tweet sadness\n",
    "file = open(\"Dataset/dataset_dt_sadness_60k.txt\", 'r',encoding=\"utf8\")\n",
    "sadness = []\n",
    "hashtagSadness = []\n",
    "emoticonSadness = []\n",
    "emojiSadness = []\n",
    "\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "    riga = re.sub(r'URL', '',riga)\n",
    "    riga = re.sub(r'USERNAME', '',riga)\n",
    "\n",
    "    #array di hashtag singoli\n",
    "    hashTag = (re.findall('#[a-zA-Z0-9]+', riga))\n",
    "\n",
    "    #generiamo l'array di hashtag senza hashtag\n",
    "    for h in hashTag:\n",
    "        hashtagSadness.append(h.replace(\"#\",\"\"))\n",
    "\n",
    "    #Rimuoviamo glin hashtag dalla riga\n",
    "    riga = riga.replace('#',\"\")\n",
    "\n",
    "    sadness.append(riga)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sadness)):\n",
    "    #Controllo se l'emoticons c'è e in caso la rimuovo\n",
    "    for singleEmoticon in emoticonsList:\n",
    "        emoticonsFind = re.findall(re.escape(singleEmoticon),sadness[i])\n",
    "        if(len(emoticonsFind) != 0):\n",
    "            emoticonSadness.extend(emoticonsFind)\n",
    "            sadness[i] = sadness[i].replace(singleEmoticon,\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sadness)):\n",
    "    #preleviamo le emoji e le rimuoviamo dal testo\n",
    "    data = regex.findall(r'\\X', sadness[i])\n",
    "    for word in data:\n",
    "        if any(char in emoji.UNICODE_EMOJI['en'] for char in word):\n",
    "            emojiSadness.append(word)\n",
    "            sadness[i] = sadness[i].replace(word, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processamento slang prima di punteggiatura e lowercase\n",
    "with open(\"Slangs/slangs_before.json\") as f:\n",
    "    slangsBefore = json.load(f)\n",
    "    for i in range(len(sadness)):\n",
    "        for word in sadness[i].split():\n",
    "            indexOfWord = sadness[i].find(word)\n",
    "            if slangsBefore.get(word) is None:\n",
    "                if slangsBefore.get(word.lower()) is None:\n",
    "                    if slangsBefore.get(word.upper()) is None:\n",
    "                        continue\n",
    "                    else:\n",
    "                        sadness[i] = sadness[i][:indexOfWord] + slangsBefore.get(word.upper()) + sadness[i][indexOfWord+len(word):]\n",
    "                else:\n",
    "                    sadness[i] = sadness[i][:indexOfWord] + slangsBefore.get(word.lower()) + sadness[i][indexOfWord+len(word):]\n",
    "            else:\n",
    "                sadness[i] = sadness[i][:indexOfWord] + slangsBefore.get(word) + sadness[i][indexOfWord+len(word):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizzazione di tutte le frasi\n",
    "sadnessTokenized = []\n",
    "for linea in sadness:\n",
    "    result = word_tokenize(linea)\n",
    "    sadnessTokenized.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Esecuzione della POS tagging per ogni riga della matrice\n",
    "for i in range(len(sadnessTokenized)):\n",
    "    sadnessTokenized[i] = nltk.pos_tag(sadnessTokenized[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trattamento punteggiatura\n",
    "for line in sadnessTokenized:\n",
    "    newLine = line\n",
    "    listToRemove = []\n",
    "    for i in range(len(line)):\n",
    "        if line[i][0] in puntuaction:\n",
    "            listToRemove.append(line[i])\n",
    "    for element in listToRemove:\n",
    "        newLine.remove(element)\n",
    "    line = newLine\n",
    "\n",
    "linePuntuaction = re.compile(\"[\" + re.escape(\"\".join(puntuaction)) + \"]\")\n",
    "for line in sadnessTokenized:\n",
    "    for i in range(len(line)):\n",
    "        tupla = [(re.sub(linePuntuaction, \"\", line[i][0])),line[i][1]]\n",
    "        line[i] = tupla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming\n",
    "#ps = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for line in sadnessTokenized:\n",
    "    for i in range(len(line)):\n",
    "        tupla = [lemmatizer.lemmatize(line[i][0]),line[i][1]]\n",
    "        line[i] = tupla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trattamnento Stop word dopo Stemming\n",
    "stop_words = set(stopwords.words('english'))\n",
    "sadnessTokStoped = []\n",
    "for line in sadnessTokenized:\n",
    "    filtred_word = []\n",
    "    for row in line:\n",
    "        if  row[0] not in stop_words:\n",
    "            filtred_word.append(row) \n",
    "    sadnessTokStoped.append(filtred_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sadnessListWord = []\n",
    "sadnessListTok = []\n",
    "for line in sadnessTokStoped:\n",
    "    for row in line:\n",
    "        if row[0] != '':\n",
    "            sadnessListWord.append(row[0])\n",
    "        sadnessListTok.append(row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lavorazione risorse lessicali\n",
    "resourceSadnessNRC = []\n",
    "resourceSadnessSentisense = []\n",
    "file = open(\"Risorse lessicali\\\\Sadness\\\\NRC_sadness.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        resourceSadnessNRC.append(riga)\n",
    "\n",
    "file.close()\n",
    "\n",
    "file = open(\"Risorse lessicali\\Sadness\\sentisense_sadness.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        resourceSadnessSentisense.append(riga)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Caricare le risorse lessicali \n",
    "SadnessNRC = {\n",
    "    '_id': 'NRC_sadness',\n",
    "    'sentiment': 'sadness',  \n",
    "    'wordList': resourceSadnessNRC\n",
    "}\n",
    "\n",
    "SadnessSentisense = {\n",
    "    '_id': 'sentisense_sadness',\n",
    "    'sentiment': 'sadness',\n",
    "    'wordList': resourceSadnessSentisense\n",
    "}\n",
    "\n",
    "resourcesCollection.insert_one(SadnessNRC)\n",
    "resourcesCollection.insert_one(SadnessSentisense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione database e collezioni\n",
    "# Inserimento singoli documenti contententi i tweet sadness\n",
    "wordsString = {}\n",
    "\n",
    "for i in range(len(sadnessListWord)):\n",
    "    wordsString = {\n",
    "        \"lemma\": sadnessListWord[i],\n",
    "        \"POS\": sadnessListTok[i]\n",
    "    }\n",
    "    sadnessTwit={\"word\":wordsString}\n",
    "    sadnessCollections.insert_one(sadnessTwit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inserimento collezione Twitter con hashtags, emoticons ed emoji \n",
    "sadnessInsert = {\n",
    "    \"sentiment\":\"sadness\",\n",
    "    \"hashtags\": hashtagSadness,\n",
    "    \"emoticons\": emoticonSadness,\n",
    "    \"emoji\": emojiSadness\n",
    "}\n",
    "\n",
    "#inserimento\n",
    "x = collectionSadness.insert_one(sadnessInsert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raggruppa i documenti di input in base all'espressione _id specificata e per ciascun raggruppamento distinto genera un documento\n",
    "pipeline = [\n",
    "    {\"$group\": {\"_id\": \"$word.lemma\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"word\": \"$_id\",\n",
    "      \"freq\": \"$count\"\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"freq\": -1\n",
    "    }},\n",
    "    {\"$out\": \"sadness\"}\n",
    "]\n",
    "\n",
    "#post processing per renderlo valido \n",
    "sadnessCollections.aggregate(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creazione Word Clouds per le parole\n",
    "data = {}\n",
    "for document in sadnessCollections.find().limit(50):\n",
    "    data[document['word']] = document['freq']\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2)\n",
    "wc.fit_words(data)\n",
    "wc.to_file('Word Clouds/Sadness/sadnessWords.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conteggio degli hashtags\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$hashtags\"},\n",
    "    {\"$group\": {\"_id\": \"$hashtags\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"word\": \"$_id\",\n",
    "      \"count\": 1\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"count\": -1\n",
    "    }}\n",
    "]\n",
    "\n",
    "hashtagsCount = collectionSadness.aggregate(pipeline)\n",
    "\n",
    "data = {}\n",
    "for document in hashtagsCount:\n",
    "    data[document['word']] = document['count']\n",
    "\n",
    "collectionSadness.update_one(\n",
    "  {\"sentiment\": \"sadness\"},\n",
    "  {\"$set\": {\"hashtags\": data}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creazione Word Clouds per gli hashtags\n",
    "\n",
    "hashtags = collectionSadness.distinct(\"hashtags\")\n",
    "\n",
    "hashtagsLimited = {}\n",
    "for (word,count) in [x for x in hashtags[0].items()][:50]:\n",
    "    hashtagsLimited[word] = count\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2)\n",
    "wc.fit_words(hashtagsLimited)\n",
    "wc.to_file('Word Clouds/Sadness/sadnessHashtags.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conteggio delle emoji\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$emoji\"},\n",
    "    {\"$group\": {\"_id\": \"$emoji\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"emoji\": \"$_id\",\n",
    "      \"count\": 1\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"count\": -1\n",
    "    }}\n",
    "]\n",
    "\n",
    "emojiCount = collectionSadness.aggregate(pipeline)\n",
    "\n",
    "data = {}\n",
    "for document in emojiCount:\n",
    "    data[document['emoji']] = document['count']\n",
    "\n",
    "collectionSadness.update_one(\n",
    "  {\"sentiment\": \"sadness\"},\n",
    "  {\"$set\": {\"emoji\": data}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creazione Word Clouds per le emoji\n",
    "\n",
    "emojis = collectionSadness.distinct(\"emoji\")\n",
    "\n",
    "emojisLimited = {}\n",
    "for (word,count) in [x for x in emojis[0].items()][:50]:\n",
    "    emojisLimited[word] = count\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2, font_path='Font Emoji\\Symbola.otf')\n",
    "wc.fit_words(emojisLimited)\n",
    "wc.to_file('Word Clouds/Sadness/sadnessEmojis.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conteggio degli emoticons\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$emoticons\"},\n",
    "    {\"$group\": {\"_id\": \"$emoticons\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"emoticons\": \"$_id\",\n",
    "      \"count\": 1\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"count\": -1\n",
    "    }}\n",
    "]\n",
    "\n",
    "emoticonsCount = collectionSadness.aggregate(pipeline)\n",
    "\n",
    "data = {}\n",
    "for document in emoticonsCount:\n",
    "    data[document['emoticons']] = document['count']\n",
    "\n",
    "collectionSadness.update_one(\n",
    "  {\"sentiment\": \"sadness\"},\n",
    "  {\"$set\": {\"emoticons\": data}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creazione Word Clouds per le emoticons\n",
    "\n",
    "emoticons = collectionSadness.distinct(\"emoticons\")\n",
    "\n",
    "emoticonsLimited = {}\n",
    "for (word,count) in [x for x in emoticons[0].items()][:50]:\n",
    "    emoticonsLimited[word] = count\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2)\n",
    "wc.fit_words(emoticonsLimited)\n",
    "wc.to_file('Word Clouds/Sadness/sadnessEmoticons.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline di aggregazione per inserire la risorsa lessicale a cui appartiene la parola\n",
    "\n",
    "pipeline = [\n",
    "    {\"$lookup\": {\n",
    "        \"from\": \"Resources\",\n",
    "        \"localField\": \"word\",\n",
    "        \"foreignField\": \"wordList\",\n",
    "        \"pipeline\": [\n",
    "            {\"$match\": {\"$expr\": {\"$regexMatch\": {\"input\": \"$_id\", \"regex\": \"sadness\"}}}}\n",
    "        ],\n",
    "        \"as\": \"LexResources\"\n",
    "    }},\n",
    "    {\"$project\": {\n",
    "        \"resources.wordList\": 0,\n",
    "        \"resources.totNumberWords\": 0,\n",
    "        \"resources.sentiment\": 0,\n",
    "        \"LexResources.sentiment\": 0,\n",
    "        \"LexResources.totNumberWords\": 0,\n",
    "        \"LexResources.wordList\": 0\n",
    "    }},\n",
    "    {\"$out\": \"sadness\"}\n",
    "]\n",
    "\n",
    "sadnessCollections.aggregate(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline di aggregazione per calcolare le percentuali richieste\n",
    "\n",
    "pipelineCount = [\n",
    "    {\"$group\": {\"_id\": \"null\", \"count\": { \"$sum\": \"$freq\" }}},\n",
    "    {\"$project\": {\n",
    "        \"_id\": 0,\n",
    "        \"count\": \"$count\"\n",
    "    }}\n",
    "]\n",
    "\n",
    "results = sadnessCollections.aggregate(pipelineCount)\n",
    "\n",
    "countWords = 0\n",
    "for data in results:\n",
    "    countWords = data['count']\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$LexResources\"},\n",
    "    {\"$lookup\": {\n",
    "        \"from\": \"Resources\",\n",
    "        \"localField\": \"LexResources._id\",\n",
    "        \"foreignField\": \"_id\",\n",
    "        \"as\": \"RefResource\"\n",
    "    }},\n",
    "    {\"$group\": {\"_id\": { \"lex\": \"$LexResources._id\", \"RefResource\": \"$RefResource\" }, \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "        \"_id\": \"$_id.lex\",\n",
    "        \"wordList\": \"$_id.RefResource.wordList\",\n",
    "        \"commonWords\": \"$count\"\n",
    "    }},\n",
    "    {\"$unwind\": \"$wordList\"},\n",
    "    {\"$project\": {\n",
    "        \"_id\": 1,\n",
    "        \"commonWords\": 1,\n",
    "        \"totWordsLex\": {\"$size\": \"$wordList\"}\n",
    "    }},\n",
    "    {\"$addFields\": {\"totalWordsTwitter\": countWords}},\n",
    "    {\"$project\": {\n",
    "        \"perc_presence_lex_res\": { \"$round\": [{\"$divide\": [ \"$commonWords\", \"$totWordsLex\" ] }, 6] },\n",
    "        \"perc_presence_twitter\": { \"$round\": [{\"$divide\": [ \"$commonWords\", \"$totalWordsTwitter\" ]}, 6] }\n",
    "    }}\n",
    "]\n",
    "\n",
    "results = sadnessCollections.aggregate(pipeline)\n",
    "\n",
    "# Genero i dati per l'istogramma\n",
    "istogramSadness_lexRes = []\n",
    "istogramSadness_twitter = []\n",
    "\n",
    "for data in results:\n",
    "    print(data)\n",
    "    log =math.log(data['perc_presence_lex_res'],10)\n",
    "    istogramSadness_lexRes.append([data['_id'], data['perc_presence_lex_res']])\n",
    "    log =math.log(data['perc_presence_twitter'],10)\n",
    "    istogramSadness_twitter.append([data['_id'], data['perc_presence_twitter']])\n",
    " \n",
    "# Create the pandas DataFrame\n",
    "dfSadness_lexRes = pd.DataFrame(istogramSadness_lexRes, columns = ['Risorsa', 'Percentuale'])\n",
    "dfSadness_twitter = pd.DataFrame(istogramSadness_twitter,columns = ['Risorsa', 'Percentuale'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione istogramma per presenza parole nelle risorse lessicali\n",
    "\n",
    "dfSadness_lexRes.plot.bar(x='Risorsa')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione istogramma per presenza parole nei twitter\n",
    "\n",
    "dfSadness_twitter.plot.bar(x='Risorsa')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connessione, creazione cursore e database\n",
    "\n",
    "conn = psycopg2.connect(database = \"maadb\", user = \"postgres\", password = \"admin\", host = \"localhost\", port = \"5432\")\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione tabella risorse lessicali sadness e inserimento\n",
    "cur.execute('''CREATE TABLE RISORSESADNESS\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    NOME             TEXT       NOT NULL,\n",
    "                    PAROLA           TEXT       NOT NULL\n",
    "                );\n",
    "            '''\n",
    ")\n",
    "\n",
    "nome = \"NRC_sadness\"\n",
    "for word in resourceSadnessNRC:\n",
    "    cur.execute(\"INSERT INTO RISORSESADNESS (NOME, PAROLA) VALUES(%s, %s)\",(nome,word))\n",
    "\n",
    "nome = \"sentisense_sadness\"\n",
    "for word in resourceSadnessSentisense:\n",
    "    cur.execute(\"INSERT INTO RISORSESADNESS (NOME, PAROLA) VALUES(%s, %s)\",(nome,word))\n",
    "\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione tabelle per le parole, emoji, emoticon e hashtag\n",
    "\n",
    "cur.execute('''CREATE TABLE PAROLESADNESS\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    PAROLA           TEXT       NOT NULL,\n",
    "                    NRC_sadness      BOOL       NOT NULL DEFAULT FALSE,\n",
    "                    sentisense_sadness      BOOL       NOT NULL DEFAULT FALSE,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ")   \n",
    "\n",
    "cur.execute('''CREATE TABLE EMOJISADNESS\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    EMOJI       TEXT       NOT NULL,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ") \n",
    "\n",
    "cur.execute('''CREATE TABLE EMOTICONSSADNESS\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    EMOTICONS       TEXT       NOT NULL,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ") \n",
    "\n",
    "cur.execute('''CREATE TABLE HASHTAGSADNESS\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    HASHTAG     TEXT       NOT NULL,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ") \n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inserimento hashtag, emoticon ed emoji\n",
    "\n",
    "for hashTag in hashtagSadness:\n",
    "    cur.execute(\"INSERT INTO HASHTAGSADNESS (HASHTAG,FREQUENCY) VALUES(%s,%s)\",(hashTag,0))\n",
    "\n",
    "for emoticons in emoticonSadness:\n",
    "    cur.execute(\"INSERT INTO EMOTICONSSADNESS (EMOTICONS,FREQUENCY) VALUES(%s,%s)\",(emoticons,0))\n",
    "\n",
    "for emojis in emojiSadness:\n",
    "    cur.execute(\"INSERT INTO EMOJISADNESS (EMOJI,FREQUENCY) VALUES(%s,%s)\",(emojis,0))\n",
    "\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inserimento parole in db\n",
    "for word in sadnessListWord:\n",
    "    cur.execute(\"INSERT INTO PAROLESADNESS (PAROLA,NRC_sadness,sentisense_sadness,FREQUENCY) VALUES(%s,%s,%s,%s)\",(word,False,False,0))\n",
    "\n",
    "conn.commit()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze hashtag\n",
    "cur.execute(\"update HASHTAGSADNESS as ha1 set frequency = (select count(*) from HASHTAGSADNESS ha2 where ha1.hashtag = ha2.hashtag);\")\n",
    "cur.execute(\"delete from HASHTAGSADNESS as ha1 where exists (Select 1 From HASHTAGSADNESS as ha2 where ha1.hashtag = ha2.hashtag and ha1.frequency = ha2.frequency and ha2.ctid > ha1.ctid);\")\n",
    "\n",
    "conn.commit()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze emoji\n",
    "cur.execute(\"update EMOJISADNESS as ha1 set frequency = (select count(*) from EMOJISADNESS ha2 where ha1.emoji = ha2.emoji);\")\n",
    "cur.execute(\"delete from EMOJISADNESS as ha1 where exists (Select 1 From EMOJISADNESS as ha2 where ha1.emoji = ha2.emoji and ha1.frequency = ha2.frequency and ha2.ctid > ha1.ctid);\")\n",
    "\n",
    "conn.commit() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze emoticons\n",
    "cur.execute(\"update EMOTICONSSADNESS as ha1 set frequency = (select count(*) from EMOTICONSSADNESS ha2 where ha1.emoticons = ha2.emoticons);\")\n",
    "cur.execute(\"delete from EMOTICONSSADNESS as ha1 where exists (Select 1 From EMOTICONSSADNESS as ha2 where ha1.emoticons = ha2.emoticons and ha1.frequency = ha2.frequency and ha2.ctid > ha1.ctid);\")\n",
    "\n",
    "conn.commit()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze parole\n",
    "cur.execute(\"select count(*),parola from PAROLESADNESS group by parola\")\n",
    "result = []\n",
    "for word in cur.fetchall():\n",
    "    result.append(word) \n",
    "\n",
    "cur.execute(\"truncate table PAROLESADNESS;\")\n",
    "cur.execute(\"ALTER SEQUENCE parolesadness_id_seq RESTART WITH 1;\")\n",
    "\n",
    "for couple in result:\n",
    "    cur.execute(\"INSERT INTO PAROLESADNESS (PAROLA,NRC_sadness,sentisense_sadness,FREQUENCY) VALUES(%s,%s,%s,%s)\",(couple[1],False,False,couple[0]))\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggiornamento presenza parola nelle risorse lessicali\n",
    "cur.execute(\"UPDATE PAROLESADNESS SET NRC_sadness = TRUE WHERE parola IN (select p.parola from PAROLESADNESS as p LEFT OUTER JOIN RISORSESADNESS as r ON p.parola = r.parola WHERE r.nome = 'NRC_sadness')\")\n",
    "cur.execute(\"UPDATE PAROLESADNESS SET sentisense_sadness = TRUE WHERE parola IN (select p.parola from PAROLESADNESS as p LEFT OUTER JOIN RISORSESADNESS as r ON p.parola = r.parola WHERE r.nome = 'sentisense_sadness')\")\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvataggio in un txt delle parole non presenti in nessuna risorsa lessicale\n",
    "\n",
    "wordsMongo = sadnessCollections.find({\"LexResources\": []}, {\"_id\": 0, \"freq\": 0, \"LexResources\": 0})\n",
    "words = []\n",
    "\n",
    "for word in wordsMongo:\n",
    "    words.append(word['word'])\n",
    "\n",
    "with open('Nuove Parole/sadness.txt', 'w',encoding=\"utf8\") as f:\n",
    "    for word in words:\n",
    "        f.write(\"%s\\n\" % word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trattamento dataset Surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lista di righe, ogni riga una tweet surprise\n",
    "file = open(\"Dataset/dataset_dt_surprise_60k.txt\", 'r',encoding=\"utf8\")\n",
    "surprise = []\n",
    "hashtagSurprise = []\n",
    "emoticonSurprise = []\n",
    "emojiSurprise = []\n",
    "\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "    riga = re.sub(r'URL', '',riga)\n",
    "    riga = re.sub(r'USERNAME', '',riga)\n",
    "\n",
    "     #array di hashtag singoli\n",
    "    hashTag = (re.findall('#[a-zA-Z0-9]+', riga))\n",
    "\n",
    "    #generiamo l'array di hashtag senza hashtag\n",
    "    for h in hashTag:\n",
    "        hashtagSurprise.append(h.replace(\"#\",\"\"))\n",
    "\n",
    "    #Rimuoviamo glin hashtag dalla riga\n",
    "    riga = riga.replace('#',\"\")\n",
    "\n",
    "    surprise.append(riga)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(surprise)):\n",
    "    #Controllo se l'emoticons c'è e in caso la rimuovo\n",
    "    for singleEmoticon in emoticonsList:\n",
    "        emoticonsFind = re.findall(re.escape(singleEmoticon),surprise[i])\n",
    "        if(len(emoticonsFind) != 0):\n",
    "            emoticonSurprise.extend(emoticonsFind)\n",
    "            surprise[i] = surprise[i].replace(singleEmoticon,\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(surprise)):\n",
    "    #preleviamo le emoji e le rimuoviamo dal testo\n",
    "    data = regex.findall(r'\\X', surprise[i])\n",
    "    for word in data:\n",
    "        if any(char in emoji.UNICODE_EMOJI['en'] for char in word):\n",
    "            emojiSurprise.append(word)\n",
    "            surprise[i] = surprise[i].replace(word, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processamento slang prima di punteggiatura e lowercase\n",
    "with open(\"Slangs/slangs_before.json\") as f:\n",
    "    slangsBefore = json.load(f)\n",
    "    for i in range(len(surprise)):\n",
    "        for word in surprise[i].split():\n",
    "            indexOfWord = surprise[i].find(word)\n",
    "            if slangsBefore.get(word) is None:\n",
    "                if slangsBefore.get(word.lower()) is None:\n",
    "                    if slangsBefore.get(word.upper()) is None:\n",
    "                        continue\n",
    "                    else:\n",
    "                        surprise[i] = surprise[i][:indexOfWord] + slangsBefore.get(word.upper()) + surprise[i][indexOfWord+len(word):]\n",
    "                else:\n",
    "                    surprise[i] = surprise[i][:indexOfWord] + slangsBefore.get(word.lower()) + surprise[i][indexOfWord+len(word):]\n",
    "            else:\n",
    "                surprise[i] = surprise[i][:indexOfWord] + slangsBefore.get(word) + surprise[i][indexOfWord+len(word):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizzazione di tutte le frasi\n",
    "surpriseTokenized = []\n",
    "for linea in surprise:\n",
    "    result = word_tokenize(linea)\n",
    "    surpriseTokenized.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Esecuzione della POS tagging per ogni riga della matrice\n",
    "for i in range(len(surpriseTokenized)):\n",
    "    surpriseTokenized[i] = nltk.pos_tag(surpriseTokenized[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trattamento punteggiatura\n",
    "for line in surpriseTokenized:\n",
    "    newLine = line\n",
    "    listToRemove = []\n",
    "    for i in range(len(line)):\n",
    "        if line[i][0] in puntuaction:\n",
    "            listToRemove.append(line[i])\n",
    "    for element in listToRemove:\n",
    "        newLine.remove(element)\n",
    "    line = newLine\n",
    "\n",
    "linePuntuaction = re.compile(\"[\" + re.escape(\"\".join(puntuaction)) + \"]\")\n",
    "for line in surpriseTokenized:\n",
    "    for i in range(len(line)):\n",
    "        tupla = [(re.sub(linePuntuaction, \"\", line[i][0])),line[i][1]]\n",
    "        line[i] = tupla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming\n",
    "#ps = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for line in surpriseTokenized:\n",
    "    for i in range(len(line)):\n",
    "        tupla = [lemmatizer.lemmatize(line[i][0]),line[i][1]]\n",
    "        line[i] = tupla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trattamnento Stop word dopo Stemming\n",
    "stop_words = set(stopwords.words('english'))\n",
    "surpriseTokStoped = []\n",
    "for line in surpriseTokenized:\n",
    "    filtred_word = []\n",
    "    for row in line:\n",
    "        if  row[0] not in stop_words:\n",
    "            filtred_word.append(row) \n",
    "    surpriseTokStoped.append(filtred_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surpriseListWord = []\n",
    "surpriseListTok = []\n",
    "for line in surpriseTokStoped:\n",
    "    for row in line:\n",
    "        if row[0] != '':\n",
    "            surpriseListWord.append(row[0])\n",
    "        surpriseListTok.append(row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lavorazione risorse lessicali\n",
    "resourceSurpriseNRC = []\n",
    "resourceSurpriseSentisense = []\n",
    "file = open(\"Risorse lessicali\\\\Surprise\\\\NRC_surprise.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        resourceSurpriseNRC.append(riga)\n",
    "\n",
    "file.close()\n",
    "\n",
    "file = open(\"Risorse lessicali\\Surprise\\sentisense_surprise.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        resourceSurpriseSentisense.append(riga)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Caricare le risorse lessicali \n",
    "SurpriseNRC = {\n",
    "    '_id': 'NRC_surprise',\n",
    "    'sentiment': 'surprise',  \n",
    "    'wordList': resourceSurpriseNRC\n",
    "}\n",
    "\n",
    "SurpriseSentisense = {\n",
    "    '_id': 'sentisense_surprise',\n",
    "    'sentiment': 'surprise',\n",
    "    'wordList': resourceSurpriseSentisense\n",
    "}\n",
    "\n",
    "resourcesCollection.insert_one(SurpriseNRC)\n",
    "resourcesCollection.insert_one(SurpriseSentisense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione database e collezioni\n",
    "# Inserimento singoli documenti contententi i tweet surprise\n",
    "wordsString = {}\n",
    "\n",
    "for i in range(len(surpriseListWord)):\n",
    "    wordsString = {\n",
    "        \"lemma\": surpriseListWord[i],\n",
    "        \"POS\": surpriseListTok[i]\n",
    "    }\n",
    "    surpriseTwit={\"word\":wordsString}\n",
    "    surpriseCollections.insert_one(surpriseTwit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inserimento collezione Twitter con hashtags, emoticons ed emoji \n",
    "surpriseInsert = {\n",
    "    \"sentiment\":\"surprise\",\n",
    "    \"hashtags\": hashtagSurprise,\n",
    "    \"emoticons\": emoticonSurprise,\n",
    "    \"emoji\": emojiSurprise\n",
    "}\n",
    "\n",
    "#inserimento\n",
    "x = collectionSurprise.insert_one(surpriseInsert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raggruppa i documenti di input in base all'espressione _id specificata e per ciascun raggruppamento distinto genera un documento\n",
    "pipeline = [\n",
    "    {\"$group\": {\"_id\": \"$word.lemma\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"word\": \"$_id\",\n",
    "      \"freq\": \"$count\"\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"freq\": -1\n",
    "    }},\n",
    "    {\"$out\": \"surprise\"}\n",
    "]\n",
    "\n",
    "#post processing per renderlo valido \n",
    "surpriseCollections.aggregate(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creazione Word Clouds per le parole\n",
    "data = {}\n",
    "for document in surpriseCollections.find().limit(50):\n",
    "    data[document['word']] = document['freq']\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2)\n",
    "wc.fit_words(data)\n",
    "wc.to_file('Word Clouds/Surprise/surpriseWords.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conteggio degli hashtags\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$hashtags\"},\n",
    "    {\"$group\": {\"_id\": \"$hashtags\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"word\": \"$_id\",\n",
    "      \"count\": 1\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"count\": -1\n",
    "    }}\n",
    "]\n",
    "\n",
    "hashtagsCount = collectionSurprise.aggregate(pipeline)\n",
    "\n",
    "data = {}\n",
    "for document in hashtagsCount:\n",
    "    data[document['word']] = document['count']\n",
    "\n",
    "collectionSurprise.update_one(\n",
    "  {\"sentiment\": \"surprise\"},\n",
    "  {\"$set\": {\"hashtags\": data}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creazione Word Clouds per gli hashtags\n",
    "\n",
    "hashtags = collectionSurprise.distinct(\"hashtags\")\n",
    "\n",
    "hashtagsLimited = {}\n",
    "for (word,count) in [x for x in hashtags[0].items()][:50]:\n",
    "    hashtagsLimited[word] = count\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2)\n",
    "wc.fit_words(hashtagsLimited)\n",
    "wc.to_file('Word Clouds/Surprise/surpriseHashtags.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conteggio delle emoji\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$emoji\"},\n",
    "    {\"$group\": {\"_id\": \"$emoji\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"emoji\": \"$_id\",\n",
    "      \"count\": 1\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"count\": -1\n",
    "    }}\n",
    "]\n",
    "\n",
    "emojiCount = collectionSurprise.aggregate(pipeline)\n",
    "\n",
    "data = {}\n",
    "for document in emojiCount:\n",
    "    data[document['emoji']] = document['count']\n",
    "\n",
    "collectionSurprise.update_one(\n",
    "  {\"sentiment\": \"surprise\"},\n",
    "  {\"$set\": {\"emoji\": data}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creazione Word Clouds per le emoji\n",
    "\n",
    "emojis = collectionSurprise.distinct(\"emoji\")\n",
    "\n",
    "emojisLimited = {}\n",
    "for (word,count) in [x for x in emojis[0].items()][:50]:\n",
    "    emojisLimited[word] = count\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2, font_path='Font Emoji\\Symbola.otf')\n",
    "wc.fit_words(emojisLimited)\n",
    "wc.to_file('Word Clouds/Surprise/surpriseEmojis.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conteggio degli emoticons\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$emoticons\"},\n",
    "    {\"$group\": {\"_id\": \"$emoticons\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"emoticons\": \"$_id\",\n",
    "      \"count\": 1\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"count\": -1\n",
    "    }}\n",
    "]\n",
    "\n",
    "emoticonsCount = collectionSurprise.aggregate(pipeline)\n",
    "\n",
    "data = {}\n",
    "for document in emoticonsCount:\n",
    "    data[document['emoticons']] = document['count']\n",
    "\n",
    "collectionSurprise.update_one(\n",
    "  {\"sentiment\": \"surprise\"},\n",
    "  {\"$set\": {\"emoticons\": data}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creazione Word Clouds per le emoticons\n",
    "\n",
    "emoticons = collectionSurprise.distinct(\"emoticons\")\n",
    "\n",
    "emoticonsLimited = {}\n",
    "for (word,count) in [x for x in emoticons[0].items()][:50]:\n",
    "    emoticonsLimited[word] = count\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2)\n",
    "wc.fit_words(emoticonsLimited)\n",
    "wc.to_file('Word Clouds/Surprise/surpriseEmoticons.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline di aggregazione per inserire la risorsa lessicale a cui appartiene la parola\n",
    "\n",
    "pipeline = [\n",
    "    {\"$lookup\": {\n",
    "        \"from\": \"Resources\",\n",
    "        \"localField\": \"word\",\n",
    "        \"foreignField\": \"wordList\",\n",
    "        \"pipeline\": [\n",
    "            {\"$match\": {\"$expr\": {\"$regexMatch\": {\"input\": \"$_id\", \"regex\": \"surprise\"}}}}\n",
    "        ],\n",
    "        \"as\": \"LexResources\"\n",
    "    }},\n",
    "    {\"$project\": {\n",
    "        \"resources.wordList\": 0,\n",
    "        \"resources.totNumberWords\": 0,\n",
    "        \"resources.sentiment\": 0,\n",
    "        \"LexResources.sentiment\": 0,\n",
    "        \"LexResources.totNumberWords\": 0,\n",
    "        \"LexResources.wordList\": 0\n",
    "    }},\n",
    "    {\"$out\": \"surprise\"}\n",
    "]\n",
    "\n",
    "surpriseCollections.aggregate(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline di aggregazione per calcolare le percentuali richieste\n",
    "\n",
    "pipelineCount = [\n",
    "    {\"$group\": {\"_id\": \"null\", \"count\": { \"$sum\": \"$freq\" }}},\n",
    "    {\"$project\": {\n",
    "        \"_id\": 0,\n",
    "        \"count\": \"$count\"\n",
    "    }}\n",
    "]\n",
    "\n",
    "results = surpriseCollections.aggregate(pipelineCount)\n",
    "\n",
    "countWords = 0\n",
    "for data in results:\n",
    "    countWords = data['count']\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$LexResources\"},\n",
    "    {\"$lookup\": {\n",
    "        \"from\": \"Resources\",\n",
    "        \"localField\": \"LexResources._id\",\n",
    "        \"foreignField\": \"_id\",\n",
    "        \"as\": \"RefResource\"\n",
    "    }},\n",
    "    {\"$group\": {\"_id\": { \"lex\": \"$LexResources._id\", \"RefResource\": \"$RefResource\" }, \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "        \"_id\": \"$_id.lex\",\n",
    "        \"wordList\": \"$_id.RefResource.wordList\",\n",
    "        \"commonWords\": \"$count\"\n",
    "    }},\n",
    "    {\"$unwind\": \"$wordList\"},\n",
    "    {\"$project\": {\n",
    "        \"_id\": 1,\n",
    "        \"commonWords\": 1,\n",
    "        \"totWordsLex\": {\"$size\": \"$wordList\"}\n",
    "    }},\n",
    "    {\"$addFields\": {\"totalWordsTwitter\": countWords}},\n",
    "    {\"$project\": {\n",
    "        \"perc_presence_lex_res\": { \"$round\": [{\"$divide\": [ \"$commonWords\", \"$totWordsLex\" ] }, 6] },\n",
    "        \"perc_presence_twitter\": { \"$round\": [{\"$divide\": [ \"$commonWords\", \"$totalWordsTwitter\" ]}, 6] }\n",
    "    }}\n",
    "]\n",
    "\n",
    "results = surpriseCollections.aggregate(pipeline)\n",
    "\n",
    "# Genero i dati per l'istogramma\n",
    "istogramSurprise_lexRes = []\n",
    "istogramSurprise_twitter = []\n",
    "\n",
    "for data in results:\n",
    "    print(data)\n",
    "    log =math.log(data['perc_presence_lex_res'],10)\n",
    "    istogramSurprise_lexRes.append([data['_id'], data['perc_presence_lex_res']])\n",
    "    log =math.log(data['perc_presence_twitter'],10)\n",
    "    istogramSurprise_twitter.append([data['_id'], data['perc_presence_twitter']])\n",
    " \n",
    "# Create the pandas DataFrame\n",
    "dfSurprise_lexRes = pd.DataFrame(istogramSurprise_lexRes, columns = ['Risorsa', 'Percentuale'])\n",
    "dfSurprise_twitter = pd.DataFrame(istogramSurprise_twitter,columns = ['Risorsa', 'Percentuale']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione istogramma per presenza parole nelle risorse lessicali\n",
    "\n",
    "dfSurprise_lexRes.plot.bar(x='Risorsa')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione istogramma per presenza parole nei twitter\n",
    "\n",
    "dfSurprise_twitter.plot.bar(x='Risorsa')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connessione, creazione cursore e database\n",
    "\n",
    "conn = psycopg2.connect(database = \"maadb\", user = \"postgres\", password = \"admin\", host = \"localhost\", port = \"5432\")\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione tabella risorse lessicali surprise e inserimento\n",
    "cur.execute('''CREATE TABLE RISORSESURPRISE\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    NOME             TEXT       NOT NULL,\n",
    "                    PAROLA           TEXT       NOT NULL\n",
    "                );\n",
    "            '''\n",
    ")\n",
    "\n",
    "nome = \"NRC_surprise\"\n",
    "for word in resourceSurpriseNRC:\n",
    "    cur.execute(\"INSERT INTO RISORSESURPRISE (NOME, PAROLA) VALUES(%s, %s)\",(nome,word))\n",
    "\n",
    "nome = \"sentisense_surprise\"\n",
    "for word in resourceSurpriseSentisense:\n",
    "    cur.execute(\"INSERT INTO RISORSESURPRISE (NOME, PAROLA) VALUES(%s, %s)\",(nome,word))\n",
    "\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione tabelle per le parole, emoji, emoticon e hashtag\n",
    "\n",
    "cur.execute('''CREATE TABLE PAROLESURPRISE\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    PAROLA           TEXT       NOT NULL,\n",
    "                    NRC_surprise      BOOL       NOT NULL DEFAULT FALSE,\n",
    "                    sentisense_surprise      BOOL       NOT NULL DEFAULT FALSE,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ")   \n",
    "\n",
    "cur.execute('''CREATE TABLE EMOJISURPRISE\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    EMOJI       TEXT       NOT NULL,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ") \n",
    "\n",
    "cur.execute('''CREATE TABLE EMOTICONSSURPRISE\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    EMOTICONS       TEXT       NOT NULL,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ") \n",
    "\n",
    "cur.execute('''CREATE TABLE HASHTAGSURPRISE\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    HASHTAG     TEXT       NOT NULL,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ") \n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inserimento hashtag, emoticon ed emoji\n",
    "\n",
    "for hashTag in hashtagSurprise:\n",
    "    cur.execute(\"INSERT INTO HASHTAGSURPRISE (HASHTAG,FREQUENCY) VALUES(%s,%s)\",(hashTag,0))\n",
    "\n",
    "for emoticons in emoticonSurprise:\n",
    "    cur.execute(\"INSERT INTO EMOTICONSSURPRISE (EMOTICONS,FREQUENCY) VALUES(%s,%s)\",(emoticons,0))\n",
    "\n",
    "for emojis in emojiSurprise:\n",
    "    cur.execute(\"INSERT INTO EMOJISURPRISE (EMOJI,FREQUENCY) VALUES(%s,%s)\",(emojis,0))\n",
    "\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inserimento parole in db\n",
    "for word in surpriseListWord:\n",
    "    cur.execute(\"INSERT INTO PAROLESURPRISE (PAROLA,NRC_surprise,sentisense_surprise,FREQUENCY) VALUES(%s,%s,%s,%s)\",(word,False,False,0))\n",
    "\n",
    "conn.commit() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze hashtag\n",
    "cur.execute(\"update hashtagsurprise as ha1 set frequency = (select count(*) from hashtagsurprise ha2 where ha1.hashtag = ha2.hashtag);\")\n",
    "cur.execute(\"delete from hashtagsurprise as ha1 where exists (Select 1 From hashtagsurprise as ha2 where ha1.hashtag = ha2.hashtag and ha1.frequency = ha2.frequency and ha2.ctid > ha1.ctid);\")\n",
    "\n",
    "conn.commit()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze emoji\n",
    "cur.execute(\"update EMOJISURPRISE as ha1 set frequency = (select count(*) from EMOJISURPRISE ha2 where ha1.emoji = ha2.emoji);\")\n",
    "cur.execute(\"delete from EMOJISURPRISE as ha1 where exists (Select 1 From EMOJISURPRISE as ha2 where ha1.emoji = ha2.emoji and ha1.frequency = ha2.frequency and ha2.ctid > ha1.ctid);\")\n",
    "\n",
    "conn.commit()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze emoticons\n",
    "cur.execute(\"update EMOTICONSSURPRISE as ha1 set frequency = (select count(*) from EMOTICONSSURPRISE ha2 where ha1.emoticons = ha2.emoticons);\")\n",
    "cur.execute(\"delete from EMOTICONSSURPRISE as ha1 where exists (Select 1 From EMOTICONSSURPRISE as ha2 where ha1.emoticons = ha2.emoticons and ha1.frequency = ha2.frequency and ha2.ctid > ha1.ctid);\")\n",
    "\n",
    "conn.commit()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze parole\n",
    "cur.execute(\"select count(*),parola from parolesurprise group by parola\")\n",
    "result = []\n",
    "for word in cur.fetchall():\n",
    "    result.append(word) \n",
    "\n",
    "cur.execute(\"truncate table parolesurprise;\")\n",
    "cur.execute(\"ALTER SEQUENCE parolesurprise_id_seq RESTART WITH 1;\")\n",
    "\n",
    "for couple in result:\n",
    "    cur.execute(\"INSERT INTO PAROLESURPRISE (PAROLA,NRC_surprise,sentisense_surprise,FREQUENCY) VALUES(%s,%s,%s,%s)\",(couple[1],False,False,couple[0]))\n",
    "\n",
    "conn.commit()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggiornamento presenza parola nelle risorse lessicali\n",
    "cur.execute(\"UPDATE parolesurprise SET NRC_SURPRISE = TRUE WHERE parola IN (select p.parola from parolesurprise as p LEFT OUTER JOIN risorsesurprise as r ON p.parola = r.parola WHERE r.nome = 'NRC_surprise')\")\n",
    "cur.execute(\"UPDATE parolesurprise SET sentisense_surprise = TRUE WHERE parola IN (select p.parola from parolesurprise as p LEFT OUTER JOIN risorsesurprise as r ON p.parola = r.parola WHERE r.nome = 'sentisense_surprise')\")\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvataggio in un txt delle parole non presenti in nessuna risorsa lessicale\n",
    "\n",
    "wordsMongo = surpriseCollections.find({\"LexResources\": []}, {\"_id\": 0, \"freq\": 0, \"LexResources\": 0})\n",
    "words = []\n",
    "\n",
    "for word in wordsMongo:\n",
    "    words.append(word['word'])\n",
    "\n",
    "with open('Nuove Parole/surprise.txt', 'w',encoding=\"utf8\") as f:\n",
    "    for word in words:\n",
    "        f.write(\"%s\\n\" % word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trattamento dataset Trust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lista di righe, ogni riga una tweet trust\n",
    "file = open(\"Dataset/dataset_dt_trust_60k.txt\", 'r',encoding=\"utf8\")\n",
    "trust = []\n",
    "hashtagTrust = []\n",
    "emoticonTrust = []\n",
    "emojiTrust = []\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "    riga = re.sub(r'URL', '',riga)\n",
    "    riga = re.sub(r'USERNAME', '',riga)\n",
    "    \n",
    "    #array di hashtag singoli\n",
    "    hashTag = (re.findall('#[a-zA-Z0-9]+', riga))\n",
    "    \n",
    "    #generiamo l'array di hashtag senza hashtag\n",
    "    for h in hashTag:\n",
    "        hashtagTrust.append(h.replace(\"#\",\"\"))\n",
    "\n",
    "    #Rimuoviamo glin hashtag dalla riga\n",
    "    riga = riga.replace('#',\"\")\n",
    "\n",
    "    trust.append(riga)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(trust)):\n",
    "    #Controllo se l'emoticons c'è e in caso la rimuovo\n",
    "    for singleEmoticon in emoticonsList:\n",
    "        emoticonsFind = re.findall(re.escape(singleEmoticon),trust[i])\n",
    "        if(len(emoticonsFind) != 0):\n",
    "            emoticonTrust.extend(emoticonsFind)\n",
    "            trust[i] = trust[i].replace(singleEmoticon,\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(trust)):\n",
    "    #preleviamo le emoji e le rimuoviamo dal testo\n",
    "    data = regex.findall(r'\\X', trust[i])\n",
    "    for word in data:\n",
    "        if any(char in emoji.UNICODE_EMOJI['en'] for char in word):\n",
    "            emojiTrust.append(word)\n",
    "            trust[i] = trust[i].replace(word, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processamento slang prima di punteggiatura e lowercase\n",
    "with open(\"Slangs/slangs_before.json\") as f:\n",
    "    slangsBefore = json.load(f)\n",
    "    for i in range(len(trust)):\n",
    "        for word in trust[i].split():\n",
    "            indexOfWord = trust[i].find(word)\n",
    "            if slangsBefore.get(word) is None:\n",
    "                if slangsBefore.get(word.lower()) is None:\n",
    "                    if slangsBefore.get(word.upper()) is None:\n",
    "                        continue\n",
    "                    else:\n",
    "                        trust[i] = trust[i][:indexOfWord] + slangsBefore.get(word.upper()) + trust[i][indexOfWord+len(word):]\n",
    "                else:\n",
    "                    trust[i] = trust[i][:indexOfWord] + slangsBefore.get(word.lower()) + trust[i][indexOfWord+len(word):]\n",
    "            else:\n",
    "                trust[i] = trust[i][:indexOfWord] + slangsBefore.get(word) + trust[i][indexOfWord+len(word):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizzazione di tutte le frasi\n",
    "trustTokenized = []\n",
    "for linea in trust:\n",
    "    result = word_tokenize(linea)\n",
    "    trustTokenized.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Esecuzione della POS tagging per ogni riga della matrice\n",
    "for i in range(len(trustTokenized)):\n",
    "    trustTokenized[i] = nltk.pos_tag(trustTokenized[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trattamento punteggiatura\n",
    "for line in trustTokenized:\n",
    "    newLine = line\n",
    "    listToRemove = []\n",
    "    for i in range(len(line)):\n",
    "        if line[i][0] in puntuaction:\n",
    "            listToRemove.append(line[i])\n",
    "    for element in listToRemove:\n",
    "        newLine.remove(element)\n",
    "    line = newLine\n",
    "\n",
    "linePuntuaction = re.compile(\"[\" + re.escape(\"\".join(puntuaction)) + \"]\")\n",
    "for line in trustTokenized:\n",
    "    for i in range(len(line)):\n",
    "        tupla = [(re.sub(linePuntuaction, \"\", line[i][0])),line[i][1]]\n",
    "        line[i] = tupla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming\n",
    "#ps = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for line in trustTokenized:\n",
    "    for i in range(len(line)):\n",
    "        tupla = [lemmatizer.lemmatize(line[i][0]),line[i][1]]\n",
    "        line[i] = tupla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trattamnento Stop word dopo Stemming\n",
    "stop_words = set(stopwords.words('english'))\n",
    "trustTokStoped = []\n",
    "for line in trustTokenized:\n",
    "    filtred_word = []\n",
    "    for row in line:\n",
    "        if  row[0] not in stop_words:\n",
    "            filtred_word.append(row) \n",
    "    trustTokStoped.append(filtred_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trustListWord = []\n",
    "trustListTok = []\n",
    "for line in trustTokStoped:\n",
    "    for row in line:\n",
    "        if row[0] != '':\n",
    "            trustListWord.append(row[0])\n",
    "        trustListTok.append(row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lavorazione risorse lessicali\n",
    "resourceTrustNRC = []\n",
    "file = open(\"Risorse lessicali\\\\Trust\\\\NRC_trust.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        resourceTrustNRC.append(riga)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Caricare le risorse lessicali \n",
    "TrustNRC = {\n",
    "    '_id': 'NRC_trust',\n",
    "    'sentiment': 'trust',  \n",
    "    'wordList': resourceTrustNRC\n",
    "}\n",
    "\n",
    "resourcesCollection.insert_one(TrustNRC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione database e collezioni\n",
    "# Inserimento singoli documenti contententi i tweet trust\n",
    "wordsString = {}\n",
    "\n",
    "for i in range(len(trustListWord)):\n",
    "    wordsString = {\n",
    "        \"lemma\": trustListWord[i],\n",
    "        \"POS\": trustListTok[i]\n",
    "    }\n",
    "    trustTwit={\"word\":wordsString}\n",
    "    trustCollections.insert_one(trustTwit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inserimento collezione Twitter con hashtags, emoticons ed emoji \n",
    "trustInsert = {\n",
    "    \"sentiment\":\"trust\",\n",
    "    \"hashtags\": hashtagTrust,\n",
    "    \"emoticons\": emoticonTrust,\n",
    "    \"emoji\": emojiTrust\n",
    "}\n",
    "\n",
    "#inserimento\n",
    "x = collectionTrust.insert_one(trustInsert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raggruppa i documenti di input in base all'espressione _id specificata e per ciascun raggruppamento distinto genera un documento\n",
    "pipeline = [\n",
    "    {\"$group\": {\"_id\": \"$word.lemma\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"word\": \"$_id\",\n",
    "      \"freq\": \"$count\"\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"freq\": -1\n",
    "    }},\n",
    "    {\"$out\": \"trust\"}\n",
    "]\n",
    "\n",
    "#post processing per renderlo valido \n",
    "trustCollections.aggregate(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creazione Word Clouds per le parole\n",
    "data = {}\n",
    "for document in trustCollections.find().limit(50):\n",
    "    data[document['word']] = document['freq']\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2)\n",
    "wc.fit_words(data)\n",
    "wc.to_file('Word Clouds/Trust/trustWords.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conteggio degli hashtags\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$hashtags\"},\n",
    "    {\"$group\": {\"_id\": \"$hashtags\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"word\": \"$_id\",\n",
    "      \"count\": 1\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"count\": -1\n",
    "    }}\n",
    "]\n",
    "\n",
    "hashtagsCount = collectionTrust.aggregate(pipeline)\n",
    "\n",
    "data = {}\n",
    "for document in hashtagsCount:\n",
    "    data[document['word']] = document['count']\n",
    "\n",
    "collectionTrust.update_one(\n",
    "  {\"sentiment\": \"trust\"},\n",
    "  {\"$set\": {\"hashtags\": data}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creazione Word Clouds per gli hashtags\n",
    "\n",
    "hashtags = collectionTrust.distinct(\"hashtags\")\n",
    "\n",
    "hashtagsLimited = {}\n",
    "for (word,count) in [x for x in hashtags[0].items()][:50]:\n",
    "    hashtagsLimited[word] = count\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2)\n",
    "wc.fit_words(hashtagsLimited)\n",
    "wc.to_file('Word Clouds/Trust/trustHashtags.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conteggio delle emoji\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$emoji\"},\n",
    "    {\"$group\": {\"_id\": \"$emoji\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"emoji\": \"$_id\",\n",
    "      \"count\": 1\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"count\": -1\n",
    "    }}\n",
    "]\n",
    "\n",
    "emojiCount = collectionTrust.aggregate(pipeline)\n",
    "\n",
    "data = {}\n",
    "for document in emojiCount:\n",
    "    data[document['emoji']] = document['count']\n",
    "\n",
    "collectionTrust.update_one(\n",
    "  {\"sentiment\": \"trust\"},\n",
    "  {\"$set\": {\"emoji\": data}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creazione Word Clouds per le emoji\n",
    "\n",
    "emojis = collectionTrust.distinct(\"emoji\")\n",
    "\n",
    "emojisLimited = {}\n",
    "for (word,count) in [x for x in emojis[0].items()][:50]:\n",
    "    emojisLimited[word] = count\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2, font_path='Font Emoji\\Symbola.otf')\n",
    "wc.fit_words(emojisLimited)\n",
    "wc.to_file('Word Clouds/Trust/trustEmojis.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conteggio degli emoticons\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$emoticons\"},\n",
    "    {\"$group\": {\"_id\": \"$emoticons\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"emoticons\": \"$_id\",\n",
    "      \"count\": 1\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"count\": -1\n",
    "    }}\n",
    "]\n",
    "\n",
    "emoticonsCount = collectionTrust.aggregate(pipeline)\n",
    "\n",
    "data = {}\n",
    "for document in emoticonsCount:\n",
    "    data[document['emoticons']] = document['count']\n",
    "\n",
    "collectionTrust.update_one(\n",
    "  {\"sentiment\": \"trust\"},\n",
    "  {\"$set\": {\"emoticons\": data}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creazione Word Clouds per le emoticons\n",
    "\n",
    "emoticons = collectionTrust.distinct(\"emoticons\")\n",
    "\n",
    "emoticonsLimited = {}\n",
    "for (word,count) in [x for x in emoticons[0].items()][:50]:\n",
    "    emoticonsLimited[word] = count\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2)\n",
    "wc.fit_words(emoticonsLimited)\n",
    "wc.to_file('Word Clouds/Trust/trustEmoticons.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline di aggregazione per inserire la risorsa lessicale a cui appartiene la parola\n",
    "\n",
    "pipeline = [\n",
    "    {\"$lookup\": {\n",
    "        \"from\": \"Resources\",\n",
    "        \"localField\": \"word\",\n",
    "        \"foreignField\": \"wordList\",\n",
    "        \"pipeline\": [\n",
    "            {\"$match\": {\"$expr\": {\"$regexMatch\": {\"input\": \"$_id\", \"regex\": \"trust\"}}}}\n",
    "        ],\n",
    "        \"as\": \"LexResources\"\n",
    "    }},\n",
    "    {\"$project\": {\n",
    "        \"resources.wordList\": 0,\n",
    "        \"resources.totNumberWords\": 0,\n",
    "        \"resources.sentiment\": 0,\n",
    "        \"LexResources.sentiment\": 0,\n",
    "        \"LexResources.totNumberWords\": 0,\n",
    "        \"LexResources.wordList\": 0\n",
    "    }},\n",
    "    {\"$out\": \"trust\"}\n",
    "]\n",
    "\n",
    "trustCollections.aggregate(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline di aggregazione per calcolare le percentuali richieste\n",
    "\n",
    "pipelineCount = [\n",
    "    {\"$group\": {\"_id\": \"null\", \"count\": { \"$sum\": \"$freq\" }}},\n",
    "    {\"$project\": {\n",
    "        \"_id\": 0,\n",
    "        \"count\": \"$count\"\n",
    "    }}\n",
    "]\n",
    "\n",
    "results = trustCollections.aggregate(pipelineCount)\n",
    "\n",
    "countWords = 0\n",
    "for data in results:\n",
    "    countWords = data['count']\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$LexResources\"},\n",
    "    {\"$lookup\": {\n",
    "        \"from\": \"Resources\",\n",
    "        \"localField\": \"LexResources._id\",\n",
    "        \"foreignField\": \"_id\",\n",
    "        \"as\": \"RefResource\"\n",
    "    }},\n",
    "    {\"$group\": {\"_id\": { \"lex\": \"$LexResources._id\", \"RefResource\": \"$RefResource\" }, \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "        \"_id\": \"$_id.lex\",\n",
    "        \"wordList\": \"$_id.RefResource.wordList\",\n",
    "        \"commonWords\": \"$count\"\n",
    "    }},\n",
    "    {\"$unwind\": \"$wordList\"},\n",
    "    {\"$project\": {\n",
    "        \"_id\": 1,\n",
    "        \"commonWords\": 1,\n",
    "        \"totWordsLex\": {\"$size\": \"$wordList\"}\n",
    "    }},\n",
    "    {\"$addFields\": {\"totalWordsTwitter\": countWords}},\n",
    "    {\"$project\": {\n",
    "        \"perc_presence_lex_res\": { \"$round\": [{\"$divide\": [ \"$commonWords\", \"$totWordsLex\" ] }, 6] },\n",
    "        \"perc_presence_twitter\": { \"$round\": [{\"$divide\": [ \"$commonWords\", \"$totalWordsTwitter\" ]}, 6] }\n",
    "    }}\n",
    "]\n",
    "\n",
    "results = trustCollections.aggregate(pipeline)\n",
    "\n",
    "# Genero i dati per l'istogramma\n",
    "istogramTrust_lexRes = []\n",
    "istogramTrust_twitter = []\n",
    "\n",
    "for data in results:\n",
    "    print(data)\n",
    "    log =math.log(data['perc_presence_lex_res'],10)\n",
    "    istogramTrust_lexRes.append([data['_id'], data['perc_presence_lex_res']])\n",
    "    log =math.log(data['perc_presence_twitter'],10)\n",
    "    istogramTrust_twitter.append([data['_id'], data['perc_presence_twitter']])\n",
    " \n",
    "# Create the pandas DataFrame\n",
    "dfTrust_lexRes = pd.DataFrame(istogramTrust_lexRes, columns = ['Risorsa', 'Percentuale'])\n",
    "dfTrust_twitter = pd.DataFrame(istogramTrust_twitter,columns = ['Risorsa', 'Percentuale'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione istogramma per presenza parole nelle risorse lessicali\n",
    "\n",
    "dfTrust_lexRes.plot.bar(x='Risorsa')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione istogramma per presenza parole nei twitter\n",
    "\n",
    "dfTrust_twitter.plot.bar(x='Risorsa')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connessione, creazione cursore e database\n",
    "\n",
    "conn = psycopg2.connect(database = \"maadb\", user = \"postgres\", password = \"admin\", host = \"localhost\", port = \"5432\")\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione tabella risorse lessicali trust e inserimento\n",
    "cur.execute('''CREATE TABLE RISORSETRUST\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    NOME             TEXT       NOT NULL,\n",
    "                    PAROLA           TEXT       NOT NULL\n",
    "                );\n",
    "            '''\n",
    ")\n",
    "\n",
    "nome = \"NRC_trust\"\n",
    "for word in resourceTrustNRC:\n",
    "    cur.execute(\"INSERT INTO RISORSETRUST (NOME, PAROLA) VALUES(%s, %s)\",(nome,word))\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione tabelle per le parole, emoji, emoticon e hashtag\n",
    "\n",
    "cur.execute('''CREATE TABLE PAROLETRUST\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    PAROLA           TEXT       NOT NULL,\n",
    "                    NRC_trust      BOOL       NOT NULL DEFAULT FALSE,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ")   \n",
    "\n",
    "cur.execute('''CREATE TABLE EMOJITRUST\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    EMOJI       TEXT       NOT NULL,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ") \n",
    "\n",
    "cur.execute('''CREATE TABLE EMOTICONSTRUST\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    EMOTICONS       TEXT       NOT NULL,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ") \n",
    "\n",
    "cur.execute('''CREATE TABLE HASHTAGTRUST\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    HASHTAG     TEXT       NOT NULL,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ") \n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inserimento hashtag, emoticon ed emoji\n",
    "\n",
    "for hashTag in hashtagTrust:\n",
    "    cur.execute(\"INSERT INTO HASHTAGTRUST (HASHTAG,FREQUENCY) VALUES(%s,%s)\",(hashTag,0))\n",
    "\n",
    "for emoticons in emoticonTrust:\n",
    "    cur.execute(\"INSERT INTO EMOTICONSTRUST (EMOTICONS,FREQUENCY) VALUES(%s,%s)\",(emoticons,0))\n",
    "\n",
    "for emojis in emojiTrust:\n",
    "    cur.execute(\"INSERT INTO EMOJITRUST (EMOJI,FREQUENCY) VALUES(%s,%s)\",(emojis,0))\n",
    "\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inserimento parole in db\n",
    "for word in trustListWord:\n",
    "    cur.execute(\"INSERT INTO PAROLETRUST (PAROLA,NRC_trust,FREQUENCY) VALUES(%s,%s,%s)\",(word,False,0))\n",
    "\n",
    "conn.commit()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze hashtag\n",
    "cur.execute(\"update HASHTAGTRUST as ha1 set frequency = (select count(*) from HASHTAGTRUST ha2 where ha1.hashtag = ha2.hashtag);\")\n",
    "cur.execute(\"delete from HASHTAGTRUST as ha1 where exists (Select 1 From HASHTAGTRUST as ha2 where ha1.hashtag = ha2.hashtag and ha1.frequency = ha2.frequency and ha2.ctid > ha1.ctid);\")\n",
    "\n",
    "conn.commit()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze emoji\n",
    "cur.execute(\"update EMOJITRUST as ha1 set frequency = (select count(*) from EMOJITRUST ha2 where ha1.emoji = ha2.emoji);\")\n",
    "cur.execute(\"delete from EMOJITRUST as ha1 where exists (Select 1 From EMOJITRUST as ha2 where ha1.emoji = ha2.emoji and ha1.frequency = ha2.frequency and ha2.ctid > ha1.ctid);\")\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze emoticons\n",
    "cur.execute(\"update EMOTICONSTRUST as ha1 set frequency = (select count(*) from EMOTICONSTRUST ha2 where ha1.emoticons = ha2.emoticons);\")\n",
    "cur.execute(\"delete from EMOTICONSTRUST as ha1 where exists (Select 1 From EMOTICONSTRUST as ha2 where ha1.emoticons = ha2.emoticons and ha1.frequency = ha2.frequency and ha2.ctid > ha1.ctid);\")\n",
    "\n",
    "conn.commit()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze parole\n",
    "cur.execute(\"select count(*),parola from PAROLETRUST group by parola\")\n",
    "result = []\n",
    "for word in cur.fetchall():\n",
    "    result.append(word) \n",
    "\n",
    "cur.execute(\"truncate table PAROLETRUST;\")\n",
    "cur.execute(\"ALTER SEQUENCE paroletrust_id_seq RESTART WITH 1;\")\n",
    "\n",
    "for couple in result:\n",
    "    cur.execute(\"INSERT INTO PAROLETRUST (PAROLA,NRC_trust,FREQUENCY) VALUES(%s,%s,%s)\",(couple[1],False,couple[0]))\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggiornamento presenza parola nelle risorse lessicali\n",
    "cur.execute(\"UPDATE PAROLETRUST SET NRC_trust = TRUE WHERE parola IN (select p.parola from PAROLETRUST as p LEFT OUTER JOIN RISORSETRUST as r ON p.parola = r.parola WHERE r.nome = 'NRC_trust')\")\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvataggio in un txt delle parole non presenti in nessuna risorsa lessicale\n",
    "\n",
    "wordsMongo = trustCollections.find({\"LexResources\": []}, {\"_id\": 0, \"freq\": 0, \"LexResources\": 0})\n",
    "words = []\n",
    "\n",
    "for word in wordsMongo:\n",
    "    words.append(word['word'])\n",
    "\n",
    "with open('Nuove Parole/trust.txt', 'w',encoding=\"utf8\") as f:\n",
    "    for word in words:\n",
    "        f.write(\"%s\\n\" % word)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "727cd1b8d9e0a8fe67ccb60934071f3fe824c12f3f18648f9575d2adb7fe198a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
