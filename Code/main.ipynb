{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\gabri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\gabri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\gabri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\gabri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\gabri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Import per il resto delle celle\n",
    "import re\n",
    "import emoji\n",
    "import regex\n",
    "import nltk\n",
    "import json\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from collections import Counter\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "import pymongo\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#from wordcloud import WordCloud\n",
    "\n",
    "emoticons = [':‑)',':)',':-]',':]',':->',':>','8-)','8)',':-}',':}',':o)',':c)',':^)','=]','=)',':‑D',':D','8‑D','8D','=D','=3','B^D','c:',\n",
    "'C:','x‑D','xD','X‑D','XD',':-))',':‑(',':(',':‑c',':c',':‑<',':<',':‑[',':[',':-||','>:[',':{',':@',':('',;(',\":'‑(\",\":'(\",':=(',\":'‑)\",\":')\",\n",
    "':\"D',\"D‑':\",'D:<','D:','D8','D;','D=','DX',':‑O',':O',':‑o',':o',':-0','8‑0','>:O','=O','=o','=0',':-3',':3','=3','x3','X3','>:3',':-*',\n",
    "':*',':×',';‑)',';)','*-)','*)',';‑]',';]',';^)',';>',':‑,',';D',';3',':‑P',':P','X‑P','XP','x‑p','xp',':‑p',':p',':‑Þ',':Þ',':‑þ',':þ',':‑b',':b',\n",
    "'d:','=p','>:P',':-/',':/',':‑.',\">:/\",'=/',':L','=L',':S',':‑|',':|',':$','://)','://3',':‑X',':X',':‑#',':#',':‑&',':&','O:‑)','O:)','0:‑3'\n",
    ",'0:3','0:‑)','0:)','0;^)','>:‑)','>:)','}:‑)','}:)','3:‑)','3:)','>;‑)','>;)','>:3',';3','|;‑)','|‑O','B-)',':‑J','#‑)','%‑)','%)',':‑###..'\n",
    ",':###..','<:‑|',\"',:-|\",\"',:-l\",':E','8-X','8=X','x-3','x=3','~:>','v.v','._.','._.;','QQ  ','qq','Qq','X_X','x_x','+_+','X_x','x_X'\n",
    "'<_<','>_>','<.<','>.>','O_O','o_o','O-O','o‑o','O_o','o_O','>.<','>_<','^5','o/\\o','>_>^ ^<_<','V.v.V','V=(° °)=V','(^^^)','(::[]::)',\n",
    "'(o)(o)','( • )( • )','[̲̅$̲̅(̲̅1̲̅)̲̅$̲̅]','[̲̅$̲̅(̲̅5̲̅)̲̅$̲̅]','[̲̅$̲̅(̲̅10)̲̅$̲̅]','[̲̅$̲̅(̲̅100)̲̅$̲̅]','( ͡° ͜ʖ ͡°)','ヽ༼ຈل͜ຈ༽ﾉ','(๑ˇεˇ๑)','(◕‿◕✿)','( ༎ຶ ۝ ༎ຶ )','(=ʘᆽʘ=)∫','ʕ •ᴥ•ʔ',\n",
    "'(>_<)','(>_<)>','(>w<)',\"(';')\",'(^_^;)','(-_-;)','(~_~;)','(・.・;)','(・_・;)','^^;','(・・;)','^_^;','(#^.^#)'\n",
    ",'(^^;)','(⁄ ⁄•⁄ω⁄•⁄ ⁄)','(-.-)y-°°°','(^.^)y-.o○','(-_-)zzz','(^_-)-☆','(^_-)','((+_+))','(+o+)','(°°)','(°-°) (°.°)',\n",
    "'(°_°)','(°_°>)','(°レ°)','(o|o)','<(｀^´)>','^_^','(°o°)','(^_^)/','(^O^)／','(^o^)／','(^^)/','(≧∇≦)/','(/◕ヮ◕)/','(^o^)丿','∩(·ω·)∩',\n",
    "'(·ω·)','^ω^','_(._.)_','_(_^_)_','<(_ _)>','<m(__)m>','m(__)m','m(_ _)m','(凸ಠ益ಠ)凸',\"('_')\",'(/_;)','(T_T)','(;_;)','(;_;','(;_:)','(;O;)'\n",
    ",'(:_;)','(ToT)','(Ｔ▽Ｔ)',';_;',';-;',';n;',';;','Q.Q','T.T','TnT','QQ','Q_Q',\n",
    "'(ー_ー)!!','(-.-)','(-_-)','(一一)','(；一_一)','(=_=)','(=^・^=)','(=^・・^=)','=^_^=','(..)','(._.)',\n",
    "'.o○','○o.','_旦~~','( ^^)','_U~~','( ^^)','☆ミ','☆彡','>°)))彡','(Q))',\n",
    "'><ヨヨ (°))<<','>°))))彡','<°)))彡','>°))彡','<+','))><<','<*))','>=<','<コ:彡',\n",
    "'Ｃ:.ミ','~>°)～～～','～°·_·°～','(°°)～','●～*','￣|○',':3ミ','^m^','(・・?','(?_?)','>^_^<','<^!^>',\n",
    "'^/^','（*^_^*','§^.^§','(^<^)','(^.^)','(^ム^)','(^·^)','(^.^)','(^_^.)','(^_^)','(^^)','(^J^)',\n",
    "'(*^.^*)','^_^','(#^.^#)','（^—^）','(^^)/~~~','(^_^)/~','(;_;)/~~~','(^.^)/~~~',\n",
    "'(-_-)/~~~','($··)/~~~','(@^^)/~~~','(T_T)/~~~','(V)o￥o(V)','＼(~o~)／','＼(^o^)／','＼(-o-)／','ヽ(^。^)ノ',\n",
    "'ヽ(^o^)丿','(*^0^*)','(*_*)','(*_*;','(+_+) (@_@)','(@_@。','(＠_＠;)',\n",
    " '＼(◎o◎)／！','(*^^)v','(^^)v','(^_^)v','（’-’*) (＾ｖ＾)','(＾▽＾)','(・∀・)','(´∀`)','(⌒▽⌒）',\n",
    " '＼(^o^)／','\\(^o^)/','(~o~)','(~_~)','(^^ゞ','ˊ＿>ˋ','(p_-)', '(-_q)','((d[-_-]b))','(-\"-)','(ーー゛)',\n",
    " '(^_^メ)', '(-_-メ)','(~_~メ)','(－－〆)','(・へ・)','(｀´)'\n",
    " , '<`～´>','<`ヘ´>','(ーー;)','(^0_0^)','(＾ｖ＾)', '(＾ｕ＾)','(＾◇＾)','( ^)o(^ )','(^O^)','(^o^)','(^○^)',')^o^(',\n",
    " '(*^▽^*)','(￣ー￣)','(￣□￣;)', '°o°','°O°',':O','o_O','o_0', 'o.O','(o.o)','oO','(°◇°)','（ ﾟ Дﾟ)','(*￣m￣)',\n",
    " 'ヽ(´ー｀)┌','¯\\_(ツ)_/¯','¯\\(°_o)/¯','(´･ω･`)', '(‘A`)','(づ￣ ³￣)づ','(*^3^)/~☆','(︶｡︶✽)','(-_-) zzz'\n",
    " , 'uwu','UwU','OWO','OwO','～°·_·°～', '(°°)～',':3ミ','(´･ω･`)','(`･ω･´)','(｀-´)>', '（　´_ゝ`）'] \n",
    "\n",
    "puntuaction = ['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '.',\n",
    "           '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', \n",
    "           '`', '{', '|', '}', '~', '»', '«', '“', '”', \"-\"]\n",
    "\n",
    "\n",
    "mongoClient = pymongo.MongoClient(\"mongodb://admin:admin@localhost:27017/\")\n",
    "mydb = mongoClient[\"maadb\"]\n",
    "collectionTwitter = mydb[\"Twitter\"] #Questa é per la collezione dei emoji emoticon hashtag per ogni emozione\n",
    "angerCollections = mydb[\"anger\"] #Collezioni per solo i twitt di anger\n",
    "anticipationCollections = mydb[\"anticipation\"]\n",
    "disgustCollections = mydb[\"disgust\"]\n",
    "fearCollections = mydb[\"fear\"]\n",
    "joyCollections = mydb[\"joy\"]\n",
    "sadnessCollections = mydb[\"sadness\"]\n",
    "surpriseCollections = mydb[\"surprise\"]\n",
    "trustCollections = mydb[\"trust\"]\n",
    "resourcesCollection = mydb[\"Resources\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trattamento dataset Anger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lista di righe, ogni riga una tweet anger\n",
    "file = open(\"Dataset/dataset_dt_anger_60k.txt\", 'r',encoding=\"utf8\")\n",
    "anger = []\n",
    "hashtagAnger = []\n",
    "emoticonAnger = []\n",
    "emojiAnger = []\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    #Eliminazione URL e USERNAME\n",
    "    riga = re.sub(r'URL', '',riga)\n",
    "    riga = re.sub(r'USERNAME', '',riga)\n",
    "\n",
    "    #array di hashtag singoli\n",
    "    hashTag = (re.findall('#[a-zA-Z0-9]+', riga))\n",
    "\n",
    "     #generiamo l'array di hashtag senza hashtag\n",
    "    for h in hashTag:\n",
    "        hashtagAnger.append(h.replace(\"#\",\"\"))\n",
    "\n",
    "    #Rimuoviamo glin hashtag dalla riga\n",
    "    riga = riga.replace('#',\"\")\n",
    "\n",
    "    #Salvataggio frase\n",
    "    anger.append(riga)\n",
    "\n",
    "\n",
    "file.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(anger)):\n",
    "    #Controllo se l'emoticons c'è e in caso la rimuovo\n",
    "    for singleEmoticon in emoticons:\n",
    "        emoticonsFind = re.findall(re.escape(singleEmoticon),anger[i])\n",
    "        if(len(emoticonsFind) != 0):\n",
    "            emoticonAnger.extend(emoticonsFind)\n",
    "            anger[i] = anger[i].replace(singleEmoticon,\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(anger)):\n",
    "    #preleviamo le emoji e le rimuoviamo dal testo\n",
    "    data = regex.findall(r'\\X', anger[i])\n",
    "    for word in data:\n",
    "        if any(char in emoji.UNICODE_EMOJI['en'] for char in word):\n",
    "            emojiAnger.append(word)\n",
    "            anger[i] = anger[i].replace(word, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processamento slang \n",
    "with open(\"Slangs/slangs_before.json\") as f:\n",
    "    slangsBefore = json.load(f)\n",
    "    for i in range(len(anger)):\n",
    "        for word in anger[i].split():\n",
    "            indexOfWord = anger[i].find(word)\n",
    "            if slangsBefore.get(word) is None:\n",
    "                if slangsBefore.get(word.lower()) is None:\n",
    "                    if slangsBefore.get(word.upper()) is None:\n",
    "                        continue\n",
    "                    else:\n",
    "                        anger[i] = anger[i][:indexOfWord] + slangsBefore.get(word.upper()) + anger[i][indexOfWord+len(word):]\n",
    "                else:\n",
    "                    anger[i] = anger[i][:indexOfWord] + slangsBefore.get(word.lower()) + anger[i][indexOfWord+len(word):]\n",
    "            else:\n",
    "                anger[i] = anger[i][:indexOfWord] + slangsBefore.get(word) + anger[i][indexOfWord+len(word):]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(anger)):\n",
    "    #Trattamento punteggiatura\n",
    "    line = re.compile(\"[\" + re.escape(\"\".join(puntuaction)) + \"]\")\n",
    "    anger[i] = re.sub(line, \"\", anger[i])\n",
    "\n",
    "    #Rendiamo la farse in minuscolo\n",
    "    anger[i] = anger[i].lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizzazione di tutte le frasi\n",
    "angerTokenized = []\n",
    "for linea in anger:\n",
    "    result = word_tokenize(linea)\n",
    "    angerTokenized.append(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Esecuzione della POS tagging per ogni riga della matrice\n",
    "for i in range(len(angerTokenized)):\n",
    "    angerTokenized[i] = nltk.pos_tag(angerTokenized[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming\n",
    "#ps = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for line in angerTokenized:\n",
    "    for i in range(len(line)):\n",
    "        tupla = [lemmatizer.lemmatize(line[i][0]),line[i][1]]\n",
    "        line[i] = tupla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trattamnento Stop word dopo Stemming\n",
    "stop_words = set(stopwords.words('english'))\n",
    "angerTokStoped = []\n",
    "for line in angerTokenized:\n",
    "    filtred_word = []\n",
    "    for row in line:\n",
    "        if  row[0] not in stop_words:\n",
    "            filtred_word.append(row) \n",
    "    angerTokStoped.append(filtred_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#liste singole di parole e token\n",
    "angerListWord = []\n",
    "angerListTok = []\n",
    "for line in angerTokStoped:\n",
    "    for row in line:\n",
    "        angerListWord.append(row[0])\n",
    "        angerListTok.append(row[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lavorazione risorse lessicali\n",
    "resourceAngerEmoSN = []\n",
    "resourceAngerNRC = []\n",
    "resourceAngerSentisense = []\n",
    "\n",
    "#Lista di righe, ogni riga una tweet anger\n",
    "file = open(\"Risorse lessicali\\Anger\\EmoSN_anger.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        resourceAngerEmoSN.append(riga)\n",
    "\n",
    "file.close()\n",
    "\n",
    "file = open(\"Risorse lessicali\\\\Anger\\\\NRC_anger.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        resourceAngerNRC.append(riga)\n",
    "\n",
    "file.close()\n",
    "\n",
    "file = open(\"Risorse lessicali\\Anger\\sentisense_anger.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        resourceAngerSentisense.append(riga)\n",
    "\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.InsertOneResult at 0x23b8a290cd0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Caricare le risorse lessicali \n",
    "\n",
    "AngerEmoSN = {\n",
    "    '_id': 'EmoSN_anger',\n",
    "    'sentiment': 'anger',\n",
    "    'totNumberWords': len(resourceAngerEmoSN),  \n",
    "    'wordList': resourceAngerEmoSN\n",
    "}\n",
    "\n",
    "AngerNRC = {\n",
    "    '_id': 'NRC_anger',\n",
    "    'sentiment': 'anger',\n",
    "    'totNumberWords': len(resourceAngerNRC),  \n",
    "    'wordList': resourceAngerNRC\n",
    "}\n",
    "\n",
    "AngerSentisense = {\n",
    "    '_id': 'sentisense_anger',\n",
    "    'sentiment': 'anger',\n",
    "    'totNumberWords': len(resourceAngerSentisense),  \n",
    "    'wordList': resourceAngerSentisense\n",
    "}\n",
    "\n",
    "resourcesCollection.insert_one(AngerEmoSN)\n",
    "resourcesCollection.insert_one(AngerNRC)\n",
    "resourcesCollection.insert_one(AngerSentisense)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conteggio delle frequenze delle parole\n",
    "countWordAnger = Counter(angerListWord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione database e collezioni\n",
    "# Inserimento singoli documenti contententi i tweet anger\n",
    "angerMongo = []\n",
    "wordsString = {}\n",
    "\n",
    "for i in range(len(angerListWord)):\n",
    "    wordsString = {\n",
    "        \"lemma\": angerListWord[i],\n",
    "        \"POS\": angerListTok[i]\n",
    "    }\n",
    "    angerTwit={\"word\":wordsString}\n",
    "    angerCollections.insert_one(angerTwit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using a JSON string\n",
    "#with open('jsonAnger_data.txt', 'w') as outfile:\n",
    "#    outfile.write(json.dumps(angerInsert))\n",
    "\n",
    "#Inserimento Documento \n",
    "angerInsert = {\n",
    "    \"sentiment\":\"anger\",\n",
    "    \"hashtags\": hashtagAnger,\n",
    "    \"emoticons\": emoticonAnger,\n",
    "    \"emoji\": emojiAnger\n",
    "}\n",
    "\n",
    "#inserimento\n",
    "x = collectionTwitter.insert_one(angerInsert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.command_cursor.CommandCursor at 0x23b8a2cc400>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# match --> { $match: { <query> } } se si soddisfa una query\n",
    "# project --> { $project: { <specification(s)> } } prende un documento che può specificare l'inclusione dei campi, \n",
    "# la soppressione del campo _id, l'aggiunta di nuovi campi e il ripristino dei valori dei campi esistenti\n",
    "# addfield --> { $addFields: { <newField>: <expression>, ... } } Specificare il nome di ogni campo da aggiungere e impostarne il valore su un'espressione di aggregazione\n",
    "# lookup --> {$lookup: {from , localField, foreignkey, as   } } Per eseguire una corrispondenza di uguaglianza tra un campo dei documenti di input\n",
    "# group --> {$groud: {_id:<expression> , <field> : {<accumulator1> : <expression1>}} } \n",
    "\n",
    "# Raggruppa i documenti di input in base all'espressione _id specificata e per ciascun raggruppamento distinto genera un documento\n",
    "pipeline = [\n",
    "    {\"$group\": {\"_id\": \"$word.lemma\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"word\": \"$_id\",\n",
    "      \"freq\": \"$count\"\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"freq\": -1\n",
    "    }},\n",
    "    {\"$out\": \"anger\"}\n",
    "]\n",
    "\n",
    "#post processing per renderlo valido \n",
    "angerCollections.aggregate(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creazione Word Clouds per le parole\n",
    "data = {}\n",
    "for document in angerCollections.find().limit(50):\n",
    "    data[document['word']] = document['freq']\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2)\n",
    "wc.fit_words(data)\n",
    "wc.to_file('Word Clouds/angerWords.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.UpdateResult at 0x23baa59f220>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Conteggio degli hashtags\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$hashtags\"},\n",
    "    {\"$group\": {\"_id\": \"$hashtags\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"word\": \"$_id\",\n",
    "      \"count\": 1\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"count\": -1\n",
    "    }}\n",
    "]\n",
    "\n",
    "hashtagsCount = collectionTwitter.aggregate(pipeline)\n",
    "\n",
    "data = {}\n",
    "for document in hashtagsCount:\n",
    "    data[document['word']] = document['count']\n",
    "\n",
    "collectionTwitter.update_one(\n",
    "  {\"sentiment\": \"anger\"},\n",
    "  {\"$set\": {\"hashtags\": data}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creazione Word Clouds per gli hashtags\n",
    "\n",
    "hashtags = collectionTwitter.distinct(\"hashtags\")\n",
    "\n",
    "hashtagsLimited = {}\n",
    "for (word,count) in [x for x in hashtags[0].items()][:50]:\n",
    "    hashtagsLimited[word] = count\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2)\n",
    "wc.fit_words(hashtagsLimited)\n",
    "wc.to_file('Word Clouds/angerHashtags.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.UpdateResult at 0x23b8a2ccbb0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Conteggio delle emoji\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$emoji\"},\n",
    "    {\"$group\": {\"_id\": \"$emoji\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"emoji\": \"$_id\",\n",
    "      \"count\": 1\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"count\": -1\n",
    "    }}\n",
    "]\n",
    "\n",
    "emojiCount = collectionTwitter.aggregate(pipeline)\n",
    "\n",
    "data = {}\n",
    "for document in emojiCount:\n",
    "    data[document['emoji']] = document['count']\n",
    "\n",
    "collectionTwitter.update_one(\n",
    "  {\"sentiment\": \"anger\"},\n",
    "  {\"$set\": {\"emoji\": data}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creazione Word Clouds per le emoji\n",
    "\n",
    "emojis = collectionTwitter.distinct(\"emoji\")\n",
    "\n",
    "emojisLimited = {}\n",
    "for (word,count) in [x for x in emojis[0].items()][:50]:\n",
    "    emojisLimited[word] = count\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2, font_path='Font Emoji\\Symbola.otf')\n",
    "wc.fit_words(emojisLimited)\n",
    "wc.to_file('Word Clouds/angerEmojis.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.UpdateResult at 0x23b8a291810>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Conteggio degli emoticons\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$emoticons\"},\n",
    "    {\"$group\": {\"_id\": \"$emoticons\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"emoticons\": \"$_id\",\n",
    "      \"count\": 1\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"count\": -1\n",
    "    }}\n",
    "]\n",
    "\n",
    "emoticonsCount = collectionTwitter.aggregate(pipeline)\n",
    "\n",
    "data = {}\n",
    "for document in emoticonsCount:\n",
    "    data[document['emoticons']] = document['count']\n",
    "\n",
    "collectionTwitter.update_one(\n",
    "  {\"sentiment\": \"anger\"},\n",
    "  {\"$set\": {\"emoticons\": data}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creazione Word Clouds per le emoticons\n",
    "\n",
    "emoticons = collectionTwitter.distinct(\"emoticons\")\n",
    "\n",
    "emoticonsLimited = {}\n",
    "for (word,count) in [x for x in emoticons[0].items()][:50]:\n",
    "    emoticonsLimited[word] = count\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2)\n",
    "wc.fit_words(emoticonsLimited)\n",
    "wc.to_file('Word Clouds/angerEmoticons.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.command_cursor.CommandCursor at 0x23b8a2cf5e0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pipeline di aggregazione per inserire la risorsa lessicale a cui appartiene la parola\n",
    "\n",
    "pipeline = [\n",
    "    {\"$lookup\": {\n",
    "        \"from\": \"Resources\",\n",
    "        \"localField\": \"word\",\n",
    "        \"foreignField\": \"wordList\",\n",
    "        \"as\": \"LexResources\"\n",
    "    }},\n",
    "    {\"$project\": {\n",
    "        \"resources.wordList\": 0,\n",
    "        \"resources.totNumberWords\": 0,\n",
    "        \"resources.sentiment\": 0,\n",
    "        \"LexResources.sentiment\": 0,\n",
    "        \"LexResources.totNumberWords\": 0,\n",
    "        \"LexResources.wordList\": 0\n",
    "    }},\n",
    "    {\"$out\": \"anger\"}\n",
    "]\n",
    "\n",
    "angerCollections.aggregate(pipeline)\n",
    "       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': 'NRC_anger', 'perc_presence_lex_res': 0.485164, 'perc_presence_twitter': 0.001179}\n",
      "{'_id': 'sentisense_anger', 'perc_presence_lex_res': 0.545455, 'perc_presence_twitter': 5.8e-05}\n",
      "{'_id': 'EmoSN_anger', 'perc_presence_lex_res': 0.567797, 'perc_presence_twitter': 0.000392}\n"
     ]
    }
   ],
   "source": [
    "#Pipeline di aggregazione per calcolare le percentuali richieste\n",
    "\n",
    "pipelineCount = [\n",
    "    {\"$group\": {\"_id\": \"null\", \"count\": { \"$sum\": \"$freq\" }}},\n",
    "    {\"$project\": {\n",
    "        \"_id\": 0,\n",
    "        \"count\": \"$count\"\n",
    "    }}\n",
    "]\n",
    "\n",
    "results = angerCollections.aggregate(pipelineCount)\n",
    "\n",
    "countWords = 0\n",
    "for data in results:\n",
    "    countWords = data['count']\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$LexResources\"},\n",
    "    {\"$lookup\": {\n",
    "        \"from\": \"Resources\",\n",
    "        \"localField\": \"LexResources._id\",\n",
    "        \"foreignField\": \"_id\",\n",
    "        \"as\": \"RefResource\"\n",
    "    }},\n",
    "    {\"$group\": {\"_id\": { \"lex\": \"$LexResources._id\", \"RefResource\": \"$RefResource\" }, \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "        \"_id\": \"$_id.lex\",\n",
    "        \"totWordsLex\": \"$_id.RefResource.totNumberWords\",\n",
    "        \"commonWords\": \"$count\",\n",
    "        \"sum_val\": \"$sum_val\"\n",
    "    }},\n",
    "    {\"$unwind\": \"$totWordsLex\"},\n",
    "    {\"$addFields\": {\"totalWordsTwitter\": countWords}},\n",
    "    {\"$project\": {\n",
    "        \"perc_presence_lex_res\": { \"$round\": [{\"$divide\": [ \"$commonWords\", \"$totWordsLex\" ] }, 6] },\n",
    "        \"perc_presence_twitter\": { \"$round\": [{\"$divide\": [ \"$commonWords\", \"$totalWordsTwitter\" ]}, 6] }\n",
    "    }}\n",
    "]\n",
    "\n",
    "results = angerCollections.aggregate(pipeline)\n",
    "\n",
    "#Genero l'isotagramma\n",
    "istogramAnger_lexRes = []\n",
    "istogramAnger_twitter = []\n",
    "\n",
    "for data in results:\n",
    "    print(data)\n",
    "    istogramAnger_lexRes.append(data['perc_presence_lex_res'])\n",
    "    istogramAnger_twitter.append(data['perc_presence_twitter'])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n, bins, patches=plt.hist(istogramAnger_lexRes)\n",
    "plt.xlabel(\"Values\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.title(\"Histogram perc_presence_lex_res\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n, bins, patches=plt.hist(istogramAnger_twitter)\n",
    "plt.xlabel(\"Values\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.title(\"Histogram perc_presence_twitter\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trattamento dataset Anticipation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lista di righe, ogni riga una tweet anticipation\n",
    "file = open(\"Dataset/dataset_dt_anticipation_60k.txt\", 'r',encoding=\"utf8\")\n",
    "anticipation = []\n",
    "hashtagAnticipation = []\n",
    "emoticonAnticipation = []\n",
    "emojiAnticipation = []\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "    riga = re.sub(r'URL', '',riga)\n",
    "    riga = re.sub(r'USERNAME', '',riga)\n",
    "\n",
    "    #array di hashtag singoli\n",
    "    hashTag = (re.findall('#[a-zA-Z]+', riga))\n",
    "\n",
    "    #generiamo l'array di hashtag senza hashtag\n",
    "    for h in hashTag:\n",
    "        hashtagAnticipation.append(h.replace(\"#\",\"\"))\n",
    "\n",
    "    #Rimuoviamo glin hashtag dalla riga\n",
    "    riga = riga.replace('#',\"\")\n",
    "    \n",
    "    anticipation.append(riga)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(anticipation)):\n",
    "    #Controllo se l'emoticons c'è e in caso la rimuovo\n",
    "    for singleEmoticon in emoticons:\n",
    "        emoticonsFind = re.findall(re.escape(singleEmoticon),anticipation[i])\n",
    "        if(len(emoticonsFind) != 0):\n",
    "            emoticonAnticipation.extend(emoticonsFind)\n",
    "            anticipation[i] = anticipation[i].replace(singleEmoticon,\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(anticipation)):\n",
    "    #preleviamo le emoji e le rimuoviamo dal testo\n",
    "    data = regex.findall(r'\\X', anticipation[i])\n",
    "    for word in data:\n",
    "        if any(char in emoji.UNICODE_EMOJI['en'] for char in word):\n",
    "            emojiAnticipation.append(word)\n",
    "            anticipation[i] = anticipation[i].replace(word, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processamento slang prima di punteggiatura e lowercase\n",
    "with open(\"Slangs/slangs_before.json\") as f:\n",
    "    slangsBefore = json.load(f)\n",
    "    for i in range(len(anticipation)):\n",
    "        for word in anticipation[i].split():\n",
    "            indexOfWord = anticipation[i].find(word)\n",
    "            if slangsBefore.get(word) is None:\n",
    "                if slangsBefore.get(word.lower()) is None:\n",
    "                    if slangsBefore.get(word.upper()) is None:\n",
    "                        continue\n",
    "                    else:\n",
    "                        anticipation[i] = anticipation[i][:indexOfWord] + slangsBefore.get(word.upper()) + anticipation[i][indexOfWord+len(word):]\n",
    "                else:\n",
    "                    anticipation[i] = anticipation[i][:indexOfWord] + slangsBefore.get(word.lower()) + anticipation[i][indexOfWord+len(word):]\n",
    "            else:\n",
    "                anticipation[i] = anticipation[i][:indexOfWord] + slangsBefore.get(word) + anticipation[i][indexOfWord+len(word):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(anticipation)):\n",
    "    #Trattamento punteggiatura\n",
    "    line = re.compile(\"[\" + re.escape(\"\".join(puntuaction)) + \"]\")\n",
    "    anticipation[i] = re.sub(line, \"\", anticipation[i])\n",
    "\n",
    "    #Rendiamo la farse in minuscolo\n",
    "    anticipation[i] = anticipation[i].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizzazione di tutte le frasi\n",
    "anticipationTokenized = []\n",
    "for linea in anticipation:\n",
    "    result = word_tokenize(linea)\n",
    "    anticipationTokenized.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Esecuzione della POS tagging per ogni riga della matrice\n",
    "for i in range(len(anticipationTokenized)):\n",
    "    anticipationTokenized[i] = nltk.pos_tag(anticipationTokenized[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming\n",
    "#ps = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for line in anticipationTokenized:\n",
    "    for i in range(len(line)):\n",
    "        tupla = [lemmatizer.lemmatize(line[i][0]),line[i][1]]\n",
    "        line[i] = tupla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trattamnento Stop word dopo Stemming\n",
    "stop_words = set(stopwords.words('english'))\n",
    "anticipationTokStoped = []\n",
    "for line in anticipationTokStoped:\n",
    "    filtred_word = []\n",
    "    for row in line:\n",
    "        if  row[0] not in stop_words:\n",
    "            filtred_word.append(row) \n",
    "    anticipationTokStoped.append(filtred_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lavorazione risorse lessicali\n",
    "resourceAnticipation = []\n",
    "#Lista di righe, ogni riga una tweet anger\n",
    "file = open(\"Risorse lessicali\\\\Anticipation\\\\NRC_anticipation.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        if(riga not in resourceAnticipation):\n",
    "            resourceAnticipation.append(line)\n",
    "\n",
    "file.close()\n",
    "\n",
    "file = open(\"Risorse lessicali\\Anticipation\\sentisense_anticipation.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        if(riga not in resourceAnticipation):\n",
    "            resourceAnticipation.append(line)\n",
    "\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anticipationListWord = []\n",
    "anticipationListTok = []\n",
    "for line in anticipationTokStoped:\n",
    "    for row in line:\n",
    "        anticipationListWord.append(row[0])\n",
    "        anticipationListTok.append(row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conteggio delle frequenze delle parole\n",
    "countWordAnticipation = Counter(anticipationListWord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione database e collezioni\n",
    "# Inserimento singoli documenti contententi i tweet anger\n",
    "anticipationMongo = []\n",
    "wordsString = {}\n",
    "\n",
    "for i in range(len(anticipationMongo)):\n",
    "    wordsString = {\n",
    "        \"lemma\": anticipationListWord[i],\n",
    "        \"POS\": anticipationListTok[i]\n",
    "    }\n",
    "    anticipationTwit={\"word\":wordsString}\n",
    "    anticipationCollections.insert_one(anticipationTwit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inserimento Documento\n",
    "anticipationInsert = {\n",
    "    \"sentiment\":\"anticipation\",\n",
    "    \"hashtags\": hashtagAnticipation,\n",
    "    \"emoticons\": emoticonAnticipation,\n",
    "    \"emoji\": emojiAnticipation\n",
    "}\n",
    "\n",
    "#inserimento\n",
    "x = collectionTwitter.insert_one(anticipationInsert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# match --> { $match: { <query> } } se si soddisfa una query\n",
    "# project --> { $project: { <specification(s)> } } prende un documento che può specificare l'inclusione dei campi, \n",
    "# la soppressione del campo _id, l'aggiunta di nuovi campi e il ripristino dei valori dei campi esistenti\n",
    "# addfield --> { $addFields: { <newField>: <expression>, ... } } Specificare il nome di ogni campo da aggiungere e impostarne il valore su un'espressione di aggregazione\n",
    "# lookup --> {$lookup: {from , localField, foreignkey, as   } } Per eseguire una corrispondenza di uguaglianza tra un campo dei documenti di input\n",
    "# group --> {$groud: {_id:<expression> , <field> : {<accumulator1> : <expression1>}} } \n",
    "#   Raggruppa i documenti di input in base all'espressione _id specificata e per ciascun raggruppamento distinto genera un documento\n",
    "\n",
    "pipeline = [\n",
    "    {\"$group\": {\"_id\": \"$word.lemma\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"word\": \"$_id\",\n",
    "      \"freq\": \"$count\"\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"freq\": -1\n",
    "    }},\n",
    "    {\"$out\": \"anticipation\"}\n",
    "]\n",
    "\n",
    "\n",
    "wordCount = anticipationCollections.aggregate(pipeline)\n",
    "#Creazione Word Clouds per le parole\n",
    "data = {}\n",
    "for document in anticipationCollections.find().limit(50):\n",
    "    data[document['word']] = document['freq']\n",
    "\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2)\n",
    "wc.fit_words(data)\n",
    "wc.to_file('Word Clouds/anticipationWords.png')\n",
    "# Conteggio degli hashtags\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$hashtags\"},\n",
    "    {\"$group\": {\"_id\": \"$hashtags\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"word\": \"$_id\",\n",
    "      \"count\": 1\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"count\": -1\n",
    "    }}\n",
    "]\n",
    "\n",
    "hashtagsCount = collectionTwitter.aggregate(pipeline)\n",
    "\n",
    "data = {}\n",
    "for document in hashtagsCount:\n",
    "    data[document['word']] = document['count']\n",
    "\n",
    "collectionTwitter.update_one(\n",
    "  {\"sentiment\": \"anticipation\"},\n",
    "  {\"$set\": {\"hashtags\": data}}\n",
    ")\n",
    "#Creazione Word Clouds per gli hashtags\n",
    "\n",
    "hashtags = collectionTwitter.distinct(\"hashtags\")\n",
    "\n",
    "hashtagsLimited = {}\n",
    "for (word,count) in [x for x in hashtags[0].items()][:50]:\n",
    "    hashtagsLimited[word] = count\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2)\n",
    "wc.fit_words(hashtagsLimited)\n",
    "wc.to_file('Word Clouds/anticipationHashtags.png')\n",
    "# Conteggio delle emoji\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$emoji\"},\n",
    "    {\"$group\": {\"_id\": \"$emoji\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"emoji\": \"$_id\",\n",
    "      \"count\": 1\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"count\": -1\n",
    "    }}\n",
    "]\n",
    "\n",
    "emojiCount = collectionTwitter.aggregate(pipeline)\n",
    "\n",
    "data = {}\n",
    "for document in emojiCount:\n",
    "    data[document['emoji']] = document['count']\n",
    "\n",
    "collectionTwitter.update_one(\n",
    "  {\"sentiment\": \"anticipation\"},\n",
    "  {\"$set\": {\"emoji\": data}}\n",
    ")\n",
    "#Creazione Word Clouds per le emoji\n",
    "\n",
    "emojis = collectionTwitter.distinct(\"emoji\")\n",
    "\n",
    "emojisLimited = {}\n",
    "for (word,count) in [x for x in emojis[0].items()][:50]:\n",
    "    emojisLimited[word] = count\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2)\n",
    "wc.fit_words(emojisLimited)\n",
    "wc.to_file('Word Clouds/anticipationEmojis.png')\n",
    "# Conteggio degli emoticons\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$emoticons\"},\n",
    "    {\"$group\": {\"_id\": \"$emoticons\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"emoticons\": \"$_id\",\n",
    "      \"count\": 1\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"count\": -1\n",
    "    }}\n",
    "]\n",
    "\n",
    "emoticonsCount = collectionTwitter.aggregate(pipeline)\n",
    "\n",
    "data = {}\n",
    "for document in emoticonsCount:\n",
    "    data[document['emoticons']] = document['count']\n",
    "\n",
    "collectionTwitter.update_one(\n",
    "  {\"sentiment\": \"anticipation\"},\n",
    "  {\"$set\": {\"emoticons\": data}}\n",
    ")\n",
    "#Creazione Word Clouds per le emoticons\n",
    "\n",
    "emoticons = collectionTwitter.distinct(\"emoticons\")\n",
    "\n",
    "emoticonsLimited = {}\n",
    "for (word,count) in [x for x in emoticons[0].items()][:50]:\n",
    "    emoticonsLimited[word] = count\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2)\n",
    "wc.fit_words(emoticonsLimited)\n",
    "wc.to_file('Word Clouds/anticipationEmoticons.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trattamento dataset Disgust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lista di righe, ogni riga una tweet disgust\n",
    "file = open(\"Dataset/dataset_dt_disgust_60k.txt\", 'r',encoding=\"utf8\")\n",
    "disgust = []\n",
    "hashtagDisgust = []\n",
    "emoticonDisgust = []\n",
    "emojiDisgust = []\n",
    "\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "    riga = re.sub(r'URL', '',riga)\n",
    "    riga = re.sub(r'USERNAME', '',riga)\n",
    "\n",
    "    #array di hashtag singoli\n",
    "    hashTag = (re.findall('#[a-zA-Z]+', riga))\n",
    "    \n",
    "    #generiamo l'array di hashtag senza hashtag\n",
    "    for h in hashTag:\n",
    "        hashtagDisgust.append(h.replace(\"#\",\"\"))\n",
    "\n",
    "    #Rimuoviamo glin hashtag dalla riga\n",
    "    riga = riga.replace('#',\"\")\n",
    "\n",
    "    disgust.append(riga)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(disgust)):\n",
    "    #Controllo se l'emoticons c'è e in caso la rimuovo\n",
    "    for singleEmoticon in emoticons:\n",
    "        emoticonsFind = re.findall(re.escape(singleEmoticon),disgust[i])\n",
    "        if(len(emoticonsFind) != 0):\n",
    "            emoticonDisgust.extend(emoticonsFind)\n",
    "            disgust[i] = disgust[i].replace(singleEmoticon,\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(disgust)):\n",
    "    #preleviamo le emoji e le rimuoviamo dal testo\n",
    "    data = regex.findall(r'\\X', disgust[i])\n",
    "    for word in data:\n",
    "        if any(char in emoji.UNICODE_EMOJI['en'] for char in word):\n",
    "            emojiDisgust = [].append(word)\n",
    "            disgust[i] = disgust[i].replace(word, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processamento slang prima di punteggiatura e lowercase\n",
    "with open(\"Slangs/slangs_before.json\") as f:\n",
    "    slangsBefore = json.load(f)\n",
    "    for i in range(len(disgust)):\n",
    "        for word in disgust[i].split():\n",
    "            indexOfWord = disgust[i].find(word)\n",
    "            if slangsBefore.get(word) is None:\n",
    "                if slangsBefore.get(word.lower()) is None:\n",
    "                    if slangsBefore.get(word.upper()) is None:\n",
    "                        continue\n",
    "                    else:\n",
    "                        disgust[i] = disgust[i][:indexOfWord] + slangsBefore.get(word.upper()) + disgust[i][indexOfWord+len(word):]\n",
    "                else:\n",
    "                    disgust[i] = disgust[i][:indexOfWord] + slangsBefore.get(word.lower()) + disgust[i][indexOfWord+len(word):]\n",
    "            else:\n",
    "                disgust[i] = disgust[i][:indexOfWord] + slangsBefore.get(word) + disgust[i][indexOfWord+len(word):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(disgust)):\n",
    "    #Trattamento punteggiatura\n",
    "    line = re.compile(\"[\" + re.escape(\"\".join(puntuaction)) + \"]\")\n",
    "    disgust[i] = re.sub(line, \"\", disgust[i])\n",
    "\n",
    "    #Rendiamo la farse in minuscolo\n",
    "    disgust[i] = disgust[i].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizzazione di tutte le frasi\n",
    "disgustTokenized = []\n",
    "for linea in disgust:\n",
    "    result = word_tokenize(linea)\n",
    "    disgustTokenized.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Esecuzione della POS tagging per ogni riga della matrice\n",
    "for i in range(len(disgustTokenized)):\n",
    "    disgustTokenized[i] = nltk.pos_tag(disgustTokenized[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming\n",
    "#ps = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for line in disgustTokenized:\n",
    "    for i in range(len(line)):\n",
    "        tupla = [lemmatizer.lemmatize(line[i][0]),line[i][1]]\n",
    "        line[i] = tupla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trattamnento Stop word dopo Stemming\n",
    "stop_words = set(stopwords.words('english'))\n",
    "disgustTokStoped = []\n",
    "for line in disgustTokStoped:\n",
    "    filtred_word = []\n",
    "    for row in line:\n",
    "        if  row[0] not in stop_words:\n",
    "            filtred_word.append(row) \n",
    "    disgustTokStoped.append(filtred_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lavorazione risorse lessicali\n",
    "resourceDisgust = []\n",
    "#Lista di righe, ogni riga una tweet anger\n",
    "file = open(\"Risorse lessicali\\Disgust-Hate\\sentisense_disgust.txt.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        if(riga not in resourceDisgust):\n",
    "            resourceDisgust.append(riga)\n",
    "\n",
    "file.close()\n",
    "\n",
    "file = open(\"C:Risorse lessicali\\\\Disgust-Hate\\\\NRC_disgust.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        if(riga not in resourceDisgust):\n",
    "            resourceDisgust.append(line)\n",
    "\n",
    "file.close()\n",
    "\n",
    "file = open(\"Risorse lessicali\\Disgust-Hate\\sentisense_hate.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        if(riga not in resourceDisgust):\n",
    "            resourceDisgust.append(line)\n",
    "\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disgustListWord = []\n",
    "disgustListTok = []\n",
    "for line in disgustTokStoped:\n",
    "    for row in line:\n",
    "        disgustListWord.append(row[0])\n",
    "        disgustListTok.append(row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Slangs vengono modificati con la loro definizione\n",
    "# with open(\"Slangs/slangs.json\", \"r\", encoding='utf-8') as f:\n",
    "#     slangs = json.load(f)\n",
    "#     for row in range(len(disgustTokenized)):\n",
    "#         for column in range(len(disgustTokenized[row])):\n",
    "#             if disgustTokenized[row][column] in slangs:\n",
    "#                 disgustTokenized[row][column] = slangs.get(disgustTokenized[row][column])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trattamento dataset Fear "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lista di righe, ogni riga una tweet fear\n",
    "file = open(\"Dataset/dataset_dt_fear_60k.txt\", 'r',encoding=\"utf8\")\n",
    "fear = []\n",
    "hashtagFear = []\n",
    "emoticonFear = []\n",
    "emojiFear = []\n",
    "\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "    riga = re.sub(r'URL', '',riga)\n",
    "    riga = re.sub(r'USERNAME', '',riga)\n",
    "\n",
    "     #array di hashtag singoli\n",
    "    hashTag = (re.findall('#[a-zA-Z]+', riga))\n",
    "\n",
    "    #generiamo l'array di hashtag senza hashtag\n",
    "    for h in hashTag:\n",
    "        hashtagFear.append(h.replace(\"#\",\"\"))\n",
    "\n",
    "    #Rimuoviamo glin hashtag dalla riga\n",
    "    riga = riga.replace('#',\"\")\n",
    "\n",
    "    fear.append(riga)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(fear)):\n",
    "    #Controllo se l'emoticons c'è e in caso la rimuovo\n",
    "    for singleEmoticon in emoticons:\n",
    "        emoticonsFind = re.findall(re.escape(singleEmoticon),fear[i])\n",
    "        if(len(emoticonsFind) != 0):\n",
    "            emoticonFear.extend(emoticonsFind)\n",
    "            fear[i] = fear[i].replace(singleEmoticon,\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(fear)):\n",
    "    #preleviamo le emoji e le rimuoviamo dal testo\n",
    "    data = regex.findall(r'\\X', fear[i])\n",
    "    for word in data:\n",
    "        if any(char in emoji.UNICODE_EMOJI['en'] for char in word):\n",
    "            emojiFear = [].append(word)\n",
    "            fear[i] = fear[i].replace(word, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processamento slang prima di punteggiatura e lowercase\n",
    "with open(\"Slangs/slangs_before.json\") as f:\n",
    "    slangsBefore = json.load(f)\n",
    "    for i in range(len(fear)):\n",
    "        for word in fear[i].split():\n",
    "            indexOfWord = fear[i].find(word)\n",
    "            if slangsBefore.get(word) is None:\n",
    "                if slangsBefore.get(word.lower()) is None:\n",
    "                    if slangsBefore.get(word.upper()) is None:\n",
    "                        continue\n",
    "                    else:\n",
    "                        fear[i] = fear[i][:indexOfWord] + slangsBefore.get(word.upper()) + fear[i][indexOfWord+len(word):]\n",
    "                else:\n",
    "                    fear[i] = fear[i][:indexOfWord] + slangsBefore.get(word.lower()) + fear[i][indexOfWord+len(word):]\n",
    "            else:\n",
    "                fear[i] = fear[i][:indexOfWord] + slangsBefore.get(word) + fear[i][indexOfWord+len(word):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(fear)):\n",
    "    #Trattamento punteggiatura\n",
    "    line = re.compile(\"[\" + re.escape(\"\".join(puntuaction)) + \"]\")\n",
    "    fear[i] = re.sub(line, \"\", fear[i])\n",
    "\n",
    "    #Rendiamo la farse in minuscolo\n",
    "    fear[i] = fear[i].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizzazione di tutte le frasi\n",
    "fearTokenized = []\n",
    "for linea in fear:\n",
    "    result = word_tokenize(linea)\n",
    "    fearTokenized.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Esecuzione della POS tagging per ogni riga della matrice\n",
    "for i in range(len(fearTokenized)):\n",
    "    fearTokenized[i] = nltk.pos_tag(fearTokenized[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming\n",
    "#ps = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for line in fearTokenized:\n",
    "    for i in range(len(line)):\n",
    "        tupla = [lemmatizer.lemmatize(line[i][0]),line[i][1]]\n",
    "        line[i] = tupla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trattamnento Stop word dopo Stemming\n",
    "stop_words = set(stopwords.words('english'))\n",
    "fearTokStoped = []\n",
    "for line in fearTokStoped:\n",
    "    filtred_word = []\n",
    "    for row in line:\n",
    "        if  row[0] not in stop_words:\n",
    "            filtred_word.append(row) \n",
    "    fearTokStoped.append(filtred_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lavorazione risorse lessicali\n",
    "resourceFear = []\n",
    "#Lista di righe, ogni riga una tweet anger\n",
    "file = open(\"Risorse lessicali\\\\Fear\\\\NRC_fear.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        if(riga not in resourceFear):\n",
    "            resourceFear.append(line)\n",
    "\n",
    "file.close()\n",
    "\n",
    "file = open(\"Risorse lessicali\\Fear\\sentisense_fear.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        if(riga not in resourceFear):\n",
    "            resourceFear.append(line)\n",
    "\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fearListWord = []\n",
    "fearListToken = []\n",
    "for line in fearTokStoped:\n",
    "    for row in line:\n",
    "        fearListWord.append(row[0])\n",
    "        fearListToken.append(row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Slangs vengono modificati con la loro definizione\n",
    "# with open(\"Slangs/slangs.json\", \"r\", encoding='utf-8') as f:\n",
    "#     slangs = json.load(f)\n",
    "#     for row in range(len(fearTokenized)):\n",
    "#         for column in range(len(fearTokenized[row])):\n",
    "#             if fearTokenized[row][column] in slangs:\n",
    "#                 fearTokenized[row][column] = slangs.get(fearTokenized[row][column])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trattamento dataset Joy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lista di righe, ogni riga una tweet joy\n",
    "file = open(\"Dataset/dataset_dt_joy_60k.txt\", 'r',encoding=\"utf8\")\n",
    "joy = []\n",
    "hashtagJoy = []\n",
    "emoticonJoy = []\n",
    "emojiJoy = []\n",
    "\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "    riga = re.sub(r'URL', '',riga)\n",
    "    riga = re.sub(r'USERNAME', '',riga)\n",
    "    \n",
    "    #array di hashtag singoli\n",
    "    hashTag = (re.findall('#[a-zA-Z]+', riga))\n",
    "\n",
    "    #generiamo l'array di hashtag senza hashtag\n",
    "    for h in hashTag:\n",
    "        hashtagJoy.append(h.replace(\"#\",\"\"))\n",
    "\n",
    "    #Rimuoviamo glin hashtag dalla riga\n",
    "    riga = riga.replace('#',\"\")\n",
    "\n",
    "    joy.append(riga)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(joy)):\n",
    "    #Controllo se l'emoticons c'è e in caso la rimuovo\n",
    "    for singleEmoticon in emoticons:\n",
    "        emoticonJoy = re.findall(re.escape(singleEmoticon),joy[i])\n",
    "        if(len(emoticonsFind) != 0):\n",
    "            emoticonFear.extend(emoticonsFind)\n",
    "            joy[i] = joy[i].replace(singleEmoticon,\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(joy)):\n",
    "    #preleviamo le emoji e le rimuoviamo dal testo\n",
    "    data = regex.findall(r'\\X', joy[i])\n",
    "    for word in data:\n",
    "        if any(char in emoji.UNICODE_EMOJI['en'] for char in word):\n",
    "            emojiJoy = [].append(word)\n",
    "            joy[i] = joy[i].replace(word, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processamento slang prima di punteggiatura e lowercase\n",
    "with open(\"Slangs/slangs_before.json\") as f:\n",
    "    slangsBefore = json.load(f)\n",
    "    for i in range(len(joy)):\n",
    "        for word in joy[i].split():\n",
    "            indexOfWord = joy[i].find(word)\n",
    "            if slangsBefore.get(word) is None:\n",
    "                if slangsBefore.get(word.lower()) is None:\n",
    "                    if slangsBefore.get(word.upper()) is None:\n",
    "                        continue\n",
    "                    else:\n",
    "                        joy[i] = joy[i][:indexOfWord] + slangsBefore.get(word.upper()) + joy[i][indexOfWord+len(word):]\n",
    "                else:\n",
    "                    joy[i] = joy[i][:indexOfWord] + slangsBefore.get(word.lower()) + joy[i][indexOfWord+len(word):]\n",
    "            else:\n",
    "                joy[i] = joy[i][:indexOfWord] + slangsBefore.get(word) + joy[i][indexOfWord+len(word):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(joy)):\n",
    "    #Trattamento punteggiatura\n",
    "    line = re.compile(\"[\" + re.escape(\"\".join(puntuaction)) + \"]\")\n",
    "    joy[i] = re.sub(line, \"\", joy[i])\n",
    "\n",
    "    #Rendiamo la farse in minuscolo\n",
    "    joy[i] = joy[i].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizzazione di tutte le frasi\n",
    "joyTokenized = []\n",
    "for linea in joy:\n",
    "    result = word_tokenize(linea)\n",
    "    joyTokenized.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Esecuzione della POS tagging per ogni riga della matrice\n",
    "for i in range(len(joyTokenized)):\n",
    "    joyTokenized[i] = nltk.pos_tag(joyTokenized[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Slangs vengono modificati con la loro definizione\n",
    "# with open(\"Slangs/slangs.json\", \"r\", encoding='utf-8') as f:\n",
    "#     slangs = json.load(f)\n",
    "#     for row in range(len(joyTokenized)):\n",
    "#         for column in range(len(joyTokenized[row])):\n",
    "#             if joyTokenized[row][column] in slangs:\n",
    "#                 joyTokenized[row][column] = slangs.get(joyTokenized[row][column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming\n",
    "#ps = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for line in joyTokenized:\n",
    "    for i in range(len(line)):\n",
    "        tupla = [lemmatizer.lemmatize(line[i][0]),line[i][1]]\n",
    "        line[i] = tupla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trattamnento Stop word dopo Stemming\n",
    "stop_words = set(stopwords.words('english'))\n",
    "joyTokStoped = []\n",
    "for line in joyTokenized:\n",
    "    filtred_word = []\n",
    "    for row in line:\n",
    "        if  row[0] not in stop_words:\n",
    "            filtred_word.append(row) \n",
    "    joyTokStoped.append(filtred_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joyListWord = []\n",
    "joyListTok = []\n",
    "for line in joyTokStoped:\n",
    "    for row in line:\n",
    "        joyListWord.append(row[0])\n",
    "        joyListTok.append(row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lavorazione risorse lessicali\n",
    "resourceJoy = []\n",
    "#Lista di righe, ogni riga una tweet anger\n",
    "file = open(\"Risorse lessicali\\Joy\\EmoSN_joy.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        if(riga not in resourceJoy):\n",
    "            resourceJoy.append(riga)\n",
    "\n",
    "file.close()\n",
    "\n",
    "file = open(\"Risorse lessicali\\\\Joy\\\\NRC_joy.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        if(riga not in resourceJoy):\n",
    "            resourceJoy.append(line)\n",
    "\n",
    "file.close()\n",
    "\n",
    "file = open(\"Risorse lessicali\\Joy\\sentisense_joy.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        if(riga not in resourceJoy):\n",
    "            resourceJoy.append(line)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trattamento dataset Sadness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lista di righe, ogni riga una tweet sadness\n",
    "file = open(\"Dataset/dataset_dt_sadness_60k.txt\", 'r',encoding=\"utf8\")\n",
    "sadness = []\n",
    "hashtagSadness = []\n",
    "emoticonSadness = []\n",
    "emojiSadness = []\n",
    "\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "    riga = re.sub(r'URL', '',riga)\n",
    "    riga = re.sub(r'USERNAME', '',riga)\n",
    "\n",
    "    #array di hashtag singoli\n",
    "    hashTag = (re.findall('#[a-zA-Z]+', riga))\n",
    "\n",
    "    #Rimuoviamo glin hashtag dalla riga\n",
    "    riga = riga.replace('#',\"\")\n",
    "\n",
    "    sadness.append(riga)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sadness)):\n",
    "    #Controllo se l'emoticons c'è e in caso la rimuovo\n",
    "    for singleEmoticon in emoticons:\n",
    "        emoticonJoy = re.findall(re.escape(singleEmoticon),sadness[i])\n",
    "        if(len(emoticonsFind) != 0):\n",
    "            emoticonSadness.extend(emoticonsFind)\n",
    "            sadness[i] = sadness[i].replace(singleEmoticon,\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sadness)):\n",
    "    #preleviamo le emoji e le rimuoviamo dal testo\n",
    "    data = regex.findall(r'\\X', sadness[i])\n",
    "    for word in data:\n",
    "        if any(char in emoji.UNICODE_EMOJI['en'] for char in word):\n",
    "            emojiSadness = [].append(word)\n",
    "            sadness[i] = sadness[i].replace(word, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processamento slang prima di punteggiatura e lowercase\n",
    "with open(\"Slangs/slangs_before.json\") as f:\n",
    "    slangsBefore = json.load(f)\n",
    "    for i in range(len(sadness)):\n",
    "        for word in sadness[i].split():\n",
    "            indexOfWord = sadness[i].find(word)\n",
    "            if slangsBefore.get(word) is None:\n",
    "                if slangsBefore.get(word.lower()) is None:\n",
    "                    if slangsBefore.get(word.upper()) is None:\n",
    "                        continue\n",
    "                    else:\n",
    "                        sadness[i] = sadness[i][:indexOfWord] + slangsBefore.get(word.upper()) + sadness[i][indexOfWord+len(word):]\n",
    "                else:\n",
    "                    sadness[i] = sadness[i][:indexOfWord] + slangsBefore.get(word.lower()) + sadness[i][indexOfWord+len(word):]\n",
    "            else:\n",
    "                sadness[i] = sadness[i][:indexOfWord] + slangsBefore.get(word) + sadness[i][indexOfWord+len(word):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sadness)):\n",
    "    #Trattamento punteggiatura\n",
    "    line = re.compile(\"[\" + re.escape(\"\".join(puntuaction)) + \"]\")\n",
    "    sadness[i] = re.sub(line, \"\", sadness[i])\n",
    "\n",
    "    #Rendiamo la farse in minuscolo\n",
    "    sadness[i] = sadness[i].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizzazione di tutte le frasi\n",
    "sadnessTokenized = []\n",
    "for linea in sadness:\n",
    "    result = word_tokenize(linea)\n",
    "    sadnessTokenized.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Esecuzione della POS tagging per ogni riga della matrice\n",
    "for i in range(len(sadnessTokenized)):\n",
    "    sadnessTokenized[i] = nltk.pos_tag(sadnessTokenized[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Slangs vengono modificati con la loro definizione\n",
    "# with open(\"Slangs/slangs.json\", \"r\", encoding='utf-8') as f:\n",
    "#     slangs = json.load(f)\n",
    "#     for row in range(len(sadnessTokenized)):\n",
    "#         for column in range(len(sadnessTokenized[row])):\n",
    "#             if sadnessTokenized[row][column] in slangs:\n",
    "#                 sadnessTokenized[row][column] = slangs.get(sadnessTokenized[row][column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming\n",
    "#ps = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for line in sadnessTokenized:\n",
    "    for i in range(len(line)):\n",
    "        tupla = [lemmatizer.lemmatize(line[i][0]),line[i][1]]\n",
    "        line[i] = tupla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trattamnento Stop word dopo Stemming\n",
    "stop_words = set(stopwords.words('english'))\n",
    "sadnessTokStoped = []\n",
    "for line in sadnessTokenized:\n",
    "    filtred_word = []\n",
    "    for row in line:\n",
    "        if  row[0] not in stop_words:\n",
    "            filtred_word.append(row) \n",
    "    sadnessTokStoped.append(filtred_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sadnessListWord = []\n",
    "sadnessListTok = []\n",
    "for line in sadnessTokStoped:\n",
    "    for row in line:\n",
    "        sadnessListWord.append(row[0])\n",
    "        sadnessListTok.append(row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lavorazione risorse lessicali\n",
    "resourceSadness = []\n",
    "file = open(\"Risorse lessicali\\\\Sadness\\\\NRC_sadness.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        if(riga not in resourceSadness):\n",
    "            resourceSadness.append(line)\n",
    "\n",
    "file.close()\n",
    "\n",
    "file = open(\"Risorse lessicali\\Sadness\\sentisense_joy.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        if(riga not in resourceSadness):\n",
    "            resourceSadness.append(line)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trattamento dataset Surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lista di righe, ogni riga una tweet surprise\n",
    "file = open(\"Dataset/dataset_dt_surprise_60k.txt\", 'r',encoding=\"utf8\")\n",
    "surprise = []\n",
    "hashtagSurprise = []\n",
    "emoticonSurprise = []\n",
    "emojiSurprise = []\n",
    "\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "    riga = re.sub(r'URL', '',riga)\n",
    "    riga = re.sub(r'USERNAME', '',riga)\n",
    "\n",
    "     #array di hashtag singoli\n",
    "    hashTag = (re.findall('#[a-zA-Z]+', riga))\n",
    "\n",
    "    #generiamo l'array di hashtag senza hashtag\n",
    "    for h in hashTag:\n",
    "        hashtagSurprise.append(h.replace(\"#\",\"\"))\n",
    "\n",
    "    #Rimuoviamo glin hashtag dalla riga\n",
    "    riga = riga.replace('#',\"\")\n",
    "\n",
    "    surprise.append(riga)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(surprise)):\n",
    "    #Controllo se l'emoticons c'è e in caso la rimuovo\n",
    "    for singleEmoticon in emoticons:\n",
    "        emoticonJoy = re.findall(re.escape(singleEmoticon),surprise[i])\n",
    "        if(len(emoticonsFind) != 0):\n",
    "            emoticonSurprise.extend(emoticonsFind)\n",
    "            surprise[i] = surprise[i].replace(singleEmoticon,\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(surprise)):\n",
    "    #preleviamo le emoji e le rimuoviamo dal testo\n",
    "    data = regex.findall(r'\\X', surprise[i])\n",
    "    for word in data:\n",
    "        if any(char in emoji.UNICODE_EMOJI['en'] for char in word):\n",
    "            emojiSurprise = [].append(word)\n",
    "            surprise[i] = surprise[i].replace(word, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processamento slang prima di punteggiatura e lowercase\n",
    "with open(\"Slangs/slangs_before.json\") as f:\n",
    "    slangsBefore = json.load(f)\n",
    "    for i in range(len(surprise)):\n",
    "        for word in surprise[i].split():\n",
    "            indexOfWord = surprise[i].find(word)\n",
    "            if slangsBefore.get(word) is None:\n",
    "                if slangsBefore.get(word.lower()) is None:\n",
    "                    if slangsBefore.get(word.upper()) is None:\n",
    "                        continue\n",
    "                    else:\n",
    "                        surprise[i] = surprise[i][:indexOfWord] + slangsBefore.get(word.upper()) + surprise[i][indexOfWord+len(word):]\n",
    "                else:\n",
    "                    surprise[i] = surprise[i][:indexOfWord] + slangsBefore.get(word.lower()) + surprise[i][indexOfWord+len(word):]\n",
    "            else:\n",
    "                surprise[i] = surprise[i][:indexOfWord] + slangsBefore.get(word) + surprise[i][indexOfWord+len(word):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(surprise)):\n",
    "    #Trattamento punteggiatura\n",
    "    line = re.compile(\"[\" + re.escape(\"\".join(puntuaction)) + \"]\")\n",
    "    surprise[i] = re.sub(line, \"\", surprise[i])\n",
    "\n",
    "    #Rendiamo la farse in minuscolo\n",
    "    surprise[i] = surprise[i].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizzazione di tutte le frasi\n",
    "surpriseTokenized = []\n",
    "for linea in surprise:\n",
    "    result = word_tokenize(linea)\n",
    "    surpriseTokenized.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Esecuzione della POS tagging per ogni riga della matrice\n",
    "for i in range(len(surpriseTokenized)):\n",
    "    surpriseTokenized[i] = nltk.pos_tag(surpriseTokenized[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Slangs vengono modificati con la loro definizione\n",
    "# with open(\"Slangs/slangs.json\", \"r\", encoding='utf-8') as f:\n",
    "#     slangs = json.load(f)\n",
    "#     for row in range(len(surpriseTokenized)):\n",
    "#         for column in range(len(surpriseTokenized[row])):\n",
    "#             if surpriseTokenized[row][column] in slangs:\n",
    "#                 surpriseTokenized[row][column] = slangs.get(surpriseTokenized[row][column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming\n",
    "#ps = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for line in surpriseTokenized:\n",
    "    for i in range(len(line)):\n",
    "        tupla = [lemmatizer.lemmatize(line[i][0]),line[i][1]]\n",
    "        line[i] = tupla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trattamnento Stop word dopo Stemming\n",
    "stop_words = set(stopwords.words('english'))\n",
    "surpriseTokStoped = []\n",
    "for line in surpriseTokenized:\n",
    "    filtred_word = []\n",
    "    for row in line:\n",
    "        if  row[0] not in stop_words:\n",
    "            filtred_word.append(row) \n",
    "    surpriseTokStoped.append(filtred_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surpriseListWord = []\n",
    "surpriseListTok = []\n",
    "for line in surpriseTokStoped:\n",
    "    for row in line:\n",
    "        surpriseListWord.append(row[0])\n",
    "        surpriseListTok.append(row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lavorazione risorse lessicali\n",
    "resourceSurprise = []\n",
    "file = open(\"Risorse lessicali\\\\Surprise\\\\NRC_surprise.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        if(riga not in resourceSurprise):\n",
    "            resourceSurprise.append(line)\n",
    "\n",
    "file.close()\n",
    "\n",
    "file = open(\"Risorse lessicali\\Surprise\\sentisense_surprise.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        if(riga not in resourceSurprise):\n",
    "            resourceSurprise.append(line)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trattamento dataset Trust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lista di righe, ogni riga una tweet trust\n",
    "file = open(\"Dataset/dataset_dt_trust_60k.txt\", 'r',encoding=\"utf8\")\n",
    "trust = []\n",
    "hashtagTrust = []\n",
    "emoticonTrust = []\n",
    "emojiTrust = []\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "    riga = re.sub(r'URL', '',riga)\n",
    "    riga = re.sub(r'USERNAME', '',riga)\n",
    "    \n",
    "    #array di hashtag singoli\n",
    "    hashTag = (re.findall('#[a-zA-Z]+', riga))\n",
    "    \n",
    "    #generiamo l'array di hashtag senza hashtag\n",
    "    for h in hashTag:\n",
    "        hashtagTrust.append(h.replace(\"#\",\"\"))\n",
    "\n",
    "    #Rimuoviamo glin hashtag dalla riga\n",
    "    riga = riga.replace('#',\"\")\n",
    "\n",
    "    trust.append(riga)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(trust)):\n",
    "    #Controllo se l'emoticons c'è e in caso la rimuovo\n",
    "    for singleEmoticon in emoticons:\n",
    "        emoticonJoy = re.findall(re.escape(singleEmoticon),trust[i])\n",
    "        if(len(emoticonsFind) != 0):\n",
    "            emoticonTrust.extend(emoticonsFind)\n",
    "            trust[i] = trust[i].replace(singleEmoticon,\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(trust)):\n",
    "    #preleviamo le emoji e le rimuoviamo dal testo\n",
    "    data = regex.findall(r'\\X', trust[i])\n",
    "    for word in data:\n",
    "        if any(char in emoji.UNICODE_EMOJI['en'] for char in word):\n",
    "            emojiTrust = [].append(word)\n",
    "            trust[i] = trust[i].replace(word, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processamento slang prima di punteggiatura e lowercase\n",
    "with open(\"Slangs/slangs_before.json\") as f:\n",
    "    slangsBefore = json.load(f)\n",
    "    for i in range(len(trust)):\n",
    "        for word in trust[i].split():\n",
    "            indexOfWord = trust[i].find(word)\n",
    "            if slangsBefore.get(word) is None:\n",
    "                if slangsBefore.get(word.lower()) is None:\n",
    "                    if slangsBefore.get(word.upper()) is None:\n",
    "                        continue\n",
    "                    else:\n",
    "                        trust[i] = trust[i][:indexOfWord] + slangsBefore.get(word.upper()) + trust[i][indexOfWord+len(word):]\n",
    "                else:\n",
    "                    trust[i] = trust[i][:indexOfWord] + slangsBefore.get(word.lower()) + trust[i][indexOfWord+len(word):]\n",
    "            else:\n",
    "                trust[i] = trust[i][:indexOfWord] + slangsBefore.get(word) + trust[i][indexOfWord+len(word):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(trust)):\n",
    "    #Trattamento punteggiatura\n",
    "    line = re.compile(\"[\" + re.escape(\"\".join(puntuaction)) + \"]\")\n",
    "    trust[i] = re.sub(line, \"\", trust[i])\n",
    "\n",
    "    #Rendiamo la farse in minuscolo\n",
    "    trust[i] = trust[i].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizzazione di tutte le frasi\n",
    "trustTokenized = []\n",
    "for linea in trust:\n",
    "    result = word_tokenize(linea)\n",
    "    trustTokenized.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Esecuzione della POS tagging per ogni riga della matrice\n",
    "for i in range(len(trustTokenized)):\n",
    "    trustTokenized[i] = nltk.pos_tag(trustTokenized[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming\n",
    "#ps = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for line in trustTokenized:\n",
    "    for i in range(len(line)):\n",
    "        tupla = [lemmatizer.lemmatize(line[i][0]),line[i][1]]\n",
    "        line[i] = tupla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trattamnento Stop word dopo Stemming\n",
    "stop_words = set(stopwords.words('english'))\n",
    "trustTokStoped = []\n",
    "for line in trustTokenized:\n",
    "    filtred_word = []\n",
    "    for row in line:\n",
    "        if  row[0] not in stop_words:\n",
    "            filtred_word.append(row) \n",
    "    trustTokStoped.append(filtred_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trustListWord = []\n",
    "trustListTok = []\n",
    "for line in trustTokStoped:\n",
    "    for row in line:\n",
    "        trustListWord.append(row[0])\n",
    "        trustListTok.append(row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lavorazione risorse lessicali\n",
    "resourceTrust = []\n",
    "file = open(\"Risorse lessicali\\\\Trust\\\\NRC_trust.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        if(riga not in resourceTrust):\n",
    "            resourceTrust.append(line)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Slangs vengono modificati con la loro definizione\n",
    "# with open(\"Slangs/slangs.json\", \"r\", encoding='utf-8') as f:\n",
    "#     slangs = json.load(f)\n",
    "#     for row in range(len(trustTokenized)):\n",
    "#         for column in range(len(trustTokenized[row])):\n",
    "#             if trustTokenized[row][column] in slangs:\n",
    "#                 trustTokenized[row][column] = slangs.get(trustTokenized[row][column])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "250b308242d36e83629721c7244075b381b1b678cf3b391cdc434a91845cedac"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
