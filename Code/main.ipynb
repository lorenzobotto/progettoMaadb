{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lores\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\lores\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\lores\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\lores\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\lores\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Import per il resto delle celle\n",
    "import re\n",
    "import emoji\n",
    "import regex\n",
    "import nltk\n",
    "import json\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from collections import Counter\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "import pymongo\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import psycopg2\n",
    "%matplotlib inline\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "emoticonsList = [':‑)',':)',':-]',':]',':->',':>','8-)','8)',':-}',':}',':o)',':c)',':^)','=]','=)',':‑D',':D','8‑D','8D','=D','=3','B^D','c:',\n",
    "'C:','x‑D','xD','X‑D','XD',':-))',':‑(',':(',':‑c',':c',':‑<',':<',':‑[',':[',':-||','>:[',':{',':@',':('',;(',\":'‑(\",\":'(\",':=(',\":'‑)\",\":')\",\n",
    "':\"D',\"D‑':\",'D:<','D:','D8','D;','D=','DX',':‑O',':O',':‑o',':o',':-0','8‑0','>:O','=O','=o','=0',':-3',':3','=3','x3','X3','>:3',':-*',\n",
    "':*',':×',';‑)',';)','*-)','*)',';‑]',';]',';^)',';>',':‑,',';D',';3',':‑P',':P','X‑P','XP','x‑p','xp',':‑p',':p',':‑Þ',':Þ',':‑þ',':þ',':‑b',':b',\n",
    "'d:','=p','>:P',':-/',':/',':‑.',\">:/\",'=/',':L','=L',':S',':‑|',':|',':$','://)','://3',':‑X',':X',':‑#',':#',':‑&',':&','O:‑)','O:)','0:‑3'\n",
    ",'0:3','0:‑)','0:)','0;^)','>:‑)','>:)','}:‑)','}:)','3:‑)','3:)','>;‑)','>;)','>:3',';3','|;‑)','|‑O','B-)',':‑J','#‑)','%‑)','%)',':‑###..'\n",
    ",':###..','<:‑|',\"',:-|\",\"',:-l\",':E','8-X','8=X','x-3','x=3','~:>','v.v','._.','._.;','QQ  ','qq','Qq','X_X','x_x','+_+','X_x','x_X'\n",
    "'<_<','>_>','<.<','>.>','O_O','o_o','O-O','o‑o','O_o','o_O','>.<','>_<','^5','o/\\o','>_>^ ^<_<','V.v.V','V=(° °)=V','(^^^)','(::[]::)',\n",
    "'(o)(o)','( • )( • )','[̲̅$̲̅(̲̅1̲̅)̲̅$̲̅]','[̲̅$̲̅(̲̅5̲̅)̲̅$̲̅]','[̲̅$̲̅(̲̅10)̲̅$̲̅]','[̲̅$̲̅(̲̅100)̲̅$̲̅]','( ͡° ͜ʖ ͡°)','ヽ༼ຈل͜ຈ༽ﾉ','(๑ˇεˇ๑)','(◕‿◕✿)','( ༎ຶ ۝ ༎ຶ )','(=ʘᆽʘ=)∫','ʕ •ᴥ•ʔ',\n",
    "'(>_<)','(>_<)>','(>w<)',\"(';')\",'(^_^;)','(-_-;)','(~_~;)','(・.・;)','(・_・;)','^^;','(・・;)','^_^;','(#^.^#)'\n",
    ",'(^^;)','(⁄ ⁄•⁄ω⁄•⁄ ⁄)','(-.-)y-°°°','(^.^)y-.o○','(-_-)zzz','(^_-)-☆','(^_-)','((+_+))','(+o+)','(°°)','(°-°) (°.°)',\n",
    "'(°_°)','(°_°>)','(°レ°)','(o|o)','<(｀^´)>','^_^','(°o°)','(^_^)/','(^O^)／','(^o^)／','(^^)/','(≧∇≦)/','(/◕ヮ◕)/','(^o^)丿','∩(·ω·)∩',\n",
    "'(·ω·)','^ω^','_(._.)_','_(_^_)_','<(_ _)>','<m(__)m>','m(__)m','m(_ _)m','(凸ಠ益ಠ)凸',\"('_')\",'(/_;)','(T_T)','(;_;)','(;_;','(;_:)','(;O;)'\n",
    ",'(:_;)','(ToT)','(Ｔ▽Ｔ)',';_;',';-;',';n;',';;','Q.Q','T.T','TnT','QQ','Q_Q',\n",
    "'(ー_ー)!!','(-.-)','(-_-)','(一一)','(；一_一)','(=_=)','(=^・^=)','(=^・・^=)','=^_^=','(..)','(._.)',\n",
    "'.o○','○o.','_旦~~','( ^^)','_U~~','( ^^)','☆ミ','☆彡','>°)))彡','(Q))',\n",
    "'><ヨヨ (°))<<','>°))))彡','<°)))彡','>°))彡','<+','))><<','<*))','>=<','<コ:彡',\n",
    "'Ｃ:.ミ','~>°)～～～','～°·_·°～','(°°)～','●～*','￣|○',':3ミ','^m^','(・・?','(?_?)','>^_^<','<^!^>',\n",
    "'^/^','（*^_^*','§^.^§','(^<^)','(^.^)','(^ム^)','(^·^)','(^.^)','(^_^.)','(^_^)','(^^)','(^J^)',\n",
    "'(*^.^*)','^_^','(#^.^#)','（^—^）','(^^)/~~~','(^_^)/~','(;_;)/~~~','(^.^)/~~~',\n",
    "'(-_-)/~~~','($··)/~~~','(@^^)/~~~','(T_T)/~~~','(V)o￥o(V)','＼(~o~)／','＼(^o^)／','＼(-o-)／','ヽ(^。^)ノ',\n",
    "'ヽ(^o^)丿','(*^0^*)','(*_*)','(*_*;','(+_+) (@_@)','(@_@。','(＠_＠;)',\n",
    " '＼(◎o◎)／！','(*^^)v','(^^)v','(^_^)v','（’-’*) (＾ｖ＾)','(＾▽＾)','(・∀・)','(´∀`)','(⌒▽⌒）',\n",
    " '＼(^o^)／','\\(^o^)/','(~o~)','(~_~)','(^^ゞ','ˊ＿>ˋ','(p_-)', '(-_q)','((d[-_-]b))','(-\"-)','(ーー゛)',\n",
    " '(^_^メ)', '(-_-メ)','(~_~メ)','(－－〆)','(・へ・)','(｀´)'\n",
    " , '<`～´>','<`ヘ´>','(ーー;)','(^0_0^)','(＾ｖ＾)', '(＾ｕ＾)','(＾◇＾)','( ^)o(^ )','(^O^)','(^o^)','(^○^)',')^o^(',\n",
    " '(*^▽^*)','(￣ー￣)','(￣□￣;)', '°o°','°O°',':O','o_O','o_0', 'o.O','(o.o)','oO','(°◇°)','（ ﾟ Дﾟ)','(*￣m￣)',\n",
    " 'ヽ(´ー｀)┌','¯\\_(ツ)_/¯','¯\\(°_o)/¯','(´･ω･`)', '(‘A`)','(づ￣ ³￣)づ','(*^3^)/~☆','(︶｡︶✽)','(-_-) zzz'\n",
    " , 'uwu','UwU','OWO','OwO','～°·_·°～', '(°°)～',':3ミ','(´･ω･`)','(`･ω･´)','(｀-´)>', '（　´_ゝ`）'] \n",
    "\n",
    "puntuaction = ['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '.',\n",
    "           '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', \n",
    "           '`', '{', '|', '}', '~', '»', '«', '“', '”', \"-\", \"...\",\"..\",\".,\",\",.\"]\n",
    "\n",
    "\n",
    "mongoClient = pymongo.MongoClient(\"mongodb://admin:admin@localhost:27017/\")\n",
    "mydb = mongoClient[\"maadb\"]\n",
    "\n",
    "collectionTwitter = mydb[\"Twitter\"] #Questa é per la collezione dei emoji emoticon hashtag per ogni emozione\n",
    "collectionAnger = mydb[\"CollectionAnger\"]\n",
    "collectionAnticipation = mydb[\"CollectionAnticipation\"]\n",
    "collectionDisgust = mydb[\"CollectionDisgust\"]\n",
    "collectionFear = mydb[\"CollectionFear\"]\n",
    "collectionHope = mydb[\"CollectionHope\"]\n",
    "collectionAJoy = mydb[\"CollectionJoy\"]\n",
    "collectionSadness = mydb[\"CollectionSadness\"]\n",
    "collectionSurprise = mydb[\"CollectionSurprise\"]\n",
    "collectionTrust = mydb[\"CollectionTrust\"]\n",
    "\n",
    "angerCollections = mydb[\"anger\"] #Collezioni per solo i twitt di anger\n",
    "anticipationCollections = mydb[\"anticipation\"]\n",
    "disgustCollections = mydb[\"disgust\"]\n",
    "fearCollections = mydb[\"fear\"]\n",
    "joyCollections = mydb[\"joy\"]\n",
    "sadnessCollections = mydb[\"sadness\"]\n",
    "surpriseCollections = mydb[\"surprise\"]\n",
    "trustCollections = mydb[\"trust\"]\n",
    "resourcesCollection = mydb[\"Resources\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trattamento dataset Anger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lista di righe, ogni riga una tweet anger\n",
    "file = open(\"Dataset/dataset_dt_anger_60k.txt\", 'r',encoding=\"utf8\")\n",
    "anger = []\n",
    "hashtagAnger = []\n",
    "emoticonAnger = []\n",
    "emojiAnger = []\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    #Eliminazione URL e USERNAME\n",
    "    riga = re.sub(r'URL', '',riga)\n",
    "    riga = re.sub(r'USERNAME', '',riga)\n",
    "\n",
    "    #array di hashtag singoli\n",
    "    hashTag = (re.findall('#[a-zA-Z0-9]+', riga))\n",
    "\n",
    "     #generiamo l'array di hashtag senza hashtag\n",
    "    for h in hashTag:\n",
    "        hashtagAnger.append(h.replace(\"#\",\"\"))\n",
    "\n",
    "    #Rimuoviamo glin hashtag dalla riga\n",
    "    riga = riga.replace('#',\"\")\n",
    "\n",
    "    #Salvataggio frase\n",
    "    anger.append(riga)\n",
    "\n",
    "\n",
    "file.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(anger)):\n",
    "    #Controllo se l'emoticons c'è e in caso la rimuovo\n",
    "    for singleEmoticon in emoticonsList:\n",
    "        emoticonsFind = re.findall(re.escape(singleEmoticon),anger[i])\n",
    "        if(len(emoticonsFind) != 0):\n",
    "            emoticonAnger.extend(emoticonsFind)\n",
    "            anger[i] = anger[i].replace(singleEmoticon,\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(anger)):\n",
    "    #preleviamo le emoji e le rimuoviamo dal testo\n",
    "    data = regex.findall(r'\\X', anger[i])\n",
    "    for word in data:\n",
    "        if any(char in emoji.UNICODE_EMOJI['en'] for char in word):\n",
    "            emojiAnger.append(word)\n",
    "            anger[i] = anger[i].replace(word, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processamento slang \n",
    "with open(\"Slangs/slangs_before.json\") as f:\n",
    "    slangsBefore = json.load(f)\n",
    "    for i in range(len(anger)):\n",
    "        for word in anger[i].split():\n",
    "            indexOfWord = anger[i].find(word)\n",
    "            if slangsBefore.get(word) is None:\n",
    "                if slangsBefore.get(word.lower()) is None:\n",
    "                    if slangsBefore.get(word.upper()) is None:\n",
    "                        continue\n",
    "                    else:\n",
    "                        anger[i] = anger[i][:indexOfWord] + slangsBefore.get(word.upper()) + anger[i][indexOfWord+len(word):]\n",
    "                else:\n",
    "                    anger[i] = anger[i][:indexOfWord] + slangsBefore.get(word.lower()) + anger[i][indexOfWord+len(word):]\n",
    "            else:\n",
    "                anger[i] = anger[i][:indexOfWord] + slangsBefore.get(word) + anger[i][indexOfWord+len(word):]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizzazione di tutte le frasi\n",
    "angerTokenized = []\n",
    "for linea in anger:\n",
    "    result = word_tokenize(linea)\n",
    "    angerTokenized.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Esecuzione della POS tagging per ogni riga della matrice\n",
    "for i in range(len(angerTokenized)):\n",
    "    angerTokenized[i] = nltk.pos_tag(angerTokenized[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trattamento punteggiatura\n",
    "for line in angerTokenized:\n",
    "    newLine = line\n",
    "    listToRemove = []\n",
    "    for i in range(len(line)):\n",
    "        if line[i][0] in puntuaction:\n",
    "            listToRemove.append(line[i])\n",
    "    for element in listToRemove:\n",
    "        newLine.remove(element)\n",
    "    line = newLine\n",
    "\n",
    "linePuntuaction = re.compile(\"[\" + re.escape(\"\".join(puntuaction)) + \"]\")\n",
    "for line in angerTokenized:\n",
    "    for i in range(len(line)):\n",
    "        tupla = [(re.sub(linePuntuaction, \"\", line[i][0])),line[i][1]]\n",
    "        line[i] = tupla\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming\n",
    "#ps = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for line in angerTokenized:\n",
    "    for i in range(len(line)):\n",
    "        tupla = [lemmatizer.lemmatize(line[i][0]),line[i][1]]\n",
    "        line[i] = tupla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trattamnento Stop word dopo Stemming\n",
    "stop_words = set(stopwords.words('english'))\n",
    "angerTokStoped = []\n",
    "for line in angerTokenized:\n",
    "    filtred_word = []\n",
    "    for row in line:\n",
    "        if  row[0] not in stop_words:\n",
    "            filtred_word.append(row) \n",
    "    angerTokStoped.append(filtred_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#liste singole di parole e token\n",
    "angerListWord = []\n",
    "angerListTok = []\n",
    "for line in angerTokStoped:\n",
    "    for row in line:\n",
    "        if row[0] != '':\n",
    "            angerListWord.append(row[0])\n",
    "        angerListTok.append(row[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lavorazione risorse lessicali\n",
    "resourceAngerEmoSN = []\n",
    "resourceAngerNRC = []\n",
    "resourceAngerSentisense = []\n",
    "\n",
    "#Lista di righe, ogni riga una tweet anger\n",
    "file = open(\"Risorse lessicali\\Anger\\EmoSN_anger.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        resourceAngerEmoSN.append(riga)\n",
    "\n",
    "file.close()\n",
    "\n",
    "file = open(\"Risorse lessicali\\\\Anger\\\\NRC_anger.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        resourceAngerNRC.append(riga)\n",
    "\n",
    "file.close()\n",
    "\n",
    "file = open(\"Risorse lessicali\\Anger\\sentisense_anger.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        resourceAngerSentisense.append(riga)\n",
    "\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Caricare le risorse lessicali \n",
    "\n",
    "AngerEmoSN = {\n",
    "    '_id': 'EmoSN_anger',\n",
    "    'sentiment': 'anger', \n",
    "    'wordList': resourceAngerEmoSN\n",
    "}\n",
    "\n",
    "AngerNRC = {\n",
    "    '_id': 'NRC_anger',\n",
    "    'sentiment': 'anger',  \n",
    "    'wordList': resourceAngerNRC\n",
    "}\n",
    "\n",
    "AngerSentisense = {\n",
    "    '_id': 'sentisense_anger',\n",
    "    'sentiment': 'anger',\n",
    "    'wordList': resourceAngerSentisense\n",
    "}\n",
    "\n",
    "resourcesCollection.insert_one(AngerEmoSN)\n",
    "resourcesCollection.insert_one(AngerNRC)\n",
    "resourcesCollection.insert_one(AngerSentisense)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione database e collezioni\n",
    "# Inserimento singoli documenti contententi i tweet anger\n",
    "wordsString = {}\n",
    "\n",
    "for i in range(len(angerListWord)):\n",
    "    wordsString = {\n",
    "        \"lemma\": angerListWord[i],\n",
    "        \"POS\": angerListTok[i]\n",
    "    }\n",
    "    angerTwit={\"word\":wordsString}\n",
    "    angerCollections.insert_one(angerTwit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inserimento collezione Twitter con hashtags, emoticons ed emoji \n",
    "angerInsert = {\n",
    "    \"sentiment\":\"anger\",\n",
    "    \"hashtags\": hashtagAnger,\n",
    "    \"emoticons\": emoticonAnger,\n",
    "    \"emoji\": emojiAnger\n",
    "}\n",
    "\n",
    "#inserimento\n",
    "x = collectionAnger.insert_one(angerInsert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raggruppa i documenti di input in base all'espressione _id specificata e per ciascun raggruppamento distinto genera un documento\n",
    "pipeline = [\n",
    "    {\"$group\": {\"_id\": \"$word.lemma\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"word\": \"$_id\",\n",
    "      \"freq\": \"$count\"\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"freq\": -1\n",
    "    }},\n",
    "    {\"$out\": \"anger\"}\n",
    "]\n",
    "\n",
    "#post processing per renderlo valido \n",
    "angerCollections.aggregate(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creazione Word Clouds per le parole\n",
    "data = {}\n",
    "for document in angerCollections.find().limit(50):\n",
    "    data[document['word']] = document['freq']\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2)\n",
    "wc.fit_words(data)\n",
    "wc.to_file('Word Clouds/Anger/angerWords.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conteggio degli hashtags\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$hashtags\"},\n",
    "    {\"$group\": {\"_id\": \"$hashtags\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"word\": \"$_id\",\n",
    "      \"count\": 1\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"count\": -1\n",
    "    }}\n",
    "]\n",
    "\n",
    "hashtagsCount = collectionAnger.aggregate(pipeline)\n",
    "\n",
    "data = {}\n",
    "for document in hashtagsCount:\n",
    "    data[document['word']] = document['count']\n",
    "\n",
    "collectionAnger.update_one(\n",
    "  {\"sentiment\": \"anger\"},\n",
    "  {\"$set\": {\"hashtags\": data}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creazione Word Clouds per gli hashtags\n",
    "\n",
    "hashtags = collectionAnger.distinct(\"hashtags\")\n",
    "\n",
    "hashtagsLimited = {}\n",
    "for (word,count) in [x for x in hashtags[0].items()][:50]:\n",
    "    hashtagsLimited[word] = count\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2)\n",
    "wc.fit_words(hashtagsLimited)\n",
    "wc.to_file('Word Clouds/Anger/angerHashtags.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conteggio delle emoji\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$emoji\"},\n",
    "    {\"$group\": {\"_id\": \"$emoji\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"emoji\": \"$_id\",\n",
    "      \"count\": 1\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"count\": -1\n",
    "    }}\n",
    "]\n",
    "\n",
    "emojiCount = collectionAnger.aggregate(pipeline)\n",
    "\n",
    "data = {}\n",
    "for document in emojiCount:\n",
    "    data[document['emoji']] = document['count']\n",
    "\n",
    "collectionAnger.update_one(\n",
    "  {\"sentiment\": \"anger\"},\n",
    "  {\"$set\": {\"emoji\": data}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creazione Word Clouds per le emoji\n",
    "\n",
    "emojis = collectionAnger.distinct(\"emoji\")\n",
    "\n",
    "emojisLimited = {}\n",
    "for (word,count) in [x for x in emojis[0].items()][:50]:\n",
    "    emojisLimited[word] = count\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2, font_path='Font Emoji\\Symbola.otf')\n",
    "wc.fit_words(emojisLimited)\n",
    "wc.to_file('Word Clouds/Anger/angerEmojis.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conteggio degli emoticons\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$emoticons\"},\n",
    "    {\"$group\": {\"_id\": \"$emoticons\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"emoticons\": \"$_id\",\n",
    "      \"count\": 1\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"count\": -1\n",
    "    }}\n",
    "]\n",
    "\n",
    "emoticonsCount = collectionAnger.aggregate(pipeline)\n",
    "\n",
    "data = {}\n",
    "for document in emoticonsCount:\n",
    "    data[document['emoticons']] = document['count']\n",
    "\n",
    "collectionAnger.update_one(\n",
    "  {\"sentiment\": \"anger\"},\n",
    "  {\"$set\": {\"emoticons\": data}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creazione Word Clouds per le emoticons\n",
    "\n",
    "emoticons = collectionAnger.distinct(\"emoticons\")\n",
    "\n",
    "emoticonsLimited = {}\n",
    "for (word,count) in [x for x in emoticons[0].items()][:50]:\n",
    "    emoticonsLimited[word] = count\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2)\n",
    "wc.fit_words(emoticonsLimited)\n",
    "wc.to_file('Word Clouds/Anger/angerEmoticons.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline di aggregazione per inserire la risorsa lessicale a cui appartiene la parola\n",
    "\n",
    "pipeline = [\n",
    "    {\"$lookup\": {\n",
    "        \"from\": \"Resources\",\n",
    "        \"localField\": \"word\",\n",
    "        \"foreignField\": \"wordList\",\n",
    "        \"pipeline\": [\n",
    "            {\"$match\": {\"$expr\": {\"$regexMatch\": {\"input\": \"$_id\", \"regex\": \"anger\"}}}}\n",
    "        ],\n",
    "        \"as\": \"LexResources\"\n",
    "    }},\n",
    "    {\"$project\": {\n",
    "        \"resources.wordList\": 0,\n",
    "        \"resources.totNumberWords\": 0,\n",
    "        \"resources.sentiment\": 0,\n",
    "        \"LexResources.sentiment\": 0,\n",
    "        \"LexResources.totNumberWords\": 0,\n",
    "        \"LexResources.wordList\": 0\n",
    "    }},\n",
    "    {\"$out\": \"anger\"}\n",
    "]\n",
    "\n",
    "angerCollections.aggregate(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline di aggregazione per calcolare le percentuali richieste\n",
    "\n",
    "pipelineCount = [\n",
    "    {\"$group\": {\"_id\": \"null\", \"count\": { \"$sum\": \"$freq\" }}},\n",
    "    {\"$project\": {\n",
    "        \"_id\": 0,\n",
    "        \"count\": \"$count\"\n",
    "    }}\n",
    "]\n",
    "\n",
    "results = angerCollections.aggregate(pipelineCount)\n",
    "\n",
    "countWords = 0\n",
    "for data in results:\n",
    "    countWords = data['count']\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$LexResources\"},\n",
    "    {\"$lookup\": {\n",
    "        \"from\": \"Resources\",\n",
    "        \"localField\": \"LexResources._id\",\n",
    "        \"foreignField\": \"_id\",\n",
    "        \"as\": \"RefResource\"\n",
    "    }},\n",
    "    {\"$group\": {\"_id\": { \"lex\": \"$LexResources._id\", \"RefResource\": \"$RefResource\" }, \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "        \"_id\": \"$_id.lex\",\n",
    "        \"wordList\": \"$_id.RefResource.wordList\",\n",
    "        \"commonWords\": \"$count\"\n",
    "    }},\n",
    "    {\"$unwind\": \"$wordList\"},\n",
    "    {\"$project\": {\n",
    "        \"_id\": 1,\n",
    "        \"commonWords\": 1,\n",
    "        \"totWordsLex\": {\"$size\": \"$wordList\"}\n",
    "    }},\n",
    "    {\"$addFields\": {\"totalWordsTwitter\": countWords}},\n",
    "    {\"$project\": {\n",
    "        \"perc_presence_lex_res\": { \"$round\": [{\"$divide\": [ \"$commonWords\", \"$totWordsLex\" ] }, 6] },\n",
    "        \"perc_presence_twitter\": { \"$round\": [{\"$divide\": [ \"$commonWords\", \"$totalWordsTwitter\" ]}, 6] }\n",
    "    }}\n",
    "]\n",
    "\n",
    "results = angerCollections.aggregate(pipeline)\n",
    "\n",
    "# Genero i dati per l'istogramma\n",
    "istogramAnger_lexRes = []\n",
    "istogramAnger_twitter = []\n",
    "\n",
    "for data in results:\n",
    "    print(data)\n",
    "    log =math.log(data['perc_presence_lex_res'],10)\n",
    "    istogramAnger_lexRes.append([data['_id'], data['perc_presence_lex_res']])\n",
    "    log =math.log(data['perc_presence_twitter'],10)\n",
    "    istogramAnger_twitter.append([data['_id'], data['perc_presence_twitter']])\n",
    " \n",
    "# Create the pandas DataFrame\n",
    "dfAnger_lexRes = pd.DataFrame(istogramAnger_lexRes, columns = ['Risorsa', 'Percentuale'])\n",
    "dfAnger_twitter = pd.DataFrame(istogramAnger_twitter,columns = ['Risorsa', 'Percentuale'])      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione istogramma per presenza parole nelle risorse lessicali\n",
    "\n",
    "dfAnger_lexRes.plot.bar(x='Risorsa')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione istogramma per presenza parole nei twitter\n",
    "\n",
    "dfAnger_twitter.plot.bar(x='Risorsa')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connessione, creazione cursore e database\n",
    "\n",
    "conn = psycopg2.connect(database = \"maadb\", user = \"postgres\", password = \"admin\", host = \"localhost\", port = \"5432\")\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione tabella risorse lessicali anger e inserimento\n",
    "cur.execute('''CREATE TABLE RISORSEANGER\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    NOME             TEXT       NOT NULL,\n",
    "                    PAROLA           TEXT       NOT NULL\n",
    "                );\n",
    "            '''\n",
    ")\n",
    "nome = \"EmoSN_anger\"\n",
    "for word in resourceAngerEmoSN:\n",
    "    cur.execute(\"INSERT INTO RISORSEANGER (NOME, PAROLA) VALUES(%s, %s)\",(nome,word))\n",
    "\n",
    "nome = \"NRC_anger\"\n",
    "for word in resourceAngerNRC:\n",
    "    cur.execute(\"INSERT INTO RISORSEANGER (NOME, PAROLA) VALUES(%s, %s)\",(nome,word))\n",
    "\n",
    "nome = \"sentisense_anger\"\n",
    "for word in resourceAngerSentisense:\n",
    "    cur.execute(\"INSERT INTO RISORSEANGER (NOME, PAROLA) VALUES(%s, %s)\",(nome,word))\n",
    "\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione tabelle per le parole, emoji, emoticon e hashtag\n",
    "\n",
    "cur.execute('''CREATE TABLE PAROLEANGER\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    PAROLA           TEXT       NOT NULL, \n",
    "                    EmoSN_anger      BOOL       NOT NULL DEFAULT FALSE,\n",
    "                    NRC_anger      BOOL       NOT NULL DEFAULT FALSE,\n",
    "                    sentisense_anger      BOOL       NOT NULL DEFAULT FALSE,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ")   \n",
    "\n",
    "cur.execute('''CREATE TABLE EMOJIANGER\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    EMOJI       TEXT       NOT NULL,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ") \n",
    "\n",
    "cur.execute('''CREATE TABLE EMOTICONSANGER\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    EMOTICONS       TEXT       NOT NULL,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ") \n",
    "\n",
    "cur.execute('''CREATE TABLE HASHTAGANGER\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    HASHTAG     TEXT       NOT NULL,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ") \n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inserimento hashtag, emoticon ed emoji\n",
    "\n",
    "for hashTag in hashtagAnger:\n",
    "    cur.execute(\"INSERT INTO HASHTAGANGER (HASHTAG,FREQUENCY) VALUES(%s,%s)\",(hashTag,0))\n",
    "\n",
    "for emoticons in emoticonAnger:\n",
    "    cur.execute(\"INSERT INTO EMOTICONSANGER (EMOTICONS,FREQUENCY) VALUES(%s,%s)\",(emoticons,0))\n",
    "\n",
    "for emojis in emojiAnger:\n",
    "    cur.execute(\"INSERT INTO EMOJIANGER (EMOJI,FREQUENCY) VALUES(%s,%s)\",(emojis,0))\n",
    "\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inserimento parole in db\n",
    "for word in angerListWord:\n",
    "    cur.execute(\"INSERT INTO PAROLEANGER (PAROLA,EmoSN_anger,NRC_anger,sentisense_anger,FREQUENCY) VALUES(%s,%s,%s,%s,%s)\",(word,False,False,False,0))\n",
    "\n",
    "conn.commit()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze hashtag\n",
    "cur.execute(\"update hashtaganger as ha1 set frequency = (select count(*) from hashtaganger ha2 where ha1.hashtag = ha2.hashtag);\")\n",
    "cur.execute(\"delete from hashtaganger as ha1 where exists (Select 1 From hashtaganger as ha2 where ha1.hashtag = ha2.hashtag and ha1.frequency = ha2.frequency and ha2.ctid > ha1.ctid);\")\n",
    "\n",
    "conn.commit()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze emoji\n",
    "cur.execute(\"update EMOJIANGER as ha1 set frequency = (select count(*) from EMOJIANGER ha2 where ha1.emoji = ha2.emoji);\")\n",
    "cur.execute(\"delete from EMOJIANGER as ha1 where exists (Select 1 From EMOJIANGER as ha2 where ha1.emoji = ha2.emoji and ha1.frequency = ha2.frequency and ha2.ctid > ha1.ctid);\")\n",
    "\n",
    "conn.commit()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze emoticons\n",
    "cur.execute(\"update EMOTICONSANGER as ha1 set frequency = (select count(*) from EMOTICONSANGER ha2 where ha1.emoticons = ha2.emoticons);\")\n",
    "cur.execute(\"delete from EMOTICONSANGER as ha1 where exists (Select 1 From EMOTICONSANGER as ha2 where ha1.emoticons = ha2.emoticons and ha1.frequency = ha2.frequency and ha2.ctid > ha1.ctid);\")\n",
    "\n",
    "conn.commit()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze parole (ci mette più di 100 minuti)\n",
    "\"\"\"\n",
    "cur.execute(\"update PAROLEANGER as ha1 set frequency = (select count(*) from PAROLEANGER ha2 where ha1.parola = ha2.parola);\")\n",
    "cur.execute(\"delete from PAROLEANGER as ha1 where exists (Select 1 From PAROLEANGER as ha2 where ha1.parola = ha2.parola and ha1.frequency = ha2.frequency and ha2.ctid > ha1.ctid);\")\n",
    "\n",
    "conn.commit()  \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze parole\n",
    "cur.execute(\"select count(*),parola from paroleanger group by parola\")\n",
    "result = []\n",
    "for word in cur.fetchall():\n",
    "    result.append(word) \n",
    "\n",
    "cur.execute(\"truncate table paroleanger;\")\n",
    "cur.execute(\"ALTER SEQUENCE paroleanger_id_seq RESTART WITH 1;\")\n",
    "\n",
    "for couple in result:\n",
    "    cur.execute(\"INSERT INTO PAROLEANGER (PAROLA,EmoSN_anger,NRC_anger,sentisense_anger,FREQUENCY) VALUES(%s,%s,%s,%s,%s)\",(couple[1],False,False,False,couple[0]))\n",
    "\n",
    "conn.commit()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggiornamento presenza parola nelle risorse lessicali\n",
    "cur.execute(\"UPDATE paroleanger SET EmoSN_anger = TRUE WHERE parola IN (select p.parola from paroleanger as p LEFT OUTER JOIN risorseanger as r ON p.parola = r.parola WHERE r.nome = 'EmoSN_anger')\")\n",
    "cur.execute(\"UPDATE paroleanger SET NRC_ANGER = TRUE WHERE parola IN (select p.parola from paroleanger as p LEFT OUTER JOIN risorseanger as r ON p.parola = r.parola WHERE r.nome = 'NRC_anger')\")\n",
    "cur.execute(\"UPDATE paroleanger SET sentisense_anger = TRUE WHERE parola IN (select p.parola from paroleanger as p LEFT OUTER JOIN risorseanger as r ON p.parola = r.parola WHERE r.nome = 'sentisense_anger')\")\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvataggio in un txt delle parole non presenti in nessuna risorsa lessicale\n",
    "\n",
    "wordsMongo = angerCollections.find({\"LexResources\": []}, {\"_id\": 0, \"freq\": 0, \"LexResources\": 0})\n",
    "words = []\n",
    "\n",
    "for word in wordsMongo:\n",
    "    words.append(word['word'])\n",
    "\n",
    "with open('Nuove Parole/anger.txt', 'w',encoding=\"utf8\") as f:\n",
    "    for word in words:\n",
    "        f.write(\"%s\\n\" % word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trattamento dataset Anticipation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lista di righe, ogni riga una tweet anticipation\n",
    "file = open(\"Dataset/dataset_dt_anticipation_60k.txt\", 'r',encoding=\"utf8\")\n",
    "anticipation = []\n",
    "hashtagAnticipation = []\n",
    "emoticonAnticipation = []\n",
    "emojiAnticipation = []\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "    riga = re.sub(r'URL', '',riga)\n",
    "    riga = re.sub(r'USERNAME', '',riga)\n",
    "\n",
    "    #array di hashtag singoli\n",
    "    hashTag = (re.findall('#[a-zA-Z0-9]+', riga))\n",
    "\n",
    "    #generiamo l'array di hashtag senza hashtag\n",
    "    for h in hashTag:\n",
    "        hashtagAnticipation.append(h.replace(\"#\",\"\"))\n",
    "\n",
    "    #Rimuoviamo glin hashtag dalla riga\n",
    "    riga = riga.replace('#',\"\")\n",
    "    \n",
    "    anticipation.append(riga)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(anticipation)):\n",
    "    #Controllo se l'emoticons c'è e in caso la rimuovo\n",
    "    for singleEmoticon in emoticonsList:\n",
    "        emoticonsFind = re.findall(re.escape(singleEmoticon),anticipation[i])\n",
    "        if(len(emoticonsFind) != 0):\n",
    "            emoticonAnticipation.extend(emoticonsFind)\n",
    "            anticipation[i] = anticipation[i].replace(singleEmoticon,\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(anticipation)):\n",
    "    #preleviamo le emoji e le rimuoviamo dal testo\n",
    "    data = regex.findall(r'\\X', anticipation[i])\n",
    "    for word in data:\n",
    "        if any(char in emoji.UNICODE_EMOJI['en'] for char in word):\n",
    "            emojiAnticipation.append(word)\n",
    "            anticipation[i] = anticipation[i].replace(word, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processamento slang prima di punteggiatura e lowercase\n",
    "with open(\"Slangs/slangs_before.json\") as f:\n",
    "    slangsBefore = json.load(f)\n",
    "    for i in range(len(anticipation)):\n",
    "        for word in anticipation[i].split():\n",
    "            indexOfWord = anticipation[i].find(word)\n",
    "            if slangsBefore.get(word) is None:\n",
    "                if slangsBefore.get(word.lower()) is None:\n",
    "                    if slangsBefore.get(word.upper()) is None:\n",
    "                        continue\n",
    "                    else:\n",
    "                        anticipation[i] = anticipation[i][:indexOfWord] + slangsBefore.get(word.upper()) + anticipation[i][indexOfWord+len(word):]\n",
    "                else:\n",
    "                    anticipation[i] = anticipation[i][:indexOfWord] + slangsBefore.get(word.lower()) + anticipation[i][indexOfWord+len(word):]\n",
    "            else:\n",
    "                anticipation[i] = anticipation[i][:indexOfWord] + slangsBefore.get(word) + anticipation[i][indexOfWord+len(word):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizzazione di tutte le frasi\n",
    "anticipationTokenized = []\n",
    "for linea in anticipation:\n",
    "    result = word_tokenize(linea)\n",
    "    anticipationTokenized.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Esecuzione della POS tagging per ogni riga della matrice\n",
    "for i in range(len(anticipationTokenized)):\n",
    "    anticipationTokenized[i] = nltk.pos_tag(anticipationTokenized[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trattamento punteggiatura\n",
    "for line in anticipationTokenized:\n",
    "    newLine = line\n",
    "    listToRemove = []\n",
    "    for i in range(len(line)):\n",
    "        if line[i][0] in puntuaction:\n",
    "            listToRemove.append(line[i])\n",
    "    for element in listToRemove:\n",
    "        newLine.remove(element)\n",
    "    line = newLine\n",
    "\n",
    "linePuntuaction = re.compile(\"[\" + re.escape(\"\".join(puntuaction)) + \"]\")\n",
    "for line in anticipationTokenized:\n",
    "    for i in range(len(line)):\n",
    "        tupla = [(re.sub(linePuntuaction, \"\", line[i][0])),line[i][1]]\n",
    "        line[i] = tupla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming\n",
    "#ps = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for line in anticipationTokenized:\n",
    "    for i in range(len(line)):\n",
    "        tupla = [lemmatizer.lemmatize(line[i][0]),line[i][1]]\n",
    "        line[i] = tupla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trattamnento Stop word dopo Stemming\n",
    "stop_words = set(stopwords.words('english'))\n",
    "anticipationTokStoped = []\n",
    "for line in anticipationTokenized:\n",
    "    filtred_word = []\n",
    "    for row in line:\n",
    "        if  row[0] not in stop_words:\n",
    "            filtred_word.append(row) \n",
    "    anticipationTokStoped.append(filtred_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anticipationListWord = []\n",
    "anticipationListTok = []\n",
    "for line in anticipationTokStoped:\n",
    "    for row in line:\n",
    "        if row[0] != '':\n",
    "            anticipationListWord.append(row[0])\n",
    "        anticipationListTok.append(row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lavorazione risorse lessicali\n",
    "resourceNRC_anticipation = []\n",
    "resourceSentisense_anticipation = []\n",
    "#Lista di righe, ogni riga una tweet anger\n",
    "file = open(\"Risorse lessicali\\\\Anticipation\\\\NRC_anticipation.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        resourceNRC_anticipation.append(riga)\n",
    "\n",
    "file.close()\n",
    "\n",
    "file = open(\"Risorse lessicali\\Anticipation\\sentisense_anticipation.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        resourceSentisense_anticipation.append(riga)\n",
    "\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Caricare le risorse lessicali \n",
    "\n",
    "\n",
    "AnticipationNRC = {\n",
    "    '_id': 'NRC_anticipation',\n",
    "    'sentiment': 'anticipation',  \n",
    "    'wordList': resourceNRC_anticipation\n",
    "}\n",
    "\n",
    "AnticipationSentisense = {\n",
    "    '_id': 'sentisense_anticipation',\n",
    "    'sentiment': 'anticipation',\n",
    "    'wordList': resourceSentisense_anticipation\n",
    "}\n",
    "\n",
    "resourcesCollection.insert_one(AnticipationNRC)\n",
    "resourcesCollection.insert_one(AnticipationSentisense)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione database e collezioni\n",
    "# Inserimento singoli documenti contententi i tweet anger\n",
    "wordsString = {}\n",
    "\n",
    "for i in range(len(anticipationListWord)):\n",
    "    wordsString = {\n",
    "        \"lemma\": anticipationListWord[i],\n",
    "        \"POS\": anticipationListTok[i]\n",
    "    }\n",
    "    anticipationTwit={\"word\":wordsString}\n",
    "    anticipationCollections.insert_one(anticipationTwit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inserimento Documento\n",
    "anticipationInsert = {\n",
    "    \"sentiment\":\"anticipation\",\n",
    "    \"hashtags\": hashtagAnticipation,\n",
    "    \"emoticons\": emoticonAnticipation,\n",
    "    \"emoji\": emojiAnticipation\n",
    "}\n",
    "\n",
    "#inserimento\n",
    "x = collectionAnticipation.insert_one(anticipationInsert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raggruppa i documenti di input in base all'espressione _id specificata e per ciascun raggruppamento distinto genera un documento\n",
    "\n",
    "pipeline = [\n",
    "    {\"$group\": {\"_id\": \"$word.lemma\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"word\": \"$_id\",\n",
    "      \"freq\": \"$count\"\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"freq\": -1\n",
    "    }},\n",
    "    {\"$out\": \"anticipation\"}\n",
    "]\n",
    "\n",
    "\n",
    "anticipationCollections.aggregate(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creazione Word Clouds per le parole\n",
    "data = {}\n",
    "for document in anticipationCollections.find().limit(50):\n",
    "    data[document['word']] = document['freq']\n",
    "\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2)\n",
    "wc.fit_words(data)\n",
    "wc.to_file('Word Clouds/Anticipation/anticipationWords.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conteggio degli hashtags\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$hashtags\"},\n",
    "    {\"$group\": {\"_id\": \"$hashtags\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"word\": \"$_id\",\n",
    "      \"count\": 1\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"count\": -1\n",
    "    }}\n",
    "]\n",
    "\n",
    "hashtagsCount = collectionAnticipation.aggregate(pipeline)\n",
    "\n",
    "data = {}\n",
    "for document in hashtagsCount:\n",
    "    data[document['word']] = document['count']\n",
    "\n",
    "collectionAnticipation.update_one(\n",
    "  {\"sentiment\": \"anticipation\"},\n",
    "  {\"$set\": {\"hashtags\": data}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creazione Word Clouds per gli hashtag\n",
    "hashtags = collectionAnticipation.distinct(\"hashtags\")\n",
    "\n",
    "hashtagsLimited = {}\n",
    "for (word,count) in [x for x in hashtags[0].items()][:50]:\n",
    "    hashtagsLimited[word] = count\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2)\n",
    "wc.fit_words(hashtagsLimited)\n",
    "wc.to_file('Word Clouds/Anticipation/anticipationHashtags.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conteggio delle emoji\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$emoji\"},\n",
    "    {\"$group\": {\"_id\": \"$emoji\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"emoji\": \"$_id\",\n",
    "      \"count\": 1\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"count\": -1\n",
    "    }}\n",
    "]\n",
    "\n",
    "emojiCount = collectionAnticipation.aggregate(pipeline)\n",
    "\n",
    "data = {}\n",
    "for document in emojiCount:\n",
    "    data[document['emoji']] = document['count']\n",
    "\n",
    "collectionAnticipation.update_one(\n",
    "  {\"sentiment\": \"anticipation\"},\n",
    "  {\"$set\": {\"emoji\": data}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creazione Word Clouds per le emoji\n",
    "emojis = collectionAnticipation.distinct(\"emoji\")\n",
    "\n",
    "emojisLimited = {}\n",
    "for (word,count) in [x for x in emojis[0].items()][:50]:\n",
    "    emojisLimited[word] = count\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2, font_path='Font Emoji\\Symbola.otf')\n",
    "wc.fit_words(emojisLimited)\n",
    "wc.to_file('Word Clouds/Anticipation/anticipationEmojis.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conteggio degli emoticons\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$emoticons\"},\n",
    "    {\"$group\": {\"_id\": \"$emoticons\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"emoticons\": \"$_id\",\n",
    "      \"count\": 1\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"count\": -1\n",
    "    }}\n",
    "]\n",
    "\n",
    "emoticonsCount = collectionAnticipation.aggregate(pipeline)\n",
    "\n",
    "data = {}\n",
    "for document in emoticonsCount:\n",
    "    data[document['emoticons']] = document['count']\n",
    "\n",
    "collectionAnticipation.update_one(\n",
    "  {\"sentiment\": \"anticipation\"},\n",
    "  {\"$set\": {\"emoticons\": data}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creazione Word Clouds per le emoticons\n",
    "emoticons = collectionAnticipation.distinct(\"emoticons\")\n",
    "\n",
    "emoticonsLimited = {}\n",
    "for (word,count) in [x for x in emoticons[0].items()][:50]:\n",
    "    emoticonsLimited[word] = count\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2)\n",
    "wc.fit_words(emoticonsLimited)\n",
    "wc.to_file('Word Clouds/Anticipation/anticipationEmoticons.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline di aggregazione per inserire la risorsa lessicale a cui appartiene la parola\n",
    "pipeline = [\n",
    "    {\"$lookup\": {\n",
    "        \"from\": \"Resources\",\n",
    "        \"localField\": \"word\",\n",
    "        \"foreignField\": \"wordList\",\n",
    "        \"pipeline\": [\n",
    "            {\"$match\": {\"$expr\": {\"$regexMatch\": {\"input\": \"$_id\", \"regex\": \"anticipation\"}}}}\n",
    "        ],\n",
    "        \"as\": \"LexResources\"\n",
    "    }},\n",
    "    {\"$project\": {\n",
    "        \"resources.wordList\": 0,\n",
    "        \"resources.totNumberWords\": 0,\n",
    "        \"resources.sentiment\": 0,\n",
    "        \"LexResources.sentiment\": 0,\n",
    "        \"LexResources.totNumberWords\": 0,\n",
    "        \"LexResources.wordList\": 0\n",
    "    }},\n",
    "    {\"$out\": \"anticipation\"}\n",
    "]\n",
    "\n",
    "anticipationCollections.aggregate(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline di aggregazione per calcolare le percentuali richieste\n",
    "\n",
    "pipelineCount = [\n",
    "    {\"$group\": {\"_id\": \"null\", \"count\": { \"$sum\": \"$freq\" }}},\n",
    "    {\"$project\": {\n",
    "        \"_id\": 0,\n",
    "        \"count\": \"$count\"\n",
    "    }}\n",
    "]\n",
    "\n",
    "results = anticipationCollections.aggregate(pipelineCount)\n",
    "\n",
    "countWords = 0\n",
    "for data in results:\n",
    "    countWords = data['count']\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$LexResources\"},\n",
    "    {\"$lookup\": {\n",
    "        \"from\": \"Resources\",\n",
    "        \"localField\": \"LexResources._id\",\n",
    "        \"foreignField\": \"_id\",\n",
    "        \"as\": \"RefResource\"\n",
    "    }},\n",
    "    {\"$group\": {\"_id\": { \"lex\": \"$LexResources._id\", \"RefResource\": \"$RefResource\" }, \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "        \"_id\": \"$_id.lex\",\n",
    "        \"wordList\": \"$_id.RefResource.wordList\",\n",
    "        \"commonWords\": \"$count\"\n",
    "    }},\n",
    "    {\"$unwind\": \"$wordList\"},\n",
    "    {\"$project\": {\n",
    "        \"_id\": 1,\n",
    "        \"commonWords\": 1,\n",
    "        \"totWordsLex\": {\"$size\": \"$wordList\"}\n",
    "    }},\n",
    "    {\"$addFields\": {\"totalWordsTwitter\": countWords}},\n",
    "    {\"$project\": {\n",
    "        \"perc_presence_lex_res\": { \"$round\": [{\"$divide\": [ \"$commonWords\", \"$totWordsLex\" ] }, 6] },\n",
    "        \"perc_presence_twitter\": { \"$round\": [{\"$divide\": [ \"$commonWords\", \"$totalWordsTwitter\" ]}, 6] }\n",
    "    }}\n",
    "]\n",
    "\n",
    "results = anticipationCollections.aggregate(pipeline)\n",
    "\n",
    "# Genero i dati per l'istogramma\n",
    "istogramAnticipation_lexRes = []\n",
    "istogramAnticipation_twitter = []\n",
    "\n",
    "for data in results:\n",
    "    print(data)\n",
    "    log =math.log(data['perc_presence_lex_res'],10)\n",
    "    istogramAnticipation_lexRes.append([data['_id'], data['perc_presence_lex_res']])\n",
    "    log =math.log(data['perc_presence_twitter'],10)\n",
    "    istogramAnticipation_twitter.append([data['_id'], data['perc_presence_twitter']])\n",
    " \n",
    "# Create the pandas DataFrame\n",
    "dfAnticipation_lexRes = pd.DataFrame(istogramAnticipation_lexRes, columns = ['Risorsa', 'Percentuale'])\n",
    "dfAnticipation_twitter = pd.DataFrame(istogramAnticipation_twitter,columns = ['Risorsa', 'Percentuale'])      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione istogramma per presenza parole nei twitter\n",
    "\n",
    "dfAnticipation_lexRes.plot.bar(x='Risorsa')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione istogramma per presenza parole nei twitter\n",
    "\n",
    "dfAnticipation_twitter.plot.bar(x='Risorsa')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connessione, creazione cursore e database\n",
    "conn = psycopg2.connect(database = \"maadb\", user = \"postgres\", password = \"admin\", host = \"localhost\", port = \"5432\")\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione tabella risorse lessicali anticipation e inserimento\n",
    "cur.execute('''CREATE TABLE RISORSEANTICIPATION\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    NOME             TEXT       NOT NULL,\n",
    "                    PAROLA           TEXT       NOT NULL\n",
    "                );\n",
    "            '''\n",
    ")\n",
    "nome = \"NRC_anticipation\"\n",
    "for word in resourceNRC_anticipation:\n",
    "    cur.execute(\"INSERT INTO RISORSEANTICIPATION (NOME, PAROLA) VALUES(%s, %s)\",(nome,word))\n",
    "\n",
    "nome = \"sentisense_anticipation\"\n",
    "for word in resourceSentisense_anticipation:\n",
    "    cur.execute(\"INSERT INTO RISORSEANTICIPATION (NOME, PAROLA) VALUES(%s, %s)\",(nome,word))\n",
    "\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione tabelle per le parole, emoji, emoticon e hashtag\n",
    "\n",
    "cur.execute('''CREATE TABLE PAROLEANTICIPATION\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    PAROLA           TEXT       NOT NULL, \n",
    "                    NRC_anticipation      BOOL       NOT NULL DEFAULT FALSE,\n",
    "                    sentisense_anticipation      BOOL       NOT NULL DEFAULT FALSE,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ")   \n",
    "\n",
    "cur.execute('''CREATE TABLE EMOJIANTICIPATION\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    EMOJI       TEXT       NOT NULL,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ") \n",
    "\n",
    "cur.execute('''CREATE TABLE EMOTICONSANTICIPATION\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    EMOTICONS       TEXT       NOT NULL,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ") \n",
    "\n",
    "cur.execute('''CREATE TABLE HASHTAGANTICIPATION\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    HASHTAG     TEXT       NOT NULL,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ") \n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inserimento hashtag, emoticon ed emoji\n",
    "\n",
    "for hashTag in hashtagAnticipation:\n",
    "    cur.execute(\"INSERT INTO HASHTAGANTICIPATION (HASHTAG,FREQUENCY) VALUES(%s,%s)\",(hashTag,0))\n",
    "\n",
    "for emoticons in emoticonAnticipation:\n",
    "    cur.execute(\"INSERT INTO EMOTICONSANTICIPATION (EMOTICONS,FREQUENCY) VALUES(%s,%s)\",(emoticons,0))\n",
    "\n",
    "for emojis in emojiAnticipation:\n",
    "    cur.execute(\"INSERT INTO EMOJIANTICIPATION (EMOJI,FREQUENCY) VALUES(%s,%s)\",(emojis,0))\n",
    "\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inserimento parole in db\n",
    "for word in anticipationListWord:\n",
    "    cur.execute(\"INSERT INTO PAROLEANTICIPATION (PAROLA,NRC_anticipation,sentisense_anticipation,FREQUENCY) VALUES(%s,%s,%s,%s)\",(word,False,False,0))\n",
    "\n",
    "conn.commit()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze hashtag\n",
    "cur.execute(\"update HASHTAGANTICIPATION as ha1 set frequency = (select count(*) from HASHTAGANTICIPATION ha2 where ha1.hashtag = ha2.hashtag);\")\n",
    "cur.execute(\"delete from HASHTAGANTICIPATION as ha1 where exists (Select 1 From HASHTAGANTICIPATION as ha2 where ha1.hashtag = ha2.hashtag and ha1.frequency = ha2.frequency and ha2.ctid > ha1.ctid);\")\n",
    "\n",
    "conn.commit()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze emoji\n",
    "cur.execute(\"update EMOJIANTICIPATION as ha1 set frequency = (select count(*) from EMOJIANTICIPATION ha2 where ha1.emoji = ha2.emoji);\")\n",
    "cur.execute(\"delete from EMOJIANTICIPATION as ha1 where exists (Select 1 From EMOJIANTICIPATION as ha2 where ha1.emoji = ha2.emoji and ha1.frequency = ha2.frequency and ha2.ctid > ha1.ctid);\")\n",
    "\n",
    "conn.commit()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze emoticons\n",
    "cur.execute(\"update EMOTICONSANTICIPATION as ha1 set frequency = (select count(*) from EMOTICONSANTICIPATION ha2 where ha1.emoticons = ha2.emoticons);\")\n",
    "cur.execute(\"delete from EMOTICONSANTICIPATION as ha1 where exists (Select 1 From EMOTICONSANTICIPATION as ha2 where ha1.emoticons = ha2.emoticons and ha1.frequency = ha2.frequency and ha2.ctid > ha1.ctid);\")\n",
    "\n",
    "conn.commit()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze parole\n",
    "cur.execute(\"select count(*),parola from PAROLEANTICIPATION group by parola\")\n",
    "result = []\n",
    "for word in cur.fetchall():\n",
    "    result.append(word) \n",
    "\n",
    "cur.execute(\"truncate table PAROLEANTICIPATION;\")\n",
    "cur.execute(\"ALTER SEQUENCE paroleanticipation_id_seq RESTART WITH 1;\")\n",
    "\n",
    "for couple in result:\n",
    "    cur.execute(\"INSERT INTO PAROLEANTICIPATION (PAROLA,NRC_anticipation,sentisense_anticipation,FREQUENCY) VALUES(%s,%s,%s,%s)\",(couple[1],False,False,couple[0]))\n",
    "\n",
    "conn.commit()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggiornamento presenza parola nelle risorse lessicali\n",
    "cur.execute(\"UPDATE PAROLEANTICIPATION SET NRC_anticipation = TRUE WHERE parola IN (select p.parola from PAROLEANTICIPATION as p LEFT OUTER JOIN RISORSEANTICIPATION as r ON p.parola = r.parola WHERE r.nome = 'NRC_anticipation')\")\n",
    "cur.execute(\"UPDATE PAROLEANTICIPATION SET sentisense_anticipation = TRUE WHERE parola IN (select p.parola from PAROLEANTICIPATION as p LEFT OUTER JOIN RISORSEANTICIPATION as r ON p.parola = r.parola WHERE r.nome = 'sentisense_anticipation')\")\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvataggio in un txt delle parole non presenti in nessuna risorsa lessicale\n",
    "wordsMongo = anticipationCollections.find({\"LexResources\": []}, {\"_id\": 0, \"freq\": 0, \"LexResources\": 0})\n",
    "words = []\n",
    "\n",
    "for word in wordsMongo:\n",
    "    words.append(word['word'])\n",
    "\n",
    "with open('Nuove Parole/anticipation.txt', 'w',encoding=\"utf8\") as f:\n",
    "    for word in words:\n",
    "        f.write(\"%s\\n\" % word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trattamento dataset Disgust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lista di righe, ogni riga una tweet disgust\n",
    "file = open(\"Dataset/dataset_dt_disgust_60k.txt\", 'r',encoding=\"utf8\")\n",
    "disgust = []\n",
    "hashtagDisgust = []\n",
    "emoticonDisgust = []\n",
    "emojiDisgust = []\n",
    "\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "    riga = re.sub(r'URL', '',riga)\n",
    "    riga = re.sub(r'USERNAME', '',riga)\n",
    "\n",
    "    #array di hashtag singoli\n",
    "    hashTag = (re.findall('#[a-zA-Z0-9]+', riga))\n",
    "    \n",
    "    #generiamo l'array di hashtag senza hashtag\n",
    "    for h in hashTag:\n",
    "        hashtagDisgust.append(h.replace(\"#\",\"\"))\n",
    "\n",
    "    #Rimuoviamo glin hashtag dalla riga\n",
    "    riga = riga.replace('#',\"\")\n",
    "\n",
    "    disgust.append(riga)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(disgust)):\n",
    "    #Controllo se l'emoticons c'è e in caso la rimuovo\n",
    "    for singleEmoticon in emoticonsList:\n",
    "        emoticonsFind = re.findall(re.escape(singleEmoticon),disgust[i])\n",
    "        if(len(emoticonsFind) != 0):\n",
    "            emoticonDisgust.extend(emoticonsFind)\n",
    "            disgust[i] = disgust[i].replace(singleEmoticon,\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(disgust)):\n",
    "    #preleviamo le emoji e le rimuoviamo dal testo\n",
    "    data = regex.findall(r'\\X', disgust[i])\n",
    "    for word in data:\n",
    "        if any(char in emoji.UNICODE_EMOJI['en'] for char in word):\n",
    "            emojiDisgust.append(word)\n",
    "            disgust[i] = disgust[i].replace(word, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processamento slang prima di punteggiatura e lowercase\n",
    "with open(\"Slangs/slangs_before.json\") as f:\n",
    "    slangsBefore = json.load(f)\n",
    "    for i in range(len(disgust)):\n",
    "        for word in disgust[i].split():\n",
    "            indexOfWord = disgust[i].find(word)\n",
    "            if slangsBefore.get(word) is None:\n",
    "                if slangsBefore.get(word.lower()) is None:\n",
    "                    if slangsBefore.get(word.upper()) is None:\n",
    "                        continue\n",
    "                    else:\n",
    "                        disgust[i] = disgust[i][:indexOfWord] + slangsBefore.get(word.upper()) + disgust[i][indexOfWord+len(word):]\n",
    "                else:\n",
    "                    disgust[i] = disgust[i][:indexOfWord] + slangsBefore.get(word.lower()) + disgust[i][indexOfWord+len(word):]\n",
    "            else:\n",
    "                disgust[i] = disgust[i][:indexOfWord] + slangsBefore.get(word) + disgust[i][indexOfWord+len(word):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizzazione di tutte le frasi\n",
    "disgustTokenized = []\n",
    "for linea in disgust:\n",
    "    result = word_tokenize(linea)\n",
    "    disgustTokenized.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Esecuzione della POS tagging per ogni riga della matrice\n",
    "for i in range(len(disgustTokenized)):\n",
    "    disgustTokenized[i] = nltk.pos_tag(disgustTokenized[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trattamento punteggiatura\n",
    "for line in disgustTokenized:\n",
    "    newLine = line\n",
    "    listToRemove = []\n",
    "    for i in range(len(line)):\n",
    "        if line[i][0] in puntuaction:\n",
    "            listToRemove.append(line[i])\n",
    "    for element in listToRemove:\n",
    "        newLine.remove(element)\n",
    "    line = newLine\n",
    "\n",
    "linePuntuaction = re.compile(\"[\" + re.escape(\"\".join(puntuaction)) + \"]\")\n",
    "for line in disgustTokenized:\n",
    "    for i in range(len(line)):\n",
    "        tupla = [(re.sub(linePuntuaction, \"\", line[i][0])),line[i][1]]\n",
    "        line[i] = tupla\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming\n",
    "#ps = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for line in disgustTokenized:\n",
    "    for i in range(len(line)):\n",
    "        tupla = [lemmatizer.lemmatize(line[i][0]),line[i][1]]\n",
    "        line[i] = tupla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trattamnento Stop word dopo Stemming\n",
    "stop_words = set(stopwords.words('english'))\n",
    "disgustTokStoped = []\n",
    "for line in disgustTokenized:\n",
    "    filtred_word = []\n",
    "    for row in line:\n",
    "        if  row[0] not in stop_words:\n",
    "            filtred_word.append(row) \n",
    "    disgustTokStoped.append(filtred_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disgustListWord = []\n",
    "disgustListTok = []\n",
    "for line in disgustTokStoped:\n",
    "    for row in line:\n",
    "        if row[0] != '':\n",
    "            disgustListWord.append(row[0])\n",
    "        disgustListTok.append(row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lavorazione risorse lessicali\n",
    "resourceDisgustSentisense_disgust = []\n",
    "resourceDisgustNRC_disgust = []\n",
    "resourceDisgustSentisense_hate = []\n",
    "#Lista di righe, ogni riga una tweet disgust\n",
    "file = open(\"Risorse lessicali\\Disgust-Hate\\sentisense_disgust.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        resourceDisgustSentisense_disgust.append(riga)\n",
    "\n",
    "file.close()\n",
    "\n",
    "file = open(\"C:Risorse lessicali\\\\Disgust-Hate\\\\NRC_disgust.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        resourceDisgustNRC_disgust.append(riga)\n",
    "\n",
    "file.close()\n",
    "\n",
    "file = open(\"Risorse lessicali\\Disgust-Hate\\sentisense_hate.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        resourceDisgustSentisense_hate.append(riga)\n",
    "\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Caricare le risorse lessicali \n",
    "\n",
    "DisgustSentisense = {\n",
    "    '_id': 'Sentisense_disgust',\n",
    "    'sentiment': 'disgust', \n",
    "    'wordList': resourceDisgustSentisense_disgust\n",
    "}\n",
    "\n",
    "DisgustNRC_disgust = {\n",
    "    '_id': 'NRC_disgust',\n",
    "    'sentiment': 'disgust',  \n",
    "    'wordList': resourceDisgustNRC_disgust\n",
    "}\n",
    "\n",
    "DisgustSentisense_hate = {\n",
    "    '_id': 'Sentisense_hatesense',\n",
    "    'sentiment': 'disgust',\n",
    "    'wordList': resourceDisgustSentisense_hate\n",
    "}\n",
    "\n",
    "resourcesCollection.insert_one(DisgustSentisense)\n",
    "resourcesCollection.insert_one(DisgustNRC_disgust)\n",
    "resourcesCollection.insert_one(DisgustSentisense_hate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione database e collezioni\n",
    "# Inserimento singoli documenti contententi i tweet disgust\n",
    "wordsString = {}\n",
    "\n",
    "for i in range(len(disgustListWord)):\n",
    "    wordsString = {\n",
    "        \"lemma\": disgustListWord[i],\n",
    "        \"POS\": disgustListTok[i]\n",
    "    }\n",
    "    disgustTwit={\"word\":wordsString}\n",
    "    disgustCollections.insert_one(disgustTwit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.InsertOneResult at 0x1b1635bf3a0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Inserimento collezione Twitter con hashtags, emoticons ed emoji \n",
    "disgustInsert = {\n",
    "    \"sentiment\":\"disgust\",\n",
    "    \"hashtags\": hashtagDisgust,\n",
    "    \"emoticons\": emoticonDisgust,\n",
    "    \"emoji\": emojiDisgust\n",
    "}\n",
    "\n",
    "#inserimento\n",
    "collectionDisgust.insert_one(disgustInsert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raggruppa i documenti di input in base all'espressione _id specificata e per ciascun raggruppamento distinto genera un documento\n",
    "pipeline = [\n",
    "    {\"$group\": {\"_id\": \"$word.lemma\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"word\": \"$_id\",\n",
    "      \"freq\": \"$count\"\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"freq\": -1\n",
    "    }},\n",
    "    {\"$out\": \"disgust\"}\n",
    "]\n",
    "\n",
    "#post processing per renderlo valido \n",
    "disgustCollections.aggregate(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creazione Word Clouds per le parole\n",
    "data = {}\n",
    "for document in disgustCollections.find().limit(50):\n",
    "    data[document['word']] = document['freq']\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2)\n",
    "wc.fit_words(data)\n",
    "wc.to_file('Word Clouds/Disgust/disgustWords.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conteggio degli hashtags\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$hashtags\"},\n",
    "    {\"$group\": {\"_id\": \"$hashtags\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"word\": \"$_id\",\n",
    "      \"count\": 1\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"count\": -1\n",
    "    }}\n",
    "]\n",
    "\n",
    "hashtagsCount = collectionDisgust.aggregate(pipeline)\n",
    "\n",
    "data = {}\n",
    "for document in hashtagsCount:\n",
    "    data[document['word']] = document['count']\n",
    "\n",
    "collectionDisgust.update_one(\n",
    "  {\"sentiment\": \"disgust\"},\n",
    "  {\"$set\": {\"hashtags\": data}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creazione Word Clouds per gli hashtags\n",
    "\n",
    "hashtags = collectionDisgust.distinct(\"hashtags\")\n",
    "\n",
    "hashtagsLimited = {}\n",
    "for (word,count) in [x for x in hashtags[0].items()][:50]:\n",
    "    hashtagsLimited[word] = count\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2)\n",
    "wc.fit_words(hashtagsLimited)\n",
    "wc.to_file('Word Clouds/Disgust/disgustHashtags.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conteggio delle emoji\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$emoji\"},\n",
    "    {\"$group\": {\"_id\": \"$emoji\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"emoji\": \"$_id\",\n",
    "      \"count\": 1\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"count\": -1\n",
    "    }}\n",
    "]\n",
    "\n",
    "emojiCount = collectionDisgust.aggregate(pipeline)\n",
    "\n",
    "data = {}\n",
    "for document in emojiCount:\n",
    "    data[document['emoji']] = document['count']\n",
    "\n",
    "collectionDisgust.update_one(\n",
    "  {\"sentiment\": \"disgust\"},\n",
    "  {\"$set\": {\"emoji\": data}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creazione Word Clouds per le emoji\n",
    "\n",
    "emojis = collectionDisgust.distinct(\"emoji\")\n",
    "\n",
    "emojisLimited = {}\n",
    "for (word,count) in [x for x in emojis[0].items()][:50]:\n",
    "    emojisLimited[word] = count\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2, font_path='Font Emoji\\Symbola.otf')\n",
    "wc.fit_words(emojisLimited)\n",
    "wc.to_file('Word Clouds/Disgust/disgustEmojis.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.UpdateResult at 0x1b1636093a0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Conteggio degli emoticons\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$emoticons\"},\n",
    "    {\"$group\": {\"_id\": \"$emoticons\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"emoticons\": \"$_id\",\n",
    "      \"count\": 1\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"count\": -1\n",
    "    }}\n",
    "]\n",
    "\n",
    "emoticonsCount = collectionDisgust.aggregate(pipeline)\n",
    "\n",
    "data = {}\n",
    "for document in emoticonsCount:\n",
    "  data[document['emoticons']] = document['count']\n",
    "\n",
    "collectionDisgust.update_one(\n",
    "  {\"sentiment\": \"disgust\"},\n",
    "  {\"$set\": {\"emoticons\": data}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<wordcloud.wordcloud.WordCloud at 0x1b16360c250>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creazione Word Clouds per le emoticons\n",
    "emoticons = collectionDisgust.distinct(\"emoticons\")\n",
    "\n",
    "emoticonsLimited = {}\n",
    "for (word,count) in [x for x in emoticons[0].items()][:50]:\n",
    "    emoticonsLimited[word] = count\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2)\n",
    "wc.fit_words(emoticonsLimited)\n",
    "wc.to_file('Word Clouds/Disgust/disgustEmoticons.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline di aggregazione per inserire la risorsa lessicale a cui appartiene la parola\n",
    "\n",
    "pipeline = [\n",
    "    {\"$lookup\": {\n",
    "        \"from\": \"Resources\",\n",
    "        \"localField\": \"word\",\n",
    "        \"foreignField\": \"wordList\",\n",
    "        \"pipeline\": [\n",
    "            {\"$match\": {\"$expr\": {\"$regexMatch\": {\"input\": \"$_id\", \"regex\": \"disgust\"}}}}\n",
    "        ],\n",
    "        \"as\": \"LexResources\"\n",
    "    }},\n",
    "    {\"$project\": {\n",
    "        \"resources.wordList\": 0,\n",
    "        \"resources.totNumberWords\": 0,\n",
    "        \"resources.sentiment\": 0,\n",
    "        \"LexResources.sentiment\": 0,\n",
    "        \"LexResources.totNumberWords\": 0,\n",
    "        \"LexResources.wordList\": 0\n",
    "    }},\n",
    "    {\"$out\": \"disgust\"}\n",
    "]\n",
    "\n",
    "disgustCollections.aggregate(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline di aggregazione per calcolare le percentuali richieste\n",
    "\n",
    "pipelineCount = [\n",
    "    {\"$group\": {\"_id\": \"null\", \"count\": { \"$sum\": \"$freq\" }}},\n",
    "    {\"$project\": {\n",
    "        \"_id\": 0,\n",
    "        \"count\": \"$count\"\n",
    "    }}\n",
    "]\n",
    "\n",
    "results = disgustCollections.aggregate(pipelineCount)\n",
    "\n",
    "countWords = 0\n",
    "for data in results:\n",
    "    countWords = data['count']\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$LexResources\"},\n",
    "    {\"$lookup\": {\n",
    "        \"from\": \"Resources\",\n",
    "        \"localField\": \"LexResources._id\",\n",
    "        \"foreignField\": \"_id\",\n",
    "        \"as\": \"RefResource\"\n",
    "    }},\n",
    "    {\"$group\": {\"_id\": { \"lex\": \"$LexResources._id\", \"RefResource\": \"$RefResource\" }, \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "        \"_id\": \"$_id.lex\",\n",
    "        \"wordList\": \"$_id.RefResource.wordList\",\n",
    "        \"commonWords\": \"$count\"\n",
    "    }},\n",
    "    {\"$unwind\": \"$wordList\"},\n",
    "    {\"$project\": {\n",
    "        \"_id\": 1,\n",
    "        \"commonWords\": 1,\n",
    "        \"totWordsLex\": {\"$size\": \"$wordList\"}\n",
    "    }},\n",
    "    {\"$addFields\": {\"totalWordsTwitter\": countWords}},\n",
    "    {\"$project\": {\n",
    "        \"perc_presence_lex_res\": { \"$round\": [{\"$divide\": [ \"$commonWords\", \"$totWordsLex\" ] }, 6] },\n",
    "        \"perc_presence_twitter\": { \"$round\": [{\"$divide\": [ \"$commonWords\", \"$totalWordsTwitter\" ]}, 6] }\n",
    "    }}\n",
    "]\n",
    "\n",
    "results = disgustCollections.aggregate(pipeline)\n",
    "\n",
    "# Genero i dati per l'istogramma\n",
    "istogramDisgust_lexRes = []\n",
    "istogramDisgust_twitter = []\n",
    "\n",
    "for data in results:\n",
    "    print(data)\n",
    "    log =math.log(data['perc_presence_lex_res'],10)\n",
    "    istogramDisgust_lexRes.append([data['_id'], data['perc_presence_lex_res']])\n",
    "    log =math.log(data['perc_presence_twitter'],10)\n",
    "    istogramDisgust_twitter.append([data['_id'], data['perc_presence_twitter']])\n",
    " \n",
    "# Create the pandas DataFrame\n",
    "dfDisgust_lexRes = pd.DataFrame(istogramDisgust_lexRes, columns = ['Risorsa', 'Percentuale'])\n",
    "dfDisgust_twitter = pd.DataFrame(istogramDisgust_twitter,columns = ['Risorsa', 'Percentuale'])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione istogramma per presenza parole nelle risorse lessicali\n",
    "\n",
    "dfDisgust_lexRes.plot.bar(x='Risorsa')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione istogramma per presenza parole nei twitter\n",
    "\n",
    "dfDisgust_twitter.plot.bar(x='Risorsa')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connessione, creazione cursore e database\n",
    "\n",
    "conn = psycopg2.connect(database = \"maadb\", user = \"postgres\", password = \"admin\", host = \"localhost\", port = \"5432\")\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione tabella risorse lessicali disgust e inserimento\n",
    "cur.execute('''CREATE TABLE RISORSEDISGUST\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    NOME             TEXT       NOT NULL,\n",
    "                    PAROLA           TEXT       NOT NULL\n",
    "                );\n",
    "            '''\n",
    ")\n",
    "nome = \"NRC_disgust\"\n",
    "for word in resourceDisgustNRC_disgust:\n",
    "    cur.execute(\"INSERT INTO RISORSEDISGUST (NOME, PAROLA) VALUES(%s, %s)\",(nome,word))\n",
    "\n",
    "nome = \"sentisense_disgust\"\n",
    "for word in resourceDisgustSentisense_disgust:\n",
    "    cur.execute(\"INSERT INTO RISORSEDISGUST (NOME, PAROLA) VALUES(%s, %s)\",(nome,word))\n",
    "\n",
    "nome = \"sentisense_hate\"\n",
    "for word in resourceDisgustSentisense_hate:\n",
    "    cur.execute(\"INSERT INTO RISORSEDISGUST (NOME, PAROLA) VALUES(%s, %s)\",(nome,word))\n",
    "\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione tabelle per le parole, emoji, emoticon e hashtag\n",
    "\n",
    "cur.execute('''CREATE TABLE PAROLEDISGUST\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    PAROLA           TEXT       NOT NULL, \n",
    "                    NRC_disgust      BOOL       NOT NULL DEFAULT FALSE,\n",
    "                    sentisense_disgust      BOOL       NOT NULL DEFAULT FALSE,\n",
    "                    sentisense_hate      BOOL       NOT NULL DEFAULT FALSE,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ")   \n",
    "\n",
    "cur.execute('''CREATE TABLE EMOJIDISGUST\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    EMOJI       TEXT       NOT NULL,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ") \n",
    "\n",
    "cur.execute('''CREATE TABLE EMOTICONSDISGUST\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    EMOTICONS       TEXT       NOT NULL,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ") \n",
    "\n",
    "cur.execute('''CREATE TABLE HASHTAGDISGUST\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    HASHTAG     TEXT       NOT NULL,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ") \n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inserimento hashtag, emoticon ed emoji\n",
    "\n",
    "for hashTag in hashtagDisgust:\n",
    "    cur.execute(\"INSERT INTO HASHTAGDISGUST (HASHTAG,FREQUENCY) VALUES(%s,%s)\",(hashTag,0))\n",
    "\n",
    "for emoticons in emoticonDisgust:\n",
    "    cur.execute(\"INSERT INTO EMOTICONSDISGUST (EMOTICONS,FREQUENCY) VALUES(%s,%s)\",(emoticons,0))\n",
    "\n",
    "for emojis in emojiDisgust:\n",
    "    cur.execute(\"INSERT INTO EMOJIDISGUST (EMOJI,FREQUENCY) VALUES(%s,%s)\",(emojis,0))\n",
    "\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inserimento parole in db\n",
    "for word in disgustListWord:\n",
    "    cur.execute(\"INSERT INTO PAROLEDISGUST (PAROLA,NRC_disgust,sentisense_disgust,sentisense_hate,FREQUENCY) VALUES(%s,%s,%s,%s,%s)\",(word,False,False,False,0))\n",
    "\n",
    "conn.commit()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze hashtag\n",
    "cur.execute(\"update HASHTAGDISGUST as ha1 set frequency = (select count(*) from HASHTAGDISGUST ha2 where ha1.hashtag = ha2.hashtag);\")\n",
    "cur.execute(\"delete from HASHTAGDISGUST as ha1 where exists (Select 1 From HASHTAGDISGUST as ha2 where ha1.hashtag = ha2.hashtag and ha1.frequency = ha2.frequency and ha2.ctid > ha1.ctid);\")\n",
    "\n",
    "conn.commit()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze emoji\n",
    "cur.execute(\"update EMOJIDISGUST as ha1 set frequency = (select count(*) from EMOJIDISGUST ha2 where ha1.emoji = ha2.emoji);\")\n",
    "cur.execute(\"delete from EMOJIDISGUST as ha1 where exists (Select 1 From EMOJIDISGUST as ha2 where ha1.emoji = ha2.emoji and ha1.frequency = ha2.frequency and ha2.ctid > ha1.ctid);\")\n",
    "\n",
    "conn.commit()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze emoticons\n",
    "cur.execute(\"update EMOTICONSDISGUST as ha1 set frequency = (select count(*) from EMOTICONSDISGUST ha2 where ha1.emoticons = ha2.emoticons);\")\n",
    "cur.execute(\"delete from EMOTICONSDISGUST as ha1 where exists (Select 1 From EMOTICONSDISGUST as ha2 where ha1.emoticons = ha2.emoticons and ha1.frequency = ha2.frequency and ha2.ctid > ha1.ctid);\")\n",
    "\n",
    "conn.commit()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze parole\n",
    "cur.execute(\"select count(*),parola from PAROLEDISGUST group by parola\")\n",
    "result = []\n",
    "for word in cur.fetchall():\n",
    "    result.append(word) \n",
    "\n",
    "cur.execute(\"truncate table PAROLEDISGUST;\")\n",
    "cur.execute(\"ALTER SEQUENCE paroledisgust_id_seq RESTART WITH 1;\")\n",
    "\n",
    "for couple in result:\n",
    "    cur.execute(\"INSERT INTO PAROLEDISGUST (PAROLA,NRC_disgust,sentisense_disgust,sentisense_hate,FREQUENCY) VALUES(%s,%s,%s,%s,%s)\",(couple[1],False,False,False,couple[0]))\n",
    "\n",
    "conn.commit()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggiornamento presenza parola nelle risorse lessicali\n",
    "cur.execute(\"UPDATE PAROLEDISGUST SET NRC_disgust = TRUE WHERE parola IN (select p.parola from PAROLEDISGUST as p LEFT OUTER JOIN RISORSEDISGUST as r ON p.parola = r.parola WHERE r.nome = 'NRC_disgust')\")\n",
    "cur.execute(\"UPDATE PAROLEDISGUST SET sentisense_disgust = TRUE WHERE parola IN (select p.parola from PAROLEDISGUST as p LEFT OUTER JOIN RISORSEDISGUST as r ON p.parola = r.parola WHERE r.nome = 'sentisense_disgust')\")\n",
    "cur.execute(\"UPDATE PAROLEDISGUST SET sentisense_hate = TRUE WHERE parola IN (select p.parola from PAROLEDISGUST as p LEFT OUTER JOIN RISORSEDISGUST as r ON p.parola = r.parola WHERE r.nome = 'sentisense_hate')\")\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvataggio in un txt delle parole non presenti in nessuna risorsa lessicale\n",
    "\n",
    "wordsMongo = disgustCollections.find({\"LexResources\": []}, {\"_id\": 0, \"freq\": 0, \"LexResources\": 0})\n",
    "words = []\n",
    "\n",
    "for word in wordsMongo:\n",
    "    words.append(word['word'])\n",
    "\n",
    "with open('Nuove Parole/disgust.txt', 'w',encoding=\"utf8\") as f:\n",
    "    for word in words:\n",
    "        f.write(\"%s\\n\" % word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trattamento dataset Fear "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lista di righe, ogni riga una tweet fear\n",
    "file = open(\"Dataset/dataset_dt_fear_60k.txt\", 'r',encoding=\"utf8\")\n",
    "fear = []\n",
    "hashtagFear = []\n",
    "emoticonFear = []\n",
    "emojiFear = []\n",
    "\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "    riga = re.sub(r'URL', '',riga)\n",
    "    riga = re.sub(r'USERNAME', '',riga)\n",
    "\n",
    "     #array di hashtag singoli\n",
    "    hashTag = (re.findall('#[a-zA-Z0-9]+', riga))\n",
    "\n",
    "    #generiamo l'array di hashtag senza hashtag\n",
    "    for h in hashTag:\n",
    "        hashtagFear.append(h.replace(\"#\",\"\"))\n",
    "\n",
    "    #Rimuoviamo glin hashtag dalla riga\n",
    "    riga = riga.replace('#',\"\")\n",
    "\n",
    "    fear.append(riga)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(fear)):\n",
    "    #Controllo se l'emoticons c'è e in caso la rimuovo\n",
    "    for singleEmoticon in emoticonsList:\n",
    "        emoticonsFind = re.findall(re.escape(singleEmoticon),fear[i])\n",
    "        if(len(emoticonsFind) != 0):\n",
    "            emoticonFear.extend(emoticonsFind)\n",
    "            fear[i] = fear[i].replace(singleEmoticon,\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(fear)):\n",
    "    #preleviamo le emoji e le rimuoviamo dal testo\n",
    "    data = regex.findall(r'\\X', fear[i])\n",
    "    for word in data:\n",
    "        if any(char in emoji.UNICODE_EMOJI['en'] for char in word):\n",
    "            emojiFear.append(word)\n",
    "            fear[i] = fear[i].replace(word, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processamento slang prima di punteggiatura e lowercase\n",
    "with open(\"Slangs/slangs_before.json\") as f:\n",
    "    slangsBefore = json.load(f)\n",
    "    for i in range(len(fear)):\n",
    "        for word in fear[i].split():\n",
    "            indexOfWord = fear[i].find(word)\n",
    "            if slangsBefore.get(word) is None:\n",
    "                if slangsBefore.get(word.lower()) is None:\n",
    "                    if slangsBefore.get(word.upper()) is None:\n",
    "                        continue\n",
    "                    else:\n",
    "                        fear[i] = fear[i][:indexOfWord] + slangsBefore.get(word.upper()) + fear[i][indexOfWord+len(word):]\n",
    "                else:\n",
    "                    fear[i] = fear[i][:indexOfWord] + slangsBefore.get(word.lower()) + fear[i][indexOfWord+len(word):]\n",
    "            else:\n",
    "                fear[i] = fear[i][:indexOfWord] + slangsBefore.get(word) + fear[i][indexOfWord+len(word):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizzazione di tutte le frasi\n",
    "fearTokenized = []\n",
    "for linea in fear:\n",
    "    result = word_tokenize(linea)\n",
    "    fearTokenized.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Esecuzione della POS tagging per ogni riga della matrice\n",
    "for i in range(len(fearTokenized)):\n",
    "    fearTokenized[i] = nltk.pos_tag(fearTokenized[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trattamento punteggiatura\n",
    "for line in fearTokenized:\n",
    "    newLine = line\n",
    "    listToRemove = []\n",
    "    for i in range(len(line)):\n",
    "        if line[i][0] in puntuaction:\n",
    "            listToRemove.append(line[i])\n",
    "    for element in listToRemove:\n",
    "        newLine.remove(element)\n",
    "    line = newLine\n",
    "\n",
    "linePuntuaction = re.compile(\"[\" + re.escape(\"\".join(puntuaction)) + \"]\")\n",
    "for line in fearTokenized:\n",
    "    for i in range(len(line)):\n",
    "        tupla = [(re.sub(linePuntuaction, \"\", line[i][0])),line[i][1]]\n",
    "        line[i] = tupla\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming\n",
    "#ps = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for line in fearTokenized:\n",
    "    for i in range(len(line)):\n",
    "        tupla = [lemmatizer.lemmatize(line[i][0]),line[i][1]]\n",
    "        line[i] = tupla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trattamnento Stop word dopo Stemming\n",
    "stop_words = set(stopwords.words('english'))\n",
    "fearTokStoped = []\n",
    "for line in fearTokenized:\n",
    "    filtred_word = []\n",
    "    for row in line:\n",
    "        if  row[0] not in stop_words:\n",
    "            filtred_word.append(row) \n",
    "    fearTokStoped.append(filtred_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "fearListWord = []\n",
    "fearListTok = []\n",
    "for line in fearTokStoped:\n",
    "    for row in line:\n",
    "        if row[0] != '':\n",
    "            fearListWord.append(row[0])\n",
    "        fearListTok.append(row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lavorazione risorse lessicali\n",
    "resourceFearNRC = []\n",
    "resourceFearSentisense = []\n",
    "#Lista di righe, ogni riga una tweet fear\n",
    "file = open(\"Risorse lessicali\\\\Fear\\\\NRC_fear.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        resourceFearNRC.append(riga)\n",
    "\n",
    "file.close()\n",
    "\n",
    "file = open(\"Risorse lessicali\\Fear\\sentisense_fear.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        resourceFearSentisense.append(riga)\n",
    "\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.InsertOneResult at 0x1b169ca7f70>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Caricare le risorse lessicali \n",
    "FearNRC = {\n",
    "    '_id': 'NRC_fear',\n",
    "    'sentiment': 'fear',  \n",
    "    'wordList': resourceFearNRC\n",
    "}\n",
    "\n",
    "FearSentisense = {\n",
    "    '_id': 'sentisense_fear',\n",
    "    'sentiment': 'fear',\n",
    "    'wordList': resourceFearSentisense\n",
    "}\n",
    "\n",
    "resourcesCollection.insert_one(FearNRC)\n",
    "resourcesCollection.insert_one(FearSentisense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione database e collezioni\n",
    "# Inserimento singoli documenti contententi i tweet fear\n",
    "wordsString = {}\n",
    "\n",
    "for i in range(len(fearListWord)):\n",
    "    wordsString = {\n",
    "        \"lemma\": fearListWord[i],\n",
    "        \"POS\": fearListTok[i]\n",
    "    }\n",
    "    fearTwit={\"word\":wordsString}\n",
    "    fearCollections.insert_one(fearTwit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inserimento collezione Twitter con hashtags, emoticons ed emoji \n",
    "fearInsert = {\n",
    "    \"sentiment\":\"fear\",\n",
    "    \"hashtags\": hashtagFear,\n",
    "    \"emoticons\": emoticonFear,\n",
    "    \"emoji\": emojiFear\n",
    "}\n",
    "\n",
    "#inserimento\n",
    "x = collectionFear.insert_one(fearInsert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.command_cursor.CommandCursor at 0x1b16360c2e0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Raggruppa i documenti di input in base all'espressione _id specificata e per ciascun raggruppamento distinto genera un documento\n",
    "pipeline = [\n",
    "    {\"$group\": {\"_id\": \"$word.lemma\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"word\": \"$_id\",\n",
    "      \"freq\": \"$count\"\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"freq\": -1\n",
    "    }},\n",
    "    {\"$out\": \"fear\"}\n",
    "]\n",
    "\n",
    "#post processing per renderlo valido \n",
    "fearCollections.aggregate(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<wordcloud.wordcloud.WordCloud at 0x1b168a8de50>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creazione Word Clouds per le parole\n",
    "data = {}\n",
    "for document in fearCollections.find().limit(50):\n",
    "    data[document['word']] = document['freq']\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2)\n",
    "wc.fit_words(data)\n",
    "wc.to_file('Word Clouds/Fear/fearWords.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.UpdateResult at 0x1b169ca7790>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Conteggio degli hashtags\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$hashtags\"},\n",
    "    {\"$group\": {\"_id\": \"$hashtags\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"word\": \"$_id\",\n",
    "      \"count\": 1\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"count\": -1\n",
    "    }}\n",
    "]\n",
    "\n",
    "hashtagsCount = collectionFear.aggregate(pipeline)\n",
    "\n",
    "data = {}\n",
    "for document in hashtagsCount:\n",
    "    data[document['word']] = document['count']\n",
    "\n",
    "collectionFear.update_one(\n",
    "  {\"sentiment\": \"fear\"},\n",
    "  {\"$set\": {\"hashtags\": data}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<wordcloud.wordcloud.WordCloud at 0x1b163606ee0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creazione Word Clouds per gli hashtags\n",
    "\n",
    "hashtags = collectionFear.distinct(\"hashtags\")\n",
    "\n",
    "hashtagsLimited = {}\n",
    "for (word,count) in [x for x in hashtags[0].items()][:50]:\n",
    "    hashtagsLimited[word] = count\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2)\n",
    "wc.fit_words(hashtagsLimited)\n",
    "wc.to_file('Word Clouds/Fear/fearHashtags.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.UpdateResult at 0x1b163606f10>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Conteggio delle emoji\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$emoji\"},\n",
    "    {\"$group\": {\"_id\": \"$emoji\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"emoji\": \"$_id\",\n",
    "      \"count\": 1\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"count\": -1\n",
    "    }}\n",
    "]\n",
    "\n",
    "emojiCount = collectionFear.aggregate(pipeline)\n",
    "\n",
    "data = {}\n",
    "for document in emojiCount:\n",
    "    data[document['emoji']] = document['count']\n",
    "\n",
    "collectionFear.update_one(\n",
    "  {\"sentiment\": \"fear\"},\n",
    "  {\"$set\": {\"emoji\": data}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<wordcloud.wordcloud.WordCloud at 0x1b16360c700>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creazione Word Clouds per le emoji\n",
    "\n",
    "emojis = collectionFear.distinct(\"emoji\")\n",
    "\n",
    "emojisLimited = {}\n",
    "for (word,count) in [x for x in emojis[0].items()][:50]:\n",
    "    emojisLimited[word] = count\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2, font_path='Font Emoji\\Symbola.otf')\n",
    "wc.fit_words(emojisLimited)\n",
    "wc.to_file('Word Clouds/Fear/fearEmojis.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.UpdateResult at 0x1b166490c70>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Conteggio degli emoticons\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$emoticons\"},\n",
    "    {\"$group\": {\"_id\": \"$emoticons\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"emoticons\": \"$_id\",\n",
    "      \"count\": 1\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"count\": -1\n",
    "    }}\n",
    "]\n",
    "\n",
    "emoticonsCount = collectionFear.aggregate(pipeline)\n",
    "\n",
    "data = {}\n",
    "for document in emoticonsCount:\n",
    "    data[document['emoticons']] = document['count']\n",
    "\n",
    "collectionFear.update_one(\n",
    "  {\"sentiment\": \"fear\"},\n",
    "  {\"$set\": {\"emoticons\": data}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<wordcloud.wordcloud.WordCloud at 0x1b16360c5e0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creazione Word Clouds per le emoticons\n",
    "\n",
    "emoticons = collectionFear.distinct(\"emoticons\")\n",
    "\n",
    "emoticonsLimited = {}\n",
    "for (word,count) in [x for x in emoticons[0].items()][:50]:\n",
    "    emoticonsLimited[word] = count\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2)\n",
    "wc.fit_words(emoticonsLimited)\n",
    "wc.to_file('Word Clouds/Fear/fearEmoticons.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.command_cursor.CommandCursor at 0x1b163609640>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pipeline di aggregazione per inserire la risorsa lessicale a cui appartiene la parola\n",
    "pipeline = [\n",
    "    {\"$lookup\": {\n",
    "        \"from\": \"Resources\",\n",
    "        \"localField\": \"word\",\n",
    "        \"foreignField\": \"wordList\",\n",
    "        \"pipeline\": [\n",
    "            {\"$match\": {\"$expr\": {\"$regexMatch\": {\"input\": \"$_id\", \"regex\": \"fear\"}}}}\n",
    "        ],\n",
    "        \"as\": \"LexResources\"\n",
    "    }},\n",
    "    {\"$project\": {\n",
    "        \"resources.wordList\": 0,\n",
    "        \"resources.totNumberWords\": 0,\n",
    "        \"resources.sentiment\": 0,\n",
    "        \"LexResources.sentiment\": 0,\n",
    "        \"LexResources.totNumberWords\": 0,\n",
    "        \"LexResources.wordList\": 0\n",
    "    }},\n",
    "    {\"$out\": \"fear\"}\n",
    "]\n",
    "\n",
    "fearCollections.aggregate(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': 'sentisense_fear', 'perc_presence_lex_res': 0.6, 'perc_presence_twitter': 0.000183}\n",
      "{'_id': 'NRC_fear', 'perc_presence_lex_res': 0.418699, 'perc_presence_twitter': 0.001215}\n"
     ]
    }
   ],
   "source": [
    "#Pipeline di aggregazione per calcolare le percentuali richieste\n",
    "\n",
    "pipelineCount = [\n",
    "    {\"$group\": {\"_id\": \"null\", \"count\": { \"$sum\": \"$freq\" }}},\n",
    "    {\"$project\": {\n",
    "        \"_id\": 0,\n",
    "        \"count\": \"$count\"\n",
    "    }}\n",
    "]\n",
    "\n",
    "results = fearCollections.aggregate(pipelineCount)\n",
    "\n",
    "countWords = 0\n",
    "for data in results:\n",
    "    countWords = data['count']\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$LexResources\"},\n",
    "    {\"$lookup\": {\n",
    "        \"from\": \"Resources\",\n",
    "        \"localField\": \"LexResources._id\",\n",
    "        \"foreignField\": \"_id\",\n",
    "        \"as\": \"RefResource\"\n",
    "    }},\n",
    "    {\"$group\": {\"_id\": { \"lex\": \"$LexResources._id\", \"RefResource\": \"$RefResource\" }, \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "        \"_id\": \"$_id.lex\",\n",
    "        \"wordList\": \"$_id.RefResource.wordList\",\n",
    "        \"commonWords\": \"$count\"\n",
    "    }},\n",
    "    {\"$unwind\": \"$wordList\"},\n",
    "    {\"$project\": {\n",
    "        \"_id\": 1,\n",
    "        \"commonWords\": 1,\n",
    "        \"totWordsLex\": {\"$size\": \"$wordList\"}\n",
    "    }},\n",
    "    {\"$addFields\": {\"totalWordsTwitter\": countWords}},\n",
    "    {\"$project\": {\n",
    "        \"perc_presence_lex_res\": { \"$round\": [{\"$divide\": [ \"$commonWords\", \"$totWordsLex\" ] }, 6] },\n",
    "        \"perc_presence_twitter\": { \"$round\": [{\"$divide\": [ \"$commonWords\", \"$totalWordsTwitter\" ]}, 6] }\n",
    "    }}\n",
    "]\n",
    "\n",
    "results = fearCollections.aggregate(pipeline)\n",
    "\n",
    "# Genero i dati per l'istogramma\n",
    "istogramFear_lexRes = []\n",
    "istogramFear_twitter = []\n",
    "\n",
    "for data in results:\n",
    "    print(data)\n",
    "    log =math.log(data['perc_presence_lex_res'],10)\n",
    "    istogramFear_lexRes.append([data['_id'], data['perc_presence_lex_res']])\n",
    "    log =math.log(data['perc_presence_twitter'],10)\n",
    "    istogramFear_twitter.append([data['_id'], data['perc_presence_twitter']])\n",
    "\n",
    "# Create the pandas DataFrame\n",
    "dfFear_lexRes = pd.DataFrame(istogramFear_lexRes, columns = ['Risorsa', 'Percentuale'])\n",
    "dfFear_twitter = pd.DataFrame(istogramFear_twitter,columns = ['Risorsa', 'Percentuale'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Creazione istogramma per presenza parole nei twitter\n",
    "\n",
    "dfFear_lexRes.plot.bar(x='Risorsa')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Creazione istogramma per presenza parole nei twitter\n",
    "\n",
    "dfFear_twitter.plot.bar(x='Risorsa')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connessione, creazione cursore e database\n",
    "conn = psycopg2.connect(database = \"maadb\", user = \"postgres\", password = \"admin\", host = \"localhost\", port = \"5432\")\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione tabella risorse lessicali fear e inserimento\n",
    "cur.execute('''CREATE TABLE RISORSEFEAR\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    NOME             TEXT       NOT NULL,\n",
    "                    PAROLA           TEXT       NOT NULL\n",
    "                );\n",
    "            '''\n",
    ")\n",
    "nome = \"NRC_fear\"\n",
    "for word in resourceFearNRC:\n",
    "    cur.execute(\"INSERT INTO RISORSEFEAR (NOME, PAROLA) VALUES(%s, %s)\",(nome,word))\n",
    "\n",
    "nome = \"sentisense_fear\"\n",
    "for word in resourceFearSentisense:\n",
    "    cur.execute(\"INSERT INTO RISORSEFEAR (NOME, PAROLA) VALUES(%s, %s)\",(nome,word))\n",
    "\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione tabelle per le parole, emoji, emoticon e hashtag\n",
    "\n",
    "cur.execute('''CREATE TABLE PAROLEFEAR\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    PAROLA           TEXT       NOT NULL, \n",
    "                    NRC_fear      BOOL       NOT NULL DEFAULT FALSE,\n",
    "                    sentisense_fear      BOOL       NOT NULL DEFAULT FALSE,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ")   \n",
    "\n",
    "cur.execute('''CREATE TABLE EMOJIFEAR\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    EMOJI       TEXT       NOT NULL,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ") \n",
    "\n",
    "cur.execute('''CREATE TABLE EMOTICONSFEAR\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    EMOTICONS       TEXT       NOT NULL,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ") \n",
    "\n",
    "cur.execute('''CREATE TABLE HASHTAGFEAR\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    HASHTAG     TEXT       NOT NULL,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ") \n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inserimento hashtag, emoticon ed emoji\n",
    "\n",
    "for hashTag in hashtagFear:\n",
    "    cur.execute(\"INSERT INTO HASHTAGFEAR (HASHTAG,FREQUENCY) VALUES(%s,%s)\",(hashTag,0))\n",
    "\n",
    "for emoticons in emoticonFear:\n",
    "    cur.execute(\"INSERT INTO EMOTICONSFEAR (EMOTICONS,FREQUENCY) VALUES(%s,%s)\",(emoticons,0))\n",
    "\n",
    "for emojis in emojiFear:\n",
    "    cur.execute(\"INSERT INTO EMOJIFEAR (EMOJI,FREQUENCY) VALUES(%s,%s)\",(emojis,0))\n",
    "\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inserimento parole in db\n",
    "for word in fearListWord:\n",
    "    cur.execute(\"INSERT INTO PAROLEFEAR (PAROLA,NRC_fear,sentisense_fear,FREQUENCY) VALUES(%s,%s,%s,%s)\",(word,False,False,0))\n",
    "\n",
    "conn.commit()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze hashtag\n",
    "cur.execute(\"update HASHTAGFEAR as ha1 set frequency = (select count(*) from HASHTAGFEAR ha2 where ha1.hashtag = ha2.hashtag);\")\n",
    "cur.execute(\"delete from HASHTAGFEAR as ha1 where exists (Select 1 From HASHTAGFEAR as ha2 where ha1.hashtag = ha2.hashtag and ha1.frequency = ha2.frequency and ha2.ctid > ha1.ctid);\")\n",
    "\n",
    "conn.commit()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze emoji\n",
    "cur.execute(\"update EMOJIFEAR as ha1 set frequency = (select count(*) from EMOJIFEAR ha2 where ha1.emoji = ha2.emoji);\")\n",
    "cur.execute(\"delete from EMOJIFEAR as ha1 where exists (Select 1 From EMOJIFEAR as ha2 where ha1.emoji = ha2.emoji and ha1.frequency = ha2.frequency and ha2.ctid > ha1.ctid);\")\n",
    "\n",
    "conn.commit()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze emoticons\n",
    "cur.execute(\"update EMOTICONSFEAR as ha1 set frequency = (select count(*) from EMOTICONSFEAR ha2 where ha1.emoticons = ha2.emoticons);\")\n",
    "cur.execute(\"delete from EMOTICONSFEAR as ha1 where exists (Select 1 From EMOTICONSFEAR as ha2 where ha1.emoticons = ha2.emoticons and ha1.frequency = ha2.frequency and ha2.ctid > ha1.ctid);\")\n",
    "\n",
    "conn.commit() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze parole\n",
    "cur.execute(\"select count(*),parola from PAROLEFEAR group by parola\")\n",
    "result = []\n",
    "for word in cur.fetchall():\n",
    "    result.append(word) \n",
    "\n",
    "cur.execute(\"truncate table PAROLEFEAR;\")\n",
    "cur.execute(\"ALTER SEQUENCE parolefear_id_seq RESTART WITH 1;\")\n",
    "\n",
    "for couple in result:\n",
    "    cur.execute(\"INSERT INTO PAROLEFEAR (PAROLA,NRC_fear,sentisense_fear,FREQUENCY) VALUES(%s,%s,%s,%s)\",(couple[1],False,False,couple[0]))\n",
    "\n",
    "conn.commit()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggiornamento presenza parola nelle risorse lessicali\n",
    "cur.execute(\"UPDATE PAROLEFEAR SET NRC_fear = TRUE WHERE parola IN (select p.parola from PAROLEFEAR as p LEFT OUTER JOIN RISORSEFEAR as r ON p.parola = r.parola WHERE r.nome = 'NRC_fear')\")\n",
    "cur.execute(\"UPDATE PAROLEFEAR SET sentisense_fear = TRUE WHERE parola IN (select p.parola from PAROLEFEAR as p LEFT OUTER JOIN RISORSEFEAR as r ON p.parola = r.parola WHERE r.nome = 'sentisense_fear')\")\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvataggio in un txt delle parole non presenti in nessuna risorsa lessicale\n",
    "wordsMongo = fearCollections.find({\"LexResources\": []}, {\"_id\": 0, \"freq\": 0, \"LexResources\": 0})\n",
    "words = []\n",
    "\n",
    "for word in wordsMongo:\n",
    "    words.append(word['word'])\n",
    "\n",
    "with open('Nuove Parole/fear.txt', 'w',encoding=\"utf8\") as f:\n",
    "    for word in words:\n",
    "        f.write(\"%s\\n\" % word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trattamento dataset Joy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lista di righe, ogni riga una tweet joy\n",
    "file = open(\"Dataset/dataset_dt_joy_60k.txt\", 'r',encoding=\"utf8\")\n",
    "joy = []\n",
    "hashtagJoy = []\n",
    "emoticonJoy = []\n",
    "emojiJoy = []\n",
    "\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "    riga = re.sub(r'URL', '',riga)\n",
    "    riga = re.sub(r'USERNAME', '',riga)\n",
    "    \n",
    "    #array di hashtag singoli\n",
    "    hashTag = (re.findall('#[a-zA-Z0-9]+', riga))\n",
    "\n",
    "    #generiamo l'array di hashtag senza hashtag\n",
    "    for h in hashTag:\n",
    "        hashtagJoy.append(h.replace(\"#\",\"\"))\n",
    "\n",
    "    #Rimuoviamo glin hashtag dalla riga\n",
    "    riga = riga.replace('#',\"\")\n",
    "\n",
    "    joy.append(riga)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(joy)):\n",
    "    #Controllo se l'emoticons c'è e in caso la rimuovo\n",
    "    for singleEmoticon in emoticonsList:\n",
    "        emoticonsFind = re.findall(re.escape(singleEmoticon),joy[i])\n",
    "        if(len(emoticonsFind) != 0):\n",
    "            emoticonJoy.extend(emoticonsFind)\n",
    "            joy[i] = joy[i].replace(singleEmoticon,\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(joy)):\n",
    "    #preleviamo le emoji e le rimuoviamo dal testo\n",
    "    data = regex.findall(r'\\X', joy[i])\n",
    "    for word in data:\n",
    "        if any(char in emoji.UNICODE_EMOJI['en'] for char in word):\n",
    "            emojiJoy.append(word)\n",
    "            joy[i] = joy[i].replace(word, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processamento slang prima di punteggiatura e lowercase\n",
    "with open(\"Slangs/slangs_before.json\") as f:\n",
    "    slangsBefore = json.load(f)\n",
    "    for i in range(len(joy)):\n",
    "        for word in joy[i].split():\n",
    "            indexOfWord = joy[i].find(word)\n",
    "            if slangsBefore.get(word) is None:\n",
    "                if slangsBefore.get(word.lower()) is None:\n",
    "                    if slangsBefore.get(word.upper()) is None:\n",
    "                        continue\n",
    "                    else:\n",
    "                        joy[i] = joy[i][:indexOfWord] + slangsBefore.get(word.upper()) + joy[i][indexOfWord+len(word):]\n",
    "                else:\n",
    "                    joy[i] = joy[i][:indexOfWord] + slangsBefore.get(word.lower()) + joy[i][indexOfWord+len(word):]\n",
    "            else:\n",
    "                joy[i] = joy[i][:indexOfWord] + slangsBefore.get(word) + joy[i][indexOfWord+len(word):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizzazione di tutte le frasi\n",
    "joyTokenized = []\n",
    "for linea in joy:\n",
    "    result = word_tokenize(linea)\n",
    "    joyTokenized.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Esecuzione della POS tagging per ogni riga della matrice\n",
    "for i in range(len(joyTokenized)):\n",
    "    joyTokenized[i] = nltk.pos_tag(joyTokenized[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trattamento punteggiatura\n",
    "for line in joyTokenized:\n",
    "    newLine = line\n",
    "    listToRemove = []\n",
    "    for i in range(len(line)):\n",
    "        if line[i][0] in puntuaction:\n",
    "            listToRemove.append(line[i])\n",
    "    for element in listToRemove:\n",
    "        newLine.remove(element)\n",
    "    line = newLine\n",
    "\n",
    "linePuntuaction = re.compile(\"[\" + re.escape(\"\".join(puntuaction)) + \"]\")\n",
    "for line in joyTokenized:\n",
    "    for i in range(len(line)):\n",
    "        tupla = [(re.sub(linePuntuaction, \"\", line[i][0])),line[i][1]]\n",
    "        line[i] = tupla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming\n",
    "#ps = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for line in joyTokenized:\n",
    "    for i in range(len(line)):\n",
    "        tupla = [lemmatizer.lemmatize(line[i][0]),line[i][1]]\n",
    "        line[i] = tupla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trattamnento Stop word dopo Stemming\n",
    "stop_words = set(stopwords.words('english'))\n",
    "joyTokStoped = []\n",
    "for line in joyTokenized:\n",
    "    filtred_word = []\n",
    "    for row in line:\n",
    "        if  row[0] not in stop_words:\n",
    "            filtred_word.append(row) \n",
    "    joyTokStoped.append(filtred_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joyListWord = []\n",
    "joyListTok = []\n",
    "for line in joyTokStoped:\n",
    "    for row in line:\n",
    "        if row[0] != '':\n",
    "            joyListWord.append(row[0])\n",
    "        joyListTok.append(row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lavorazione risorse lessicali\n",
    "resourceJoyEmoSN = []\n",
    "resourceJoyNRC = []\n",
    "resourceJoySentisence = []\n",
    "#Lista di righe, ogni riga una tweet anger\n",
    "file = open(\"Risorse lessicali\\Joy\\EmoSN_joy.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        resourceJoyEmoSN.append(riga)\n",
    "\n",
    "file.close()\n",
    "\n",
    "file = open(\"Risorse lessicali\\\\Joy\\\\NRC_joy.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        resourceJoyNRC.append(riga)\n",
    "\n",
    "file.close()\n",
    "\n",
    "file = open(\"Risorse lessicali\\Joy\\sentisense_joy.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        resourceJoySentisence.append(riga)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Caricare le risorse lessicali \n",
    "\n",
    "JoyEmoSN = {\n",
    "    '_id': 'EmoSN_joy',\n",
    "    'sentiment': 'joy', \n",
    "    'wordList': resourceJoyEmoSN\n",
    "}\n",
    "\n",
    "JoyNRC = {\n",
    "    '_id': 'NRC_joy',\n",
    "    'sentiment': 'joy',  \n",
    "    'wordList': resourceJoyNRC\n",
    "}\n",
    "\n",
    "JoySentisense = {\n",
    "    '_id': 'sentisense_joy',\n",
    "    'sentiment': 'joy',\n",
    "    'wordList': resourceJoySentisence\n",
    "}\n",
    "\n",
    "resourcesCollection.insert_one(JoyEmoSN)\n",
    "resourcesCollection.insert_one(JoyNRC)\n",
    "resourcesCollection.insert_one(JoySentisense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione database e collezioni\n",
    "# Inserimento singoli documenti contententi i tweet joy\n",
    "wordsString = {}\n",
    "\n",
    "for i in range(len(joyListWord)):\n",
    "    wordsString = {\n",
    "        \"lemma\": joyListWord[i],\n",
    "        \"POS\": joyListTok[i]\n",
    "    }\n",
    "    joyTwit={\"word\":wordsString}\n",
    "    joyCollections.insert_one(joyTwit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inserimento collezione Twitter con hashtags, emoticons ed emoji \n",
    "joyInsert = {\n",
    "    \"sentiment\":\"joy\",\n",
    "    \"hashtags\": hashtagJoy,\n",
    "    \"emoticons\": emoticonJoy,\n",
    "    \"emoji\": emojiJoy\n",
    "}\n",
    "\n",
    "#inserimento\n",
    "x = collectionAJoy.insert_one(joyInsert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raggruppa i documenti di input in base all'espressione _id specificata e per ciascun raggruppamento distinto genera un documento\n",
    "pipeline = [\n",
    "    {\"$group\": {\"_id\": \"$word.lemma\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"word\": \"$_id\",\n",
    "      \"freq\": \"$count\"\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"freq\": -1\n",
    "    }},\n",
    "    {\"$out\": \"joy\"}\n",
    "]\n",
    "\n",
    "#post processing per renderlo valido \n",
    "joyCollections.aggregate(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creazione Word Clouds per le parole\n",
    "data = {}\n",
    "for document in joyCollections.find().limit(50):\n",
    "    data[document['word']] = document['freq']\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2)\n",
    "wc.fit_words(data)\n",
    "wc.to_file('Word Clouds/Joy/joyWords.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conteggio degli hashtags\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$hashtags\"},\n",
    "    {\"$group\": {\"_id\": \"$hashtags\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"word\": \"$_id\",\n",
    "      \"count\": 1\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"count\": -1\n",
    "    }}\n",
    "]\n",
    "\n",
    "hashtagsCount = collectionAJoy.aggregate(pipeline)\n",
    "\n",
    "data = {}\n",
    "for document in hashtagsCount:\n",
    "    data[document['word']] = document['count']\n",
    "\n",
    "collectionAJoy.update_one(\n",
    "  {\"sentiment\": \"joy\"},\n",
    "  {\"$set\": {\"hashtags\": data}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creazione Word Clouds per gli hashtags\n",
    "\n",
    "hashtags = collectionAJoy.distinct(\"hashtags\")\n",
    "\n",
    "hashtagsLimited = {}\n",
    "for (word,count) in [x for x in hashtags[0].items()][:50]:\n",
    "    hashtagsLimited[word] = count\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2)\n",
    "wc.fit_words(hashtagsLimited)\n",
    "wc.to_file('Word Clouds/Joy/joyHashtags.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conteggio delle emoji\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$emoji\"},\n",
    "    {\"$group\": {\"_id\": \"$emoji\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"emoji\": \"$_id\",\n",
    "      \"count\": 1\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"count\": -1\n",
    "    }}\n",
    "]\n",
    "\n",
    "emojiCount = collectionAJoy.aggregate(pipeline)\n",
    "\n",
    "data = {}\n",
    "for document in emojiCount:\n",
    "    data[document['emoji']] = document['count']\n",
    "\n",
    "collectionAJoy.update_one(\n",
    "  {\"sentiment\": \"joy\"},\n",
    "  {\"$set\": {\"emoji\": data}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creazione Word Clouds per le emoji\n",
    "\n",
    "emojis = collectionAJoy.distinct(\"emoji\")\n",
    "\n",
    "emojisLimited = {}\n",
    "for (word,count) in [x for x in emojis[0].items()][:50]:\n",
    "    emojisLimited[word] = count\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2, font_path='Font Emoji\\Symbola.otf')\n",
    "wc.fit_words(emojisLimited)\n",
    "wc.to_file('Word Clouds/Joy/joyEmojis.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conteggio degli emoticons\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$emoticons\"},\n",
    "    {\"$group\": {\"_id\": \"$emoticons\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"emoticons\": \"$_id\",\n",
    "      \"count\": 1\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"count\": -1\n",
    "    }}\n",
    "]\n",
    "\n",
    "emoticonsCount = collectionAJoy.aggregate(pipeline)\n",
    "\n",
    "data = {}\n",
    "for document in emoticonsCount:\n",
    "    data[document['emoticons']] = document['count']\n",
    "\n",
    "collectionAJoy.update_one(\n",
    "  {\"sentiment\": \"joy\"},\n",
    "  {\"$set\": {\"emoticons\": data}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creazione Word Clouds per le emoticons\n",
    "\n",
    "emoticons = collectionAJoy.distinct(\"emoticons\")\n",
    "\n",
    "emoticonsLimited = {}\n",
    "for (word,count) in [x for x in emoticons[0].items()][:50]:\n",
    "    emoticonsLimited[word] = count\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2)\n",
    "wc.fit_words(emoticonsLimited)\n",
    "wc.to_file('Word Clouds/Joy/joyEmoticons.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline di aggregazione per inserire la risorsa lessicale a cui appartiene la parola\n",
    "\n",
    "pipeline = [\n",
    "    {\"$lookup\": {\n",
    "        \"from\": \"Resources\",\n",
    "        \"localField\": \"word\",\n",
    "        \"foreignField\": \"wordList\",\n",
    "        \"pipeline\": [\n",
    "            {\"$match\": {\"$expr\": {\"$regexMatch\": {\"input\": \"$_id\", \"regex\": \"joy\"}}}}\n",
    "        ],\n",
    "        \"as\": \"LexResources\"\n",
    "    }},\n",
    "    {\"$project\": {\n",
    "        \"resources.wordList\": 0,\n",
    "        \"resources.totNumberWords\": 0,\n",
    "        \"resources.sentiment\": 0,\n",
    "        \"LexResources.sentiment\": 0,\n",
    "        \"LexResources.totNumberWords\": 0,\n",
    "        \"LexResources.wordList\": 0\n",
    "    }},\n",
    "    {\"$out\": \"joy\"}\n",
    "]\n",
    "\n",
    "joyCollections.aggregate(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline di aggregazione per calcolare le percentuali richieste\n",
    "\n",
    "pipelineCount = [\n",
    "    {\"$group\": {\"_id\": \"null\", \"count\": { \"$sum\": \"$freq\" }}},\n",
    "    {\"$project\": {\n",
    "        \"_id\": 0,\n",
    "        \"count\": \"$count\"\n",
    "    }}\n",
    "]\n",
    "\n",
    "results = joyCollections.aggregate(pipelineCount)\n",
    "\n",
    "countWords = 0\n",
    "for data in results:\n",
    "    countWords = data['count']\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$LexResources\"},\n",
    "    {\"$lookup\": {\n",
    "        \"from\": \"Resources\",\n",
    "        \"localField\": \"LexResources._id\",\n",
    "        \"foreignField\": \"_id\",\n",
    "        \"as\": \"RefResource\"\n",
    "    }},\n",
    "    {\"$group\": {\"_id\": { \"lex\": \"$LexResources._id\", \"RefResource\": \"$RefResource\" }, \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "        \"_id\": \"$_id.lex\",\n",
    "        \"wordList\": \"$_id.RefResource.wordList\",\n",
    "        \"commonWords\": \"$count\"\n",
    "    }},\n",
    "    {\"$unwind\": \"$wordList\"},\n",
    "    {\"$project\": {\n",
    "        \"_id\": 1,\n",
    "        \"commonWords\": 1,\n",
    "        \"totWordsLex\": {\"$size\": \"$wordList\"}\n",
    "    }},\n",
    "    {\"$addFields\": {\"totalWordsTwitter\": countWords}},\n",
    "    {\"$project\": {\n",
    "        \"perc_presence_lex_res\": { \"$round\": [{\"$divide\": [ \"$commonWords\", \"$totWordsLex\" ] }, 6] },\n",
    "        \"perc_presence_twitter\": { \"$round\": [{\"$divide\": [ \"$commonWords\", \"$totalWordsTwitter\" ]}, 6] }\n",
    "    }}\n",
    "]\n",
    "\n",
    "results = joyCollections.aggregate(pipeline)\n",
    "\n",
    "# Genero i dati per l'istogramma\n",
    "istogramJoy_lexRes = []\n",
    "istogramJoy_twitter = []\n",
    "\n",
    "for data in results:\n",
    "    print(data)\n",
    "    log =math.log(data['perc_presence_lex_res'],10)\n",
    "    istogramJoy_lexRes.append([data['_id'], data['perc_presence_lex_res']])\n",
    "    log =math.log(data['perc_presence_twitter'],10)\n",
    "    istogramJoy_twitter.append([data['_id'], data['perc_presence_twitter']])\n",
    " \n",
    "# Create the pandas DataFrame\n",
    "dfJoy_lexRes = pd.DataFrame(istogramJoy_lexRes, columns = ['Risorsa', 'Percentuale'])\n",
    "dfJoy_twitter = pd.DataFrame(istogramJoy_twitter,columns = ['Risorsa', 'Percentuale'])      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione istogramma per presenza parole nelle risorse lessicali\n",
    "\n",
    "dfJoy_lexRes.plot.bar(x='Risorsa')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione istogramma per presenza parole nei twitter\n",
    "\n",
    "dfJoy_twitter.plot.bar(x='Risorsa')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connessione, creazione cursore e database\n",
    "\n",
    "conn = psycopg2.connect(database = \"maadb\", user = \"postgres\", password = \"admin\", host = \"localhost\", port = \"5432\")\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione tabella risorse lessicali joy e inserimento\n",
    "cur.execute('''CREATE TABLE RISORSEJOY\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    NOME             TEXT       NOT NULL,\n",
    "                    PAROLA           TEXT       NOT NULL\n",
    "                );\n",
    "            '''\n",
    ")\n",
    "nome = \"EmoSN_joy\"\n",
    "for word in resourceJoyEmoSN:\n",
    "    cur.execute(\"INSERT INTO RISORSEJOY (NOME, PAROLA) VALUES(%s, %s)\",(nome,word))\n",
    "\n",
    "nome = \"NRC_joy\"\n",
    "for word in resourceJoyNRC:\n",
    "    cur.execute(\"INSERT INTO RISORSEJOY (NOME, PAROLA) VALUES(%s, %s)\",(nome,word))\n",
    "\n",
    "nome = \"sentisense_joy\"\n",
    "for word in resourceJoySentisence:\n",
    "    cur.execute(\"INSERT INTO RISORSEJOY (NOME, PAROLA) VALUES(%s, %s)\",(nome,word))\n",
    "\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione tabelle per le parole, emoji, emoticon e hashtag\n",
    "\n",
    "cur.execute('''CREATE TABLE PAROLEJOY\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    PAROLA           TEXT       NOT NULL, \n",
    "                    EmoSN_joy      BOOL       NOT NULL DEFAULT FALSE,\n",
    "                    NRC_joy      BOOL       NOT NULL DEFAULT FALSE,\n",
    "                    sentisense_joy      BOOL       NOT NULL DEFAULT FALSE,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ")   \n",
    "\n",
    "cur.execute('''CREATE TABLE EMOJIJOY\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    EMOJI       TEXT       NOT NULL,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ") \n",
    "\n",
    "cur.execute('''CREATE TABLE EMOTICONSJOY\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    EMOTICONS       TEXT       NOT NULL,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ") \n",
    "\n",
    "cur.execute('''CREATE TABLE HASHTAGJOY\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    HASHTAG     TEXT       NOT NULL,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ") \n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inserimento hashtag, emoticon ed emoji\n",
    "\n",
    "for hashTag in hashtagJoy:\n",
    "    cur.execute(\"INSERT INTO HASHTAGJOY (HASHTAG,FREQUENCY) VALUES(%s,%s)\",(hashTag,0))\n",
    "\n",
    "for emoticons in emoticonJoy:\n",
    "    cur.execute(\"INSERT INTO EMOTICONSJOY (EMOTICONS,FREQUENCY) VALUES(%s,%s)\",(emoticons,0))\n",
    "\n",
    "for emojis in emojiJoy:\n",
    "    cur.execute(\"INSERT INTO EMOJIJOY (EMOJI,FREQUENCY) VALUES(%s,%s)\",(emojis,0))\n",
    "\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inserimento parole in db\n",
    "for word in joyListWord:\n",
    "    cur.execute(\"INSERT INTO PAROLEJOY (PAROLA,EmoSN_joy,NRC_joy,sentisense_joy,FREQUENCY) VALUES(%s,%s,%s,%s,%s)\",(word,False,False,False,0))\n",
    "\n",
    "conn.commit()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze hashtag\n",
    "cur.execute(\"update HASHTAGJOY as ha1 set frequency = (select count(*) from HASHTAGJOY ha2 where ha1.hashtag = ha2.hashtag);\")\n",
    "cur.execute(\"delete from HASHTAGJOY as ha1 where exists (Select 1 From HASHTAGJOY as ha2 where ha1.hashtag = ha2.hashtag and ha1.frequency = ha2.frequency and ha2.ctid > ha1.ctid);\")\n",
    "\n",
    "conn.commit()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze emoji\n",
    "cur.execute(\"update EMOJIJOY as ha1 set frequency = (select count(*) from EMOJIJOY ha2 where ha1.emoji = ha2.emoji);\")\n",
    "cur.execute(\"delete from EMOJIJOY as ha1 where exists (Select 1 From EMOJIJOY as ha2 where ha1.emoji = ha2.emoji and ha1.frequency = ha2.frequency and ha2.ctid > ha1.ctid);\")\n",
    "\n",
    "conn.commit() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze emoticons\n",
    "cur.execute(\"update EMOTICONSJOY as ha1 set frequency = (select count(*) from EMOTICONSJOY ha2 where ha1.emoticons = ha2.emoticons);\")\n",
    "cur.execute(\"delete from EMOTICONSJOY as ha1 where exists (Select 1 From EMOTICONSJOY as ha2 where ha1.emoticons = ha2.emoticons and ha1.frequency = ha2.frequency and ha2.ctid > ha1.ctid);\")\n",
    "\n",
    "conn.commit()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze parole\n",
    "cur.execute(\"select count(*),parola from PAROLEJOY group by parola\")\n",
    "result = []\n",
    "for word in cur.fetchall():\n",
    "    result.append(word) \n",
    "\n",
    "cur.execute(\"truncate table PAROLEJOY;\")\n",
    "cur.execute(\"ALTER SEQUENCE parolejoy_id_seq RESTART WITH 1;\")\n",
    "\n",
    "for couple in result:\n",
    "    cur.execute(\"INSERT INTO PAROLEJOY (PAROLA,EmoSN_joy,NRC_joy,sentisense_joy,FREQUENCY) VALUES(%s,%s,%s,%s,%s)\",(couple[1],False,False,False,couple[0]))\n",
    "\n",
    "conn.commit()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggiornamento presenza parola nelle risorse lessicali\n",
    "cur.execute(\"UPDATE PAROLEJOY SET EmoSN_joy = TRUE WHERE parola IN (select p.parola from PAROLEJOY as p LEFT OUTER JOIN RISORSEJOY as r ON p.parola = r.parola WHERE r.nome = 'EmoSN_joy')\")\n",
    "cur.execute(\"UPDATE PAROLEJOY SET NRC_joy = TRUE WHERE parola IN (select p.parola from PAROLEJOY as p LEFT OUTER JOIN RISORSEJOY as r ON p.parola = r.parola WHERE r.nome = 'NRC_joy')\")\n",
    "cur.execute(\"UPDATE PAROLEJOY SET sentisense_joy = TRUE WHERE parola IN (select p.parola from PAROLEJOY as p LEFT OUTER JOIN RISORSEJOY as r ON p.parola = r.parola WHERE r.nome = 'sentisense_joy')\")\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvataggio in un txt delle parole non presenti in nessuna risorsa lessicale\n",
    "\n",
    "wordsMongo = joyCollections.find({\"LexResources\": []}, {\"_id\": 0, \"freq\": 0, \"LexResources\": 0})\n",
    "words = []\n",
    "\n",
    "for word in wordsMongo:\n",
    "    words.append(word['word'])\n",
    "\n",
    "with open('Nuove Parole/joy.txt', 'w',encoding=\"utf8\") as f:\n",
    "    for word in words:\n",
    "        f.write(\"%s\\n\" % word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trattamento dataset Sadness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lista di righe, ogni riga una tweet sadness\n",
    "file = open(\"Dataset/dataset_dt_sadness_60k.txt\", 'r',encoding=\"utf8\")\n",
    "sadness = []\n",
    "hashtagSadness = []\n",
    "emoticonSadness = []\n",
    "emojiSadness = []\n",
    "\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "    riga = re.sub(r'URL', '',riga)\n",
    "    riga = re.sub(r'USERNAME', '',riga)\n",
    "\n",
    "    #array di hashtag singoli\n",
    "    hashTag = (re.findall('#[a-zA-Z0-9]+', riga))\n",
    "\n",
    "    #generiamo l'array di hashtag senza hashtag\n",
    "    for h in hashTag:\n",
    "        hashtagSadness.append(h.replace(\"#\",\"\"))\n",
    "\n",
    "    #Rimuoviamo glin hashtag dalla riga\n",
    "    riga = riga.replace('#',\"\")\n",
    "\n",
    "    sadness.append(riga)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sadness)):\n",
    "    #Controllo se l'emoticons c'è e in caso la rimuovo\n",
    "    for singleEmoticon in emoticonsList:\n",
    "        emoticonsFind = re.findall(re.escape(singleEmoticon),sadness[i])\n",
    "        if(len(emoticonsFind) != 0):\n",
    "            emoticonSadness.extend(emoticonsFind)\n",
    "            sadness[i] = sadness[i].replace(singleEmoticon,\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sadness)):\n",
    "    #preleviamo le emoji e le rimuoviamo dal testo\n",
    "    data = regex.findall(r'\\X', sadness[i])\n",
    "    for word in data:\n",
    "        if any(char in emoji.UNICODE_EMOJI['en'] for char in word):\n",
    "            emojiSadness.append(word)\n",
    "            sadness[i] = sadness[i].replace(word, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processamento slang prima di punteggiatura e lowercase\n",
    "with open(\"Slangs/slangs_before.json\") as f:\n",
    "    slangsBefore = json.load(f)\n",
    "    for i in range(len(sadness)):\n",
    "        for word in sadness[i].split():\n",
    "            indexOfWord = sadness[i].find(word)\n",
    "            if slangsBefore.get(word) is None:\n",
    "                if slangsBefore.get(word.lower()) is None:\n",
    "                    if slangsBefore.get(word.upper()) is None:\n",
    "                        continue\n",
    "                    else:\n",
    "                        sadness[i] = sadness[i][:indexOfWord] + slangsBefore.get(word.upper()) + sadness[i][indexOfWord+len(word):]\n",
    "                else:\n",
    "                    sadness[i] = sadness[i][:indexOfWord] + slangsBefore.get(word.lower()) + sadness[i][indexOfWord+len(word):]\n",
    "            else:\n",
    "                sadness[i] = sadness[i][:indexOfWord] + slangsBefore.get(word) + sadness[i][indexOfWord+len(word):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizzazione di tutte le frasi\n",
    "sadnessTokenized = []\n",
    "for linea in sadness:\n",
    "    result = word_tokenize(linea)\n",
    "    sadnessTokenized.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Esecuzione della POS tagging per ogni riga della matrice\n",
    "for i in range(len(sadnessTokenized)):\n",
    "    sadnessTokenized[i] = nltk.pos_tag(sadnessTokenized[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trattamento punteggiatura\n",
    "for line in sadnessTokenized:\n",
    "    newLine = line\n",
    "    listToRemove = []\n",
    "    for i in range(len(line)):\n",
    "        if line[i][0] in puntuaction:\n",
    "            listToRemove.append(line[i])\n",
    "    for element in listToRemove:\n",
    "        newLine.remove(element)\n",
    "    line = newLine\n",
    "\n",
    "linePuntuaction = re.compile(\"[\" + re.escape(\"\".join(puntuaction)) + \"]\")\n",
    "for line in sadnessTokenized:\n",
    "    for i in range(len(line)):\n",
    "        tupla = [(re.sub(linePuntuaction, \"\", line[i][0])),line[i][1]]\n",
    "        line[i] = tupla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming\n",
    "#ps = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for line in sadnessTokenized:\n",
    "    for i in range(len(line)):\n",
    "        tupla = [lemmatizer.lemmatize(line[i][0]),line[i][1]]\n",
    "        line[i] = tupla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trattamnento Stop word dopo Stemming\n",
    "stop_words = set(stopwords.words('english'))\n",
    "sadnessTokStoped = []\n",
    "for line in sadnessTokenized:\n",
    "    filtred_word = []\n",
    "    for row in line:\n",
    "        if  row[0] not in stop_words:\n",
    "            filtred_word.append(row) \n",
    "    sadnessTokStoped.append(filtred_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sadnessListWord = []\n",
    "sadnessListTok = []\n",
    "for line in sadnessTokStoped:\n",
    "    for row in line:\n",
    "        if row[0] != '':\n",
    "            sadnessListWord.append(row[0])\n",
    "        sadnessListTok.append(row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lavorazione risorse lessicali\n",
    "resourceSadnessNRC = []\n",
    "resourceSadnessSentisense = []\n",
    "file = open(\"Risorse lessicali\\\\Sadness\\\\NRC_sadness.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        resourceSadnessNRC.append(riga)\n",
    "\n",
    "file.close()\n",
    "\n",
    "file = open(\"Risorse lessicali\\Sadness\\sentisense_sadness.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        resourceSadnessSentisense.append(riga)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Caricare le risorse lessicali \n",
    "SadnessNRC = {\n",
    "    '_id': 'NRC_sadness',\n",
    "    'sentiment': 'sadness',  \n",
    "    'wordList': resourceSadnessNRC\n",
    "}\n",
    "\n",
    "SadnessSentisense = {\n",
    "    '_id': 'sentisense_sadness',\n",
    "    'sentiment': 'sadness',\n",
    "    'wordList': resourceSadnessSentisense\n",
    "}\n",
    "\n",
    "resourcesCollection.insert_one(SadnessNRC)\n",
    "resourcesCollection.insert_one(SadnessSentisense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione database e collezioni\n",
    "# Inserimento singoli documenti contententi i tweet sadness\n",
    "wordsString = {}\n",
    "\n",
    "for i in range(len(sadnessListWord)):\n",
    "    wordsString = {\n",
    "        \"lemma\": sadnessListWord[i],\n",
    "        \"POS\": sadnessListTok[i]\n",
    "    }\n",
    "    sadnessTwit={\"word\":wordsString}\n",
    "    sadnessCollections.insert_one(sadnessTwit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inserimento collezione Twitter con hashtags, emoticons ed emoji \n",
    "sadnessInsert = {\n",
    "    \"sentiment\":\"sadness\",\n",
    "    \"hashtags\": hashtagSadness,\n",
    "    \"emoticons\": emoticonSadness,\n",
    "    \"emoji\": emojiSadness\n",
    "}\n",
    "\n",
    "#inserimento\n",
    "x = collectionSadness.insert_one(sadnessInsert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raggruppa i documenti di input in base all'espressione _id specificata e per ciascun raggruppamento distinto genera un documento\n",
    "pipeline = [\n",
    "    {\"$group\": {\"_id\": \"$word.lemma\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"word\": \"$_id\",\n",
    "      \"freq\": \"$count\"\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"freq\": -1\n",
    "    }},\n",
    "    {\"$out\": \"sadness\"}\n",
    "]\n",
    "\n",
    "#post processing per renderlo valido \n",
    "sadnessCollections.aggregate(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creazione Word Clouds per le parole\n",
    "data = {}\n",
    "for document in sadnessCollections.find().limit(50):\n",
    "    data[document['word']] = document['freq']\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2)\n",
    "wc.fit_words(data)\n",
    "wc.to_file('Word Clouds/Sadness/sadnessWords.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conteggio degli hashtags\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$hashtags\"},\n",
    "    {\"$group\": {\"_id\": \"$hashtags\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"word\": \"$_id\",\n",
    "      \"count\": 1\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"count\": -1\n",
    "    }}\n",
    "]\n",
    "\n",
    "hashtagsCount = collectionSadness.aggregate(pipeline)\n",
    "\n",
    "data = {}\n",
    "for document in hashtagsCount:\n",
    "    data[document['word']] = document['count']\n",
    "\n",
    "collectionSadness.update_one(\n",
    "  {\"sentiment\": \"sadness\"},\n",
    "  {\"$set\": {\"hashtags\": data}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creazione Word Clouds per gli hashtags\n",
    "\n",
    "hashtags = collectionSadness.distinct(\"hashtags\")\n",
    "\n",
    "hashtagsLimited = {}\n",
    "for (word,count) in [x for x in hashtags[0].items()][:50]:\n",
    "    hashtagsLimited[word] = count\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2)\n",
    "wc.fit_words(hashtagsLimited)\n",
    "wc.to_file('Word Clouds/Sadness/sadnessHashtags.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conteggio delle emoji\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$emoji\"},\n",
    "    {\"$group\": {\"_id\": \"$emoji\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"emoji\": \"$_id\",\n",
    "      \"count\": 1\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"count\": -1\n",
    "    }}\n",
    "]\n",
    "\n",
    "emojiCount = collectionSadness.aggregate(pipeline)\n",
    "\n",
    "data = {}\n",
    "for document in emojiCount:\n",
    "    data[document['emoji']] = document['count']\n",
    "\n",
    "collectionSadness.update_one(\n",
    "  {\"sentiment\": \"sadness\"},\n",
    "  {\"$set\": {\"emoji\": data}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creazione Word Clouds per le emoji\n",
    "\n",
    "emojis = collectionSadness.distinct(\"emoji\")\n",
    "\n",
    "emojisLimited = {}\n",
    "for (word,count) in [x for x in emojis[0].items()][:50]:\n",
    "    emojisLimited[word] = count\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2, font_path='Font Emoji\\Symbola.otf')\n",
    "wc.fit_words(emojisLimited)\n",
    "wc.to_file('Word Clouds/Sadness/sadnessEmojis.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conteggio degli emoticons\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$emoticons\"},\n",
    "    {\"$group\": {\"_id\": \"$emoticons\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"emoticons\": \"$_id\",\n",
    "      \"count\": 1\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"count\": -1\n",
    "    }}\n",
    "]\n",
    "\n",
    "emoticonsCount = collectionSadness.aggregate(pipeline)\n",
    "\n",
    "data = {}\n",
    "for document in emoticonsCount:\n",
    "    data[document['emoticons']] = document['count']\n",
    "\n",
    "collectionSadness.update_one(\n",
    "  {\"sentiment\": \"sadness\"},\n",
    "  {\"$set\": {\"emoticons\": data}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creazione Word Clouds per le emoticons\n",
    "\n",
    "emoticons = collectionSadness.distinct(\"emoticons\")\n",
    "\n",
    "emoticonsLimited = {}\n",
    "for (word,count) in [x for x in emoticons[0].items()][:50]:\n",
    "    emoticonsLimited[word] = count\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2)\n",
    "wc.fit_words(emoticonsLimited)\n",
    "wc.to_file('Word Clouds/Sadness/sadnessEmoticons.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline di aggregazione per inserire la risorsa lessicale a cui appartiene la parola\n",
    "\n",
    "pipeline = [\n",
    "    {\"$lookup\": {\n",
    "        \"from\": \"Resources\",\n",
    "        \"localField\": \"word\",\n",
    "        \"foreignField\": \"wordList\",\n",
    "        \"pipeline\": [\n",
    "            {\"$match\": {\"$expr\": {\"$regexMatch\": {\"input\": \"$_id\", \"regex\": \"sadness\"}}}}\n",
    "        ],\n",
    "        \"as\": \"LexResources\"\n",
    "    }},\n",
    "    {\"$project\": {\n",
    "        \"resources.wordList\": 0,\n",
    "        \"resources.totNumberWords\": 0,\n",
    "        \"resources.sentiment\": 0,\n",
    "        \"LexResources.sentiment\": 0,\n",
    "        \"LexResources.totNumberWords\": 0,\n",
    "        \"LexResources.wordList\": 0\n",
    "    }},\n",
    "    {\"$out\": \"sadness\"}\n",
    "]\n",
    "\n",
    "sadnessCollections.aggregate(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline di aggregazione per calcolare le percentuali richieste\n",
    "\n",
    "pipelineCount = [\n",
    "    {\"$group\": {\"_id\": \"null\", \"count\": { \"$sum\": \"$freq\" }}},\n",
    "    {\"$project\": {\n",
    "        \"_id\": 0,\n",
    "        \"count\": \"$count\"\n",
    "    }}\n",
    "]\n",
    "\n",
    "results = sadnessCollections.aggregate(pipelineCount)\n",
    "\n",
    "countWords = 0\n",
    "for data in results:\n",
    "    countWords = data['count']\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$LexResources\"},\n",
    "    {\"$lookup\": {\n",
    "        \"from\": \"Resources\",\n",
    "        \"localField\": \"LexResources._id\",\n",
    "        \"foreignField\": \"_id\",\n",
    "        \"as\": \"RefResource\"\n",
    "    }},\n",
    "    {\"$group\": {\"_id\": { \"lex\": \"$LexResources._id\", \"RefResource\": \"$RefResource\" }, \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "        \"_id\": \"$_id.lex\",\n",
    "        \"wordList\": \"$_id.RefResource.wordList\",\n",
    "        \"commonWords\": \"$count\"\n",
    "    }},\n",
    "    {\"$unwind\": \"$wordList\"},\n",
    "    {\"$project\": {\n",
    "        \"_id\": 1,\n",
    "        \"commonWords\": 1,\n",
    "        \"totWordsLex\": {\"$size\": \"$wordList\"}\n",
    "    }},\n",
    "    {\"$addFields\": {\"totalWordsTwitter\": countWords}},\n",
    "    {\"$project\": {\n",
    "        \"perc_presence_lex_res\": { \"$round\": [{\"$divide\": [ \"$commonWords\", \"$totWordsLex\" ] }, 6] },\n",
    "        \"perc_presence_twitter\": { \"$round\": [{\"$divide\": [ \"$commonWords\", \"$totalWordsTwitter\" ]}, 6] }\n",
    "    }}\n",
    "]\n",
    "\n",
    "results = sadnessCollections.aggregate(pipeline)\n",
    "\n",
    "# Genero i dati per l'istogramma\n",
    "istogramSadness_lexRes = []\n",
    "istogramSadness_twitter = []\n",
    "\n",
    "for data in results:\n",
    "    print(data)\n",
    "    log =math.log(data['perc_presence_lex_res'],10)\n",
    "    istogramSadness_lexRes.append([data['_id'], data['perc_presence_lex_res']])\n",
    "    log =math.log(data['perc_presence_twitter'],10)\n",
    "    istogramSadness_twitter.append([data['_id'], data['perc_presence_twitter']])\n",
    " \n",
    "# Create the pandas DataFrame\n",
    "dfSadness_lexRes = pd.DataFrame(istogramSadness_lexRes, columns = ['Risorsa', 'Percentuale'])\n",
    "dfSadness_twitter = pd.DataFrame(istogramSadness_twitter,columns = ['Risorsa', 'Percentuale'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione istogramma per presenza parole nelle risorse lessicali\n",
    "\n",
    "dfSadness_lexRes.plot.bar(x='Risorsa')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione istogramma per presenza parole nei twitter\n",
    "\n",
    "dfSadness_twitter.plot.bar(x='Risorsa')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connessione, creazione cursore e database\n",
    "\n",
    "conn = psycopg2.connect(database = \"maadb\", user = \"postgres\", password = \"admin\", host = \"localhost\", port = \"5432\")\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione tabella risorse lessicali sadness e inserimento\n",
    "cur.execute('''CREATE TABLE RISORSESADNESS\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    NOME             TEXT       NOT NULL,\n",
    "                    PAROLA           TEXT       NOT NULL\n",
    "                );\n",
    "            '''\n",
    ")\n",
    "\n",
    "nome = \"NRC_sadness\"\n",
    "for word in resourceSadnessNRC:\n",
    "    cur.execute(\"INSERT INTO RISORSESADNESS (NOME, PAROLA) VALUES(%s, %s)\",(nome,word))\n",
    "\n",
    "nome = \"sentisense_sadness\"\n",
    "for word in resourceSadnessSentisense:\n",
    "    cur.execute(\"INSERT INTO RISORSESADNESS (NOME, PAROLA) VALUES(%s, %s)\",(nome,word))\n",
    "\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione tabelle per le parole, emoji, emoticon e hashtag\n",
    "\n",
    "cur.execute('''CREATE TABLE PAROLESADNESS\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    PAROLA           TEXT       NOT NULL,\n",
    "                    NRC_sadness      BOOL       NOT NULL DEFAULT FALSE,\n",
    "                    sentisense_sadness      BOOL       NOT NULL DEFAULT FALSE,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ")   \n",
    "\n",
    "cur.execute('''CREATE TABLE EMOJISADNESS\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    EMOJI       TEXT       NOT NULL,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ") \n",
    "\n",
    "cur.execute('''CREATE TABLE EMOTICONSSADNESS\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    EMOTICONS       TEXT       NOT NULL,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ") \n",
    "\n",
    "cur.execute('''CREATE TABLE HASHTAGSADNESS\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    HASHTAG     TEXT       NOT NULL,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ") \n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inserimento hashtag, emoticon ed emoji\n",
    "\n",
    "for hashTag in hashtagSadness:\n",
    "    cur.execute(\"INSERT INTO HASHTAGSADNESS (HASHTAG,FREQUENCY) VALUES(%s,%s)\",(hashTag,0))\n",
    "\n",
    "for emoticons in emoticonSadness:\n",
    "    cur.execute(\"INSERT INTO EMOTICONSSADNESS (EMOTICONS,FREQUENCY) VALUES(%s,%s)\",(emoticons,0))\n",
    "\n",
    "for emojis in emojiSadness:\n",
    "    cur.execute(\"INSERT INTO EMOJISADNESS (EMOJI,FREQUENCY) VALUES(%s,%s)\",(emojis,0))\n",
    "\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inserimento parole in db\n",
    "for word in sadnessListWord:\n",
    "    cur.execute(\"INSERT INTO PAROLESADNESS (PAROLA,NRC_sadness,sentisense_sadness,FREQUENCY) VALUES(%s,%s,%s,%s)\",(word,False,False,0))\n",
    "\n",
    "conn.commit()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze hashtag\n",
    "cur.execute(\"update HASHTAGSADNESS as ha1 set frequency = (select count(*) from HASHTAGSADNESS ha2 where ha1.hashtag = ha2.hashtag);\")\n",
    "cur.execute(\"delete from HASHTAGSADNESS as ha1 where exists (Select 1 From HASHTAGSADNESS as ha2 where ha1.hashtag = ha2.hashtag and ha1.frequency = ha2.frequency and ha2.ctid > ha1.ctid);\")\n",
    "\n",
    "conn.commit()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze emoji\n",
    "cur.execute(\"update EMOJISADNESS as ha1 set frequency = (select count(*) from EMOJISADNESS ha2 where ha1.emoji = ha2.emoji);\")\n",
    "cur.execute(\"delete from EMOJISADNESS as ha1 where exists (Select 1 From EMOJISADNESS as ha2 where ha1.emoji = ha2.emoji and ha1.frequency = ha2.frequency and ha2.ctid > ha1.ctid);\")\n",
    "\n",
    "conn.commit() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze emoticons\n",
    "cur.execute(\"update EMOTICONSSADNESS as ha1 set frequency = (select count(*) from EMOTICONSSADNESS ha2 where ha1.emoticons = ha2.emoticons);\")\n",
    "cur.execute(\"delete from EMOTICONSSADNESS as ha1 where exists (Select 1 From EMOTICONSSADNESS as ha2 where ha1.emoticons = ha2.emoticons and ha1.frequency = ha2.frequency and ha2.ctid > ha1.ctid);\")\n",
    "\n",
    "conn.commit()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze parole\n",
    "cur.execute(\"select count(*),parola from PAROLESADNESS group by parola\")\n",
    "result = []\n",
    "for word in cur.fetchall():\n",
    "    result.append(word) \n",
    "\n",
    "cur.execute(\"truncate table PAROLESADNESS;\")\n",
    "cur.execute(\"ALTER SEQUENCE parolesadness_id_seq RESTART WITH 1;\")\n",
    "\n",
    "for couple in result:\n",
    "    cur.execute(\"INSERT INTO PAROLESADNESS (PAROLA,NRC_sadness,sentisense_sadness,FREQUENCY) VALUES(%s,%s,%s,%s)\",(couple[1],False,False,couple[0]))\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggiornamento presenza parola nelle risorse lessicali\n",
    "cur.execute(\"UPDATE PAROLESADNESS SET NRC_sadness = TRUE WHERE parola IN (select p.parola from PAROLESADNESS as p LEFT OUTER JOIN RISORSESADNESS as r ON p.parola = r.parola WHERE r.nome = 'NRC_sadness')\")\n",
    "cur.execute(\"UPDATE PAROLESADNESS SET sentisense_sadness = TRUE WHERE parola IN (select p.parola from PAROLESADNESS as p LEFT OUTER JOIN RISORSESADNESS as r ON p.parola = r.parola WHERE r.nome = 'sentisense_sadness')\")\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvataggio in un txt delle parole non presenti in nessuna risorsa lessicale\n",
    "\n",
    "wordsMongo = sadnessCollections.find({\"LexResources\": []}, {\"_id\": 0, \"freq\": 0, \"LexResources\": 0})\n",
    "words = []\n",
    "\n",
    "for word in wordsMongo:\n",
    "    words.append(word['word'])\n",
    "\n",
    "with open('Nuove Parole/sadness.txt', 'w',encoding=\"utf8\") as f:\n",
    "    for word in words:\n",
    "        f.write(\"%s\\n\" % word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trattamento dataset Surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lista di righe, ogni riga una tweet surprise\n",
    "file = open(\"Dataset/dataset_dt_surprise_60k.txt\", 'r',encoding=\"utf8\")\n",
    "surprise = []\n",
    "hashtagSurprise = []\n",
    "emoticonSurprise = []\n",
    "emojiSurprise = []\n",
    "\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "    riga = re.sub(r'URL', '',riga)\n",
    "    riga = re.sub(r'USERNAME', '',riga)\n",
    "\n",
    "     #array di hashtag singoli\n",
    "    hashTag = (re.findall('#[a-zA-Z0-9]+', riga))\n",
    "\n",
    "    #generiamo l'array di hashtag senza hashtag\n",
    "    for h in hashTag:\n",
    "        hashtagSurprise.append(h.replace(\"#\",\"\"))\n",
    "\n",
    "    #Rimuoviamo glin hashtag dalla riga\n",
    "    riga = riga.replace('#',\"\")\n",
    "\n",
    "    surprise.append(riga)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(surprise)):\n",
    "    #Controllo se l'emoticons c'è e in caso la rimuovo\n",
    "    for singleEmoticon in emoticonsList:\n",
    "        emoticonsFind = re.findall(re.escape(singleEmoticon),surprise[i])\n",
    "        if(len(emoticonsFind) != 0):\n",
    "            emoticonSurprise.extend(emoticonsFind)\n",
    "            surprise[i] = surprise[i].replace(singleEmoticon,\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(surprise)):\n",
    "    #preleviamo le emoji e le rimuoviamo dal testo\n",
    "    data = regex.findall(r'\\X', surprise[i])\n",
    "    for word in data:\n",
    "        if any(char in emoji.UNICODE_EMOJI['en'] for char in word):\n",
    "            emojiSurprise.append(word)\n",
    "            surprise[i] = surprise[i].replace(word, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processamento slang prima di punteggiatura e lowercase\n",
    "with open(\"Slangs/slangs_before.json\") as f:\n",
    "    slangsBefore = json.load(f)\n",
    "    for i in range(len(surprise)):\n",
    "        for word in surprise[i].split():\n",
    "            indexOfWord = surprise[i].find(word)\n",
    "            if slangsBefore.get(word) is None:\n",
    "                if slangsBefore.get(word.lower()) is None:\n",
    "                    if slangsBefore.get(word.upper()) is None:\n",
    "                        continue\n",
    "                    else:\n",
    "                        surprise[i] = surprise[i][:indexOfWord] + slangsBefore.get(word.upper()) + surprise[i][indexOfWord+len(word):]\n",
    "                else:\n",
    "                    surprise[i] = surprise[i][:indexOfWord] + slangsBefore.get(word.lower()) + surprise[i][indexOfWord+len(word):]\n",
    "            else:\n",
    "                surprise[i] = surprise[i][:indexOfWord] + slangsBefore.get(word) + surprise[i][indexOfWord+len(word):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizzazione di tutte le frasi\n",
    "surpriseTokenized = []\n",
    "for linea in surprise:\n",
    "    result = word_tokenize(linea)\n",
    "    surpriseTokenized.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Esecuzione della POS tagging per ogni riga della matrice\n",
    "for i in range(len(surpriseTokenized)):\n",
    "    surpriseTokenized[i] = nltk.pos_tag(surpriseTokenized[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trattamento punteggiatura\n",
    "for line in surpriseTokenized:\n",
    "    newLine = line\n",
    "    listToRemove = []\n",
    "    for i in range(len(line)):\n",
    "        if line[i][0] in puntuaction:\n",
    "            listToRemove.append(line[i])\n",
    "    for element in listToRemove:\n",
    "        newLine.remove(element)\n",
    "    line = newLine\n",
    "\n",
    "linePuntuaction = re.compile(\"[\" + re.escape(\"\".join(puntuaction)) + \"]\")\n",
    "for line in surpriseTokenized:\n",
    "    for i in range(len(line)):\n",
    "        tupla = [(re.sub(linePuntuaction, \"\", line[i][0])),line[i][1]]\n",
    "        line[i] = tupla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming\n",
    "#ps = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for line in surpriseTokenized:\n",
    "    for i in range(len(line)):\n",
    "        tupla = [lemmatizer.lemmatize(line[i][0]),line[i][1]]\n",
    "        line[i] = tupla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trattamnento Stop word dopo Stemming\n",
    "stop_words = set(stopwords.words('english'))\n",
    "surpriseTokStoped = []\n",
    "for line in surpriseTokenized:\n",
    "    filtred_word = []\n",
    "    for row in line:\n",
    "        if  row[0] not in stop_words:\n",
    "            filtred_word.append(row) \n",
    "    surpriseTokStoped.append(filtred_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surpriseListWord = []\n",
    "surpriseListTok = []\n",
    "for line in surpriseTokStoped:\n",
    "    for row in line:\n",
    "        if row[0] != '':\n",
    "            surpriseListWord.append(row[0])\n",
    "        surpriseListTok.append(row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lavorazione risorse lessicali\n",
    "resourceSurpriseNRC = []\n",
    "resourceSurpriseSentisense = []\n",
    "file = open(\"Risorse lessicali\\\\Surprise\\\\NRC_surprise.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        resourceSurpriseNRC.append(riga)\n",
    "\n",
    "file.close()\n",
    "\n",
    "file = open(\"Risorse lessicali\\Surprise\\sentisense_surprise.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        resourceSurpriseSentisense.append(riga)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Caricare le risorse lessicali \n",
    "SurpriseNRC = {\n",
    "    '_id': 'NRC_surprise',\n",
    "    'sentiment': 'surprise',  \n",
    "    'wordList': resourceSurpriseNRC\n",
    "}\n",
    "\n",
    "SurpriseSentisense = {\n",
    "    '_id': 'sentisense_surprise',\n",
    "    'sentiment': 'surprise',\n",
    "    'wordList': resourceSurpriseSentisense\n",
    "}\n",
    "\n",
    "resourcesCollection.insert_one(SurpriseNRC)\n",
    "resourcesCollection.insert_one(SurpriseSentisense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione database e collezioni\n",
    "# Inserimento singoli documenti contententi i tweet surprise\n",
    "wordsString = {}\n",
    "\n",
    "for i in range(len(surpriseListWord)):\n",
    "    wordsString = {\n",
    "        \"lemma\": surpriseListWord[i],\n",
    "        \"POS\": surpriseListTok[i]\n",
    "    }\n",
    "    surpriseTwit={\"word\":wordsString}\n",
    "    surpriseCollections.insert_one(surpriseTwit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inserimento collezione Twitter con hashtags, emoticons ed emoji \n",
    "surpriseInsert = {\n",
    "    \"sentiment\":\"surprise\",\n",
    "    \"hashtags\": hashtagSurprise,\n",
    "    \"emoticons\": emoticonSurprise,\n",
    "    \"emoji\": emojiSurprise\n",
    "}\n",
    "\n",
    "#inserimento\n",
    "x = collectionSurprise.insert_one(surpriseInsert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raggruppa i documenti di input in base all'espressione _id specificata e per ciascun raggruppamento distinto genera un documento\n",
    "pipeline = [\n",
    "    {\"$group\": {\"_id\": \"$word.lemma\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"word\": \"$_id\",\n",
    "      \"freq\": \"$count\"\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"freq\": -1\n",
    "    }},\n",
    "    {\"$out\": \"surprise\"}\n",
    "]\n",
    "\n",
    "#post processing per renderlo valido \n",
    "surpriseCollections.aggregate(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creazione Word Clouds per le parole\n",
    "data = {}\n",
    "for document in surpriseCollections.find().limit(50):\n",
    "    data[document['word']] = document['freq']\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2)\n",
    "wc.fit_words(data)\n",
    "wc.to_file('Word Clouds/Surprise/surpriseWords.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conteggio degli hashtags\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$hashtags\"},\n",
    "    {\"$group\": {\"_id\": \"$hashtags\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"word\": \"$_id\",\n",
    "      \"count\": 1\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"count\": -1\n",
    "    }}\n",
    "]\n",
    "\n",
    "hashtagsCount = collectionSurprise.aggregate(pipeline)\n",
    "\n",
    "data = {}\n",
    "for document in hashtagsCount:\n",
    "    data[document['word']] = document['count']\n",
    "\n",
    "collectionSurprise.update_one(\n",
    "  {\"sentiment\": \"surprise\"},\n",
    "  {\"$set\": {\"hashtags\": data}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creazione Word Clouds per gli hashtags\n",
    "\n",
    "hashtags = collectionSurprise.distinct(\"hashtags\")\n",
    "\n",
    "hashtagsLimited = {}\n",
    "for (word,count) in [x for x in hashtags[0].items()][:50]:\n",
    "    hashtagsLimited[word] = count\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2)\n",
    "wc.fit_words(hashtagsLimited)\n",
    "wc.to_file('Word Clouds/Surprise/surpriseHashtags.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conteggio delle emoji\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$emoji\"},\n",
    "    {\"$group\": {\"_id\": \"$emoji\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"emoji\": \"$_id\",\n",
    "      \"count\": 1\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"count\": -1\n",
    "    }}\n",
    "]\n",
    "\n",
    "emojiCount = collectionSurprise.aggregate(pipeline)\n",
    "\n",
    "data = {}\n",
    "for document in emojiCount:\n",
    "    data[document['emoji']] = document['count']\n",
    "\n",
    "collectionSurprise.update_one(\n",
    "  {\"sentiment\": \"surprise\"},\n",
    "  {\"$set\": {\"emoji\": data}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creazione Word Clouds per le emoji\n",
    "\n",
    "emojis = collectionSurprise.distinct(\"emoji\")\n",
    "\n",
    "emojisLimited = {}\n",
    "for (word,count) in [x for x in emojis[0].items()][:50]:\n",
    "    emojisLimited[word] = count\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2, font_path='Font Emoji\\Symbola.otf')\n",
    "wc.fit_words(emojisLimited)\n",
    "wc.to_file('Word Clouds/Surprise/surpriseEmojis.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conteggio degli emoticons\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$emoticons\"},\n",
    "    {\"$group\": {\"_id\": \"$emoticons\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"emoticons\": \"$_id\",\n",
    "      \"count\": 1\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"count\": -1\n",
    "    }}\n",
    "]\n",
    "\n",
    "emoticonsCount = collectionSurprise.aggregate(pipeline)\n",
    "\n",
    "data = {}\n",
    "for document in emoticonsCount:\n",
    "    data[document['emoticons']] = document['count']\n",
    "\n",
    "collectionSurprise.update_one(\n",
    "  {\"sentiment\": \"surprise\"},\n",
    "  {\"$set\": {\"emoticons\": data}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creazione Word Clouds per le emoticons\n",
    "\n",
    "emoticons = collectionSurprise.distinct(\"emoticons\")\n",
    "\n",
    "emoticonsLimited = {}\n",
    "for (word,count) in [x for x in emoticons[0].items()][:50]:\n",
    "    emoticonsLimited[word] = count\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2)\n",
    "wc.fit_words(emoticonsLimited)\n",
    "wc.to_file('Word Clouds/Surprise/surpriseEmoticons.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline di aggregazione per inserire la risorsa lessicale a cui appartiene la parola\n",
    "\n",
    "pipeline = [\n",
    "    {\"$lookup\": {\n",
    "        \"from\": \"Resources\",\n",
    "        \"localField\": \"word\",\n",
    "        \"foreignField\": \"wordList\",\n",
    "        \"pipeline\": [\n",
    "            {\"$match\": {\"$expr\": {\"$regexMatch\": {\"input\": \"$_id\", \"regex\": \"surprise\"}}}}\n",
    "        ],\n",
    "        \"as\": \"LexResources\"\n",
    "    }},\n",
    "    {\"$project\": {\n",
    "        \"resources.wordList\": 0,\n",
    "        \"resources.totNumberWords\": 0,\n",
    "        \"resources.sentiment\": 0,\n",
    "        \"LexResources.sentiment\": 0,\n",
    "        \"LexResources.totNumberWords\": 0,\n",
    "        \"LexResources.wordList\": 0\n",
    "    }},\n",
    "    {\"$out\": \"surprise\"}\n",
    "]\n",
    "\n",
    "surpriseCollections.aggregate(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline di aggregazione per calcolare le percentuali richieste\n",
    "\n",
    "pipelineCount = [\n",
    "    {\"$group\": {\"_id\": \"null\", \"count\": { \"$sum\": \"$freq\" }}},\n",
    "    {\"$project\": {\n",
    "        \"_id\": 0,\n",
    "        \"count\": \"$count\"\n",
    "    }}\n",
    "]\n",
    "\n",
    "results = surpriseCollections.aggregate(pipelineCount)\n",
    "\n",
    "countWords = 0\n",
    "for data in results:\n",
    "    countWords = data['count']\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$LexResources\"},\n",
    "    {\"$lookup\": {\n",
    "        \"from\": \"Resources\",\n",
    "        \"localField\": \"LexResources._id\",\n",
    "        \"foreignField\": \"_id\",\n",
    "        \"as\": \"RefResource\"\n",
    "    }},\n",
    "    {\"$group\": {\"_id\": { \"lex\": \"$LexResources._id\", \"RefResource\": \"$RefResource\" }, \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "        \"_id\": \"$_id.lex\",\n",
    "        \"wordList\": \"$_id.RefResource.wordList\",\n",
    "        \"commonWords\": \"$count\"\n",
    "    }},\n",
    "    {\"$unwind\": \"$wordList\"},\n",
    "    {\"$project\": {\n",
    "        \"_id\": 1,\n",
    "        \"commonWords\": 1,\n",
    "        \"totWordsLex\": {\"$size\": \"$wordList\"}\n",
    "    }},\n",
    "    {\"$addFields\": {\"totalWordsTwitter\": countWords}},\n",
    "    {\"$project\": {\n",
    "        \"perc_presence_lex_res\": { \"$round\": [{\"$divide\": [ \"$commonWords\", \"$totWordsLex\" ] }, 6] },\n",
    "        \"perc_presence_twitter\": { \"$round\": [{\"$divide\": [ \"$commonWords\", \"$totalWordsTwitter\" ]}, 6] }\n",
    "    }}\n",
    "]\n",
    "\n",
    "results = surpriseCollections.aggregate(pipeline)\n",
    "\n",
    "# Genero i dati per l'istogramma\n",
    "istogramSurprise_lexRes = []\n",
    "istogramSurprise_twitter = []\n",
    "\n",
    "for data in results:\n",
    "    print(data)\n",
    "    log =math.log(data['perc_presence_lex_res'],10)\n",
    "    istogramSurprise_lexRes.append([data['_id'], data['perc_presence_lex_res']])\n",
    "    log =math.log(data['perc_presence_twitter'],10)\n",
    "    istogramSurprise_twitter.append([data['_id'], data['perc_presence_twitter']])\n",
    " \n",
    "# Create the pandas DataFrame\n",
    "dfSurprise_lexRes = pd.DataFrame(istogramSurprise_lexRes, columns = ['Risorsa', 'Percentuale'])\n",
    "dfSurprise_twitter = pd.DataFrame(istogramSurprise_twitter,columns = ['Risorsa', 'Percentuale']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione istogramma per presenza parole nelle risorse lessicali\n",
    "\n",
    "dfSurprise_lexRes.plot.bar(x='Risorsa')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione istogramma per presenza parole nei twitter\n",
    "\n",
    "dfSurprise_twitter.plot.bar(x='Risorsa')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connessione, creazione cursore e database\n",
    "\n",
    "conn = psycopg2.connect(database = \"maadb\", user = \"postgres\", password = \"admin\", host = \"localhost\", port = \"5432\")\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione tabella risorse lessicali surprise e inserimento\n",
    "cur.execute('''CREATE TABLE RISORSESURPRISE\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    NOME             TEXT       NOT NULL,\n",
    "                    PAROLA           TEXT       NOT NULL\n",
    "                );\n",
    "            '''\n",
    ")\n",
    "\n",
    "nome = \"NRC_surprise\"\n",
    "for word in resourceSurpriseNRC:\n",
    "    cur.execute(\"INSERT INTO RISORSESURPRISE (NOME, PAROLA) VALUES(%s, %s)\",(nome,word))\n",
    "\n",
    "nome = \"sentisense_surprise\"\n",
    "for word in resourceSurpriseSentisense:\n",
    "    cur.execute(\"INSERT INTO RISORSESURPRISE (NOME, PAROLA) VALUES(%s, %s)\",(nome,word))\n",
    "\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione tabelle per le parole, emoji, emoticon e hashtag\n",
    "\n",
    "cur.execute('''CREATE TABLE PAROLESURPRISE\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    PAROLA           TEXT       NOT NULL,\n",
    "                    NRC_surprise      BOOL       NOT NULL DEFAULT FALSE,\n",
    "                    sentisense_surprise      BOOL       NOT NULL DEFAULT FALSE,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ")   \n",
    "\n",
    "cur.execute('''CREATE TABLE EMOJISURPRISE\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    EMOJI       TEXT       NOT NULL,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ") \n",
    "\n",
    "cur.execute('''CREATE TABLE EMOTICONSSURPRISE\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    EMOTICONS       TEXT       NOT NULL,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ") \n",
    "\n",
    "cur.execute('''CREATE TABLE HASHTAGSURPRISE\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    HASHTAG     TEXT       NOT NULL,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ") \n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inserimento hashtag, emoticon ed emoji\n",
    "\n",
    "for hashTag in hashtagSurprise:\n",
    "    cur.execute(\"INSERT INTO HASHTAGSURPRISE (HASHTAG,FREQUENCY) VALUES(%s,%s)\",(hashTag,0))\n",
    "\n",
    "for emoticons in emoticonSurprise:\n",
    "    cur.execute(\"INSERT INTO EMOTICONSSURPRISE (EMOTICONS,FREQUENCY) VALUES(%s,%s)\",(emoticons,0))\n",
    "\n",
    "for emojis in emojiSurprise:\n",
    "    cur.execute(\"INSERT INTO EMOJISURPRISE (EMOJI,FREQUENCY) VALUES(%s,%s)\",(emojis,0))\n",
    "\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inserimento parole in db\n",
    "for word in surpriseListWord:\n",
    "    cur.execute(\"INSERT INTO PAROLESURPRISE (PAROLA,NRC_surprise,sentisense_surprise,FREQUENCY) VALUES(%s,%s,%s,%s)\",(word,False,False,0))\n",
    "\n",
    "conn.commit() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze hashtag\n",
    "cur.execute(\"update hashtagsurprise as ha1 set frequency = (select count(*) from hashtagsurprise ha2 where ha1.hashtag = ha2.hashtag);\")\n",
    "cur.execute(\"delete from hashtagsurprise as ha1 where exists (Select 1 From hashtagsurprise as ha2 where ha1.hashtag = ha2.hashtag and ha1.frequency = ha2.frequency and ha2.ctid > ha1.ctid);\")\n",
    "\n",
    "conn.commit()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze emoji\n",
    "cur.execute(\"update EMOJISURPRISE as ha1 set frequency = (select count(*) from EMOJISURPRISE ha2 where ha1.emoji = ha2.emoji);\")\n",
    "cur.execute(\"delete from EMOJISURPRISE as ha1 where exists (Select 1 From EMOJISURPRISE as ha2 where ha1.emoji = ha2.emoji and ha1.frequency = ha2.frequency and ha2.ctid > ha1.ctid);\")\n",
    "\n",
    "conn.commit()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze emoticons\n",
    "cur.execute(\"update EMOTICONSSURPRISE as ha1 set frequency = (select count(*) from EMOTICONSSURPRISE ha2 where ha1.emoticons = ha2.emoticons);\")\n",
    "cur.execute(\"delete from EMOTICONSSURPRISE as ha1 where exists (Select 1 From EMOTICONSSURPRISE as ha2 where ha1.emoticons = ha2.emoticons and ha1.frequency = ha2.frequency and ha2.ctid > ha1.ctid);\")\n",
    "\n",
    "conn.commit()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze parole\n",
    "cur.execute(\"select count(*),parola from parolesurprise group by parola\")\n",
    "result = []\n",
    "for word in cur.fetchall():\n",
    "    result.append(word) \n",
    "\n",
    "cur.execute(\"truncate table parolesurprise;\")\n",
    "cur.execute(\"ALTER SEQUENCE parolesurprise_id_seq RESTART WITH 1;\")\n",
    "\n",
    "for couple in result:\n",
    "    cur.execute(\"INSERT INTO PAROLESURPRISE (PAROLA,NRC_surprise,sentisense_surprise,FREQUENCY) VALUES(%s,%s,%s,%s)\",(couple[1],False,False,couple[0]))\n",
    "\n",
    "conn.commit()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggiornamento presenza parola nelle risorse lessicali\n",
    "cur.execute(\"UPDATE parolesurprise SET NRC_SURPRISE = TRUE WHERE parola IN (select p.parola from parolesurprise as p LEFT OUTER JOIN risorsesurprise as r ON p.parola = r.parola WHERE r.nome = 'NRC_surprise')\")\n",
    "cur.execute(\"UPDATE parolesurprise SET sentisense_surprise = TRUE WHERE parola IN (select p.parola from parolesurprise as p LEFT OUTER JOIN risorsesurprise as r ON p.parola = r.parola WHERE r.nome = 'sentisense_surprise')\")\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvataggio in un txt delle parole non presenti in nessuna risorsa lessicale\n",
    "\n",
    "wordsMongo = surpriseCollections.find({\"LexResources\": []}, {\"_id\": 0, \"freq\": 0, \"LexResources\": 0})\n",
    "words = []\n",
    "\n",
    "for word in wordsMongo:\n",
    "    words.append(word['word'])\n",
    "\n",
    "with open('Nuove Parole/surprise.txt', 'w',encoding=\"utf8\") as f:\n",
    "    for word in words:\n",
    "        f.write(\"%s\\n\" % word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trattamento dataset Trust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lista di righe, ogni riga una tweet trust\n",
    "file = open(\"Dataset/dataset_dt_trust_60k.txt\", 'r',encoding=\"utf8\")\n",
    "trust = []\n",
    "hashtagTrust = []\n",
    "emoticonTrust = []\n",
    "emojiTrust = []\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "    riga = re.sub(r'URL', '',riga)\n",
    "    riga = re.sub(r'USERNAME', '',riga)\n",
    "    \n",
    "    #array di hashtag singoli\n",
    "    hashTag = (re.findall('#[a-zA-Z0-9]+', riga))\n",
    "    \n",
    "    #generiamo l'array di hashtag senza hashtag\n",
    "    for h in hashTag:\n",
    "        hashtagTrust.append(h.replace(\"#\",\"\"))\n",
    "\n",
    "    #Rimuoviamo glin hashtag dalla riga\n",
    "    riga = riga.replace('#',\"\")\n",
    "\n",
    "    trust.append(riga)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(trust)):\n",
    "    #Controllo se l'emoticons c'è e in caso la rimuovo\n",
    "    for singleEmoticon in emoticonsList:\n",
    "        emoticonsFind = re.findall(re.escape(singleEmoticon),trust[i])\n",
    "        if(len(emoticonsFind) != 0):\n",
    "            emoticonTrust.extend(emoticonsFind)\n",
    "            trust[i] = trust[i].replace(singleEmoticon,\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(trust)):\n",
    "    #preleviamo le emoji e le rimuoviamo dal testo\n",
    "    data = regex.findall(r'\\X', trust[i])\n",
    "    for word in data:\n",
    "        if any(char in emoji.UNICODE_EMOJI['en'] for char in word):\n",
    "            emojiTrust.append(word)\n",
    "            trust[i] = trust[i].replace(word, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processamento slang prima di punteggiatura e lowercase\n",
    "with open(\"Slangs/slangs_before.json\") as f:\n",
    "    slangsBefore = json.load(f)\n",
    "    for i in range(len(trust)):\n",
    "        for word in trust[i].split():\n",
    "            indexOfWord = trust[i].find(word)\n",
    "            if slangsBefore.get(word) is None:\n",
    "                if slangsBefore.get(word.lower()) is None:\n",
    "                    if slangsBefore.get(word.upper()) is None:\n",
    "                        continue\n",
    "                    else:\n",
    "                        trust[i] = trust[i][:indexOfWord] + slangsBefore.get(word.upper()) + trust[i][indexOfWord+len(word):]\n",
    "                else:\n",
    "                    trust[i] = trust[i][:indexOfWord] + slangsBefore.get(word.lower()) + trust[i][indexOfWord+len(word):]\n",
    "            else:\n",
    "                trust[i] = trust[i][:indexOfWord] + slangsBefore.get(word) + trust[i][indexOfWord+len(word):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizzazione di tutte le frasi\n",
    "trustTokenized = []\n",
    "for linea in trust:\n",
    "    result = word_tokenize(linea)\n",
    "    trustTokenized.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Esecuzione della POS tagging per ogni riga della matrice\n",
    "for i in range(len(trustTokenized)):\n",
    "    trustTokenized[i] = nltk.pos_tag(trustTokenized[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trattamento punteggiatura\n",
    "for line in trustTokenized:\n",
    "    newLine = line\n",
    "    listToRemove = []\n",
    "    for i in range(len(line)):\n",
    "        if line[i][0] in puntuaction:\n",
    "            listToRemove.append(line[i])\n",
    "    for element in listToRemove:\n",
    "        newLine.remove(element)\n",
    "    line = newLine\n",
    "\n",
    "linePuntuaction = re.compile(\"[\" + re.escape(\"\".join(puntuaction)) + \"]\")\n",
    "for line in trustTokenized:\n",
    "    for i in range(len(line)):\n",
    "        tupla = [(re.sub(linePuntuaction, \"\", line[i][0])),line[i][1]]\n",
    "        line[i] = tupla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming\n",
    "#ps = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for line in trustTokenized:\n",
    "    for i in range(len(line)):\n",
    "        tupla = [lemmatizer.lemmatize(line[i][0]),line[i][1]]\n",
    "        line[i] = tupla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trattamnento Stop word dopo Stemming\n",
    "stop_words = set(stopwords.words('english'))\n",
    "trustTokStoped = []\n",
    "for line in trustTokenized:\n",
    "    filtred_word = []\n",
    "    for row in line:\n",
    "        if  row[0] not in stop_words:\n",
    "            filtred_word.append(row) \n",
    "    trustTokStoped.append(filtred_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trustListWord = []\n",
    "trustListTok = []\n",
    "for line in trustTokStoped:\n",
    "    for row in line:\n",
    "        if row[0] != '':\n",
    "            trustListWord.append(row[0])\n",
    "        trustListTok.append(row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lavorazione risorse lessicali\n",
    "resourceTrustNRC = []\n",
    "file = open(\"Risorse lessicali\\\\Trust\\\\NRC_trust.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        resourceTrustNRC.append(riga)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Caricare le risorse lessicali \n",
    "TrustNRC = {\n",
    "    '_id': 'NRC_trust',\n",
    "    'sentiment': 'trust',  \n",
    "    'wordList': resourceTrustNRC\n",
    "}\n",
    "\n",
    "resourcesCollection.insert_one(TrustNRC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione database e collezioni\n",
    "# Inserimento singoli documenti contententi i tweet trust\n",
    "wordsString = {}\n",
    "\n",
    "for i in range(len(trustListWord)):\n",
    "    wordsString = {\n",
    "        \"lemma\": trustListWord[i],\n",
    "        \"POS\": trustListTok[i]\n",
    "    }\n",
    "    trustTwit={\"word\":wordsString}\n",
    "    trustCollections.insert_one(trustTwit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inserimento collezione Twitter con hashtags, emoticons ed emoji \n",
    "trustInsert = {\n",
    "    \"sentiment\":\"trust\",\n",
    "    \"hashtags\": hashtagTrust,\n",
    "    \"emoticons\": emoticonTrust,\n",
    "    \"emoji\": emojiTrust\n",
    "}\n",
    "\n",
    "#inserimento\n",
    "x = collectionTrust.insert_one(trustInsert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raggruppa i documenti di input in base all'espressione _id specificata e per ciascun raggruppamento distinto genera un documento\n",
    "pipeline = [\n",
    "    {\"$group\": {\"_id\": \"$word.lemma\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"word\": \"$_id\",\n",
    "      \"freq\": \"$count\"\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"freq\": -1\n",
    "    }},\n",
    "    {\"$out\": \"trust\"}\n",
    "]\n",
    "\n",
    "#post processing per renderlo valido \n",
    "trustCollections.aggregate(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creazione Word Clouds per le parole\n",
    "data = {}\n",
    "for document in trustCollections.find().limit(50):\n",
    "    data[document['word']] = document['freq']\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2)\n",
    "wc.fit_words(data)\n",
    "wc.to_file('Word Clouds/Trust/trustWords.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conteggio degli hashtags\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$hashtags\"},\n",
    "    {\"$group\": {\"_id\": \"$hashtags\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"word\": \"$_id\",\n",
    "      \"count\": 1\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"count\": -1\n",
    "    }}\n",
    "]\n",
    "\n",
    "hashtagsCount = collectionTrust.aggregate(pipeline)\n",
    "\n",
    "data = {}\n",
    "for document in hashtagsCount:\n",
    "    data[document['word']] = document['count']\n",
    "\n",
    "collectionTrust.update_one(\n",
    "  {\"sentiment\": \"trust\"},\n",
    "  {\"$set\": {\"hashtags\": data}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creazione Word Clouds per gli hashtags\n",
    "\n",
    "hashtags = collectionTrust.distinct(\"hashtags\")\n",
    "\n",
    "hashtagsLimited = {}\n",
    "for (word,count) in [x for x in hashtags[0].items()][:50]:\n",
    "    hashtagsLimited[word] = count\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2)\n",
    "wc.fit_words(hashtagsLimited)\n",
    "wc.to_file('Word Clouds/Trust/trustHashtags.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conteggio delle emoji\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$emoji\"},\n",
    "    {\"$group\": {\"_id\": \"$emoji\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"emoji\": \"$_id\",\n",
    "      \"count\": 1\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"count\": -1\n",
    "    }}\n",
    "]\n",
    "\n",
    "emojiCount = collectionTrust.aggregate(pipeline)\n",
    "\n",
    "data = {}\n",
    "for document in emojiCount:\n",
    "    data[document['emoji']] = document['count']\n",
    "\n",
    "collectionTrust.update_one(\n",
    "  {\"sentiment\": \"trust\"},\n",
    "  {\"$set\": {\"emoji\": data}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creazione Word Clouds per le emoji\n",
    "\n",
    "emojis = collectionTrust.distinct(\"emoji\")\n",
    "\n",
    "emojisLimited = {}\n",
    "for (word,count) in [x for x in emojis[0].items()][:50]:\n",
    "    emojisLimited[word] = count\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2, font_path='Font Emoji\\Symbola.otf')\n",
    "wc.fit_words(emojisLimited)\n",
    "wc.to_file('Word Clouds/Trust/trustEmojis.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conteggio degli emoticons\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$emoticons\"},\n",
    "    {\"$group\": {\"_id\": \"$emoticons\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"emoticons\": \"$_id\",\n",
    "      \"count\": 1\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"count\": -1\n",
    "    }}\n",
    "]\n",
    "\n",
    "emoticonsCount = collectionTrust.aggregate(pipeline)\n",
    "\n",
    "data = {}\n",
    "for document in emoticonsCount:\n",
    "    data[document['emoticons']] = document['count']\n",
    "\n",
    "collectionTrust.update_one(\n",
    "  {\"sentiment\": \"trust\"},\n",
    "  {\"$set\": {\"emoticons\": data}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creazione Word Clouds per le emoticons\n",
    "\n",
    "emoticons = collectionTrust.distinct(\"emoticons\")\n",
    "\n",
    "emoticonsLimited = {}\n",
    "for (word,count) in [x for x in emoticons[0].items()][:50]:\n",
    "    emoticonsLimited[word] = count\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2)\n",
    "wc.fit_words(emoticonsLimited)\n",
    "wc.to_file('Word Clouds/Trust/trustEmoticons.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline di aggregazione per inserire la risorsa lessicale a cui appartiene la parola\n",
    "\n",
    "pipeline = [\n",
    "    {\"$lookup\": {\n",
    "        \"from\": \"Resources\",\n",
    "        \"localField\": \"word\",\n",
    "        \"foreignField\": \"wordList\",\n",
    "        \"pipeline\": [\n",
    "            {\"$match\": {\"$expr\": {\"$regexMatch\": {\"input\": \"$_id\", \"regex\": \"trust\"}}}}\n",
    "        ],\n",
    "        \"as\": \"LexResources\"\n",
    "    }},\n",
    "    {\"$project\": {\n",
    "        \"resources.wordList\": 0,\n",
    "        \"resources.totNumberWords\": 0,\n",
    "        \"resources.sentiment\": 0,\n",
    "        \"LexResources.sentiment\": 0,\n",
    "        \"LexResources.totNumberWords\": 0,\n",
    "        \"LexResources.wordList\": 0\n",
    "    }},\n",
    "    {\"$out\": \"trust\"}\n",
    "]\n",
    "\n",
    "trustCollections.aggregate(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline di aggregazione per calcolare le percentuali richieste\n",
    "\n",
    "pipelineCount = [\n",
    "    {\"$group\": {\"_id\": \"null\", \"count\": { \"$sum\": \"$freq\" }}},\n",
    "    {\"$project\": {\n",
    "        \"_id\": 0,\n",
    "        \"count\": \"$count\"\n",
    "    }}\n",
    "]\n",
    "\n",
    "results = trustCollections.aggregate(pipelineCount)\n",
    "\n",
    "countWords = 0\n",
    "for data in results:\n",
    "    countWords = data['count']\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$LexResources\"},\n",
    "    {\"$lookup\": {\n",
    "        \"from\": \"Resources\",\n",
    "        \"localField\": \"LexResources._id\",\n",
    "        \"foreignField\": \"_id\",\n",
    "        \"as\": \"RefResource\"\n",
    "    }},\n",
    "    {\"$group\": {\"_id\": { \"lex\": \"$LexResources._id\", \"RefResource\": \"$RefResource\" }, \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "        \"_id\": \"$_id.lex\",\n",
    "        \"wordList\": \"$_id.RefResource.wordList\",\n",
    "        \"commonWords\": \"$count\"\n",
    "    }},\n",
    "    {\"$unwind\": \"$wordList\"},\n",
    "    {\"$project\": {\n",
    "        \"_id\": 1,\n",
    "        \"commonWords\": 1,\n",
    "        \"totWordsLex\": {\"$size\": \"$wordList\"}\n",
    "    }},\n",
    "    {\"$addFields\": {\"totalWordsTwitter\": countWords}},\n",
    "    {\"$project\": {\n",
    "        \"perc_presence_lex_res\": { \"$round\": [{\"$divide\": [ \"$commonWords\", \"$totWordsLex\" ] }, 6] },\n",
    "        \"perc_presence_twitter\": { \"$round\": [{\"$divide\": [ \"$commonWords\", \"$totalWordsTwitter\" ]}, 6] }\n",
    "    }}\n",
    "]\n",
    "\n",
    "results = trustCollections.aggregate(pipeline)\n",
    "\n",
    "# Genero i dati per l'istogramma\n",
    "istogramTrust_lexRes = []\n",
    "istogramTrust_twitter = []\n",
    "\n",
    "for data in results:\n",
    "    print(data)\n",
    "    log =math.log(data['perc_presence_lex_res'],10)\n",
    "    istogramTrust_lexRes.append([data['_id'], data['perc_presence_lex_res']])\n",
    "    log =math.log(data['perc_presence_twitter'],10)\n",
    "    istogramTrust_twitter.append([data['_id'], data['perc_presence_twitter']])\n",
    " \n",
    "# Create the pandas DataFrame\n",
    "dfTrust_lexRes = pd.DataFrame(istogramTrust_lexRes, columns = ['Risorsa', 'Percentuale'])\n",
    "dfTrust_twitter = pd.DataFrame(istogramTrust_twitter,columns = ['Risorsa', 'Percentuale'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione istogramma per presenza parole nelle risorse lessicali\n",
    "\n",
    "dfTrust_lexRes.plot.bar(x='Risorsa')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione istogramma per presenza parole nei twitter\n",
    "\n",
    "dfTrust_twitter.plot.bar(x='Risorsa')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connessione, creazione cursore e database\n",
    "\n",
    "conn = psycopg2.connect(database = \"maadb\", user = \"postgres\", password = \"admin\", host = \"localhost\", port = \"5432\")\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione tabella risorse lessicali trust e inserimento\n",
    "cur.execute('''CREATE TABLE RISORSETRUST\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    NOME             TEXT       NOT NULL,\n",
    "                    PAROLA           TEXT       NOT NULL\n",
    "                );\n",
    "            '''\n",
    ")\n",
    "\n",
    "nome = \"NRC_trust\"\n",
    "for word in resourceTrustNRC:\n",
    "    cur.execute(\"INSERT INTO RISORSETRUST (NOME, PAROLA) VALUES(%s, %s)\",(nome,word))\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione tabelle per le parole, emoji, emoticon e hashtag\n",
    "\n",
    "cur.execute('''CREATE TABLE PAROLETRUST\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    PAROLA           TEXT       NOT NULL,\n",
    "                    NRC_trust      BOOL       NOT NULL DEFAULT FALSE,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ")   \n",
    "\n",
    "cur.execute('''CREATE TABLE EMOJITRUST\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    EMOJI       TEXT       NOT NULL,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ") \n",
    "\n",
    "cur.execute('''CREATE TABLE EMOTICONSTRUST\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    EMOTICONS       TEXT       NOT NULL,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ") \n",
    "\n",
    "cur.execute('''CREATE TABLE HASHTAGTRUST\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    HASHTAG     TEXT       NOT NULL,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ") \n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inserimento hashtag, emoticon ed emoji\n",
    "\n",
    "for hashTag in hashtagTrust:\n",
    "    cur.execute(\"INSERT INTO HASHTAGTRUST (HASHTAG,FREQUENCY) VALUES(%s,%s)\",(hashTag,0))\n",
    "\n",
    "for emoticons in emoticonTrust:\n",
    "    cur.execute(\"INSERT INTO EMOTICONSTRUST (EMOTICONS,FREQUENCY) VALUES(%s,%s)\",(emoticons,0))\n",
    "\n",
    "for emojis in emojiTrust:\n",
    "    cur.execute(\"INSERT INTO EMOJITRUST (EMOJI,FREQUENCY) VALUES(%s,%s)\",(emojis,0))\n",
    "\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inserimento parole in db\n",
    "for word in trustListWord:\n",
    "    cur.execute(\"INSERT INTO PAROLETRUST (PAROLA,NRC_trust,FREQUENCY) VALUES(%s,%s,%s)\",(word,False,0))\n",
    "\n",
    "conn.commit()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze hashtag\n",
    "cur.execute(\"update HASHTAGTRUST as ha1 set frequency = (select count(*) from HASHTAGTRUST ha2 where ha1.hashtag = ha2.hashtag);\")\n",
    "cur.execute(\"delete from HASHTAGTRUST as ha1 where exists (Select 1 From HASHTAGTRUST as ha2 where ha1.hashtag = ha2.hashtag and ha1.frequency = ha2.frequency and ha2.ctid > ha1.ctid);\")\n",
    "\n",
    "conn.commit()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze emoji\n",
    "cur.execute(\"update EMOJITRUST as ha1 set frequency = (select count(*) from EMOJITRUST ha2 where ha1.emoji = ha2.emoji);\")\n",
    "cur.execute(\"delete from EMOJITRUST as ha1 where exists (Select 1 From EMOJITRUST as ha2 where ha1.emoji = ha2.emoji and ha1.frequency = ha2.frequency and ha2.ctid > ha1.ctid);\")\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze emoticons\n",
    "cur.execute(\"update EMOTICONSTRUST as ha1 set frequency = (select count(*) from EMOTICONSTRUST ha2 where ha1.emoticons = ha2.emoticons);\")\n",
    "cur.execute(\"delete from EMOTICONSTRUST as ha1 where exists (Select 1 From EMOTICONSTRUST as ha2 where ha1.emoticons = ha2.emoticons and ha1.frequency = ha2.frequency and ha2.ctid > ha1.ctid);\")\n",
    "\n",
    "conn.commit()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze parole\n",
    "cur.execute(\"select count(*),parola from PAROLETRUST group by parola\")\n",
    "result = []\n",
    "for word in cur.fetchall():\n",
    "    result.append(word) \n",
    "\n",
    "cur.execute(\"truncate table PAROLETRUST;\")\n",
    "cur.execute(\"ALTER SEQUENCE paroletrust_id_seq RESTART WITH 1;\")\n",
    "\n",
    "for couple in result:\n",
    "    cur.execute(\"INSERT INTO PAROLETRUST (PAROLA,NRC_trust,FREQUENCY) VALUES(%s,%s,%s)\",(couple[1],False,couple[0]))\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggiornamento presenza parola nelle risorse lessicali\n",
    "cur.execute(\"UPDATE PAROLETRUST SET NRC_trust = TRUE WHERE parola IN (select p.parola from PAROLETRUST as p LEFT OUTER JOIN RISORSETRUST as r ON p.parola = r.parola WHERE r.nome = 'NRC_trust')\")\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvataggio in un txt delle parole non presenti in nessuna risorsa lessicale\n",
    "\n",
    "wordsMongo = trustCollections.find({\"LexResources\": []}, {\"_id\": 0, \"freq\": 0, \"LexResources\": 0})\n",
    "words = []\n",
    "\n",
    "for word in wordsMongo:\n",
    "    words.append(word['word'])\n",
    "\n",
    "with open('Nuove Parole/trust.txt', 'w',encoding=\"utf8\") as f:\n",
    "    for word in words:\n",
    "        f.write(\"%s\\n\" % word)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "727cd1b8d9e0a8fe67ccb60934071f3fe824c12f3f18648f9575d2adb7fe198a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
