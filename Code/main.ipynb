{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\gabri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\gabri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\gabri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\gabri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\gabri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Import per il resto delle celle\n",
    "import re\n",
    "import emoji\n",
    "import regex\n",
    "import nltk\n",
    "import json\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from collections import Counter\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "import pymongo\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import psycopg2\n",
    "%matplotlib inline\n",
    "#from wordcloud import WordCloud\n",
    "\n",
    "emoticons = [':‑)',':)',':-]',':]',':->',':>','8-)','8)',':-}',':}',':o)',':c)',':^)','=]','=)',':‑D',':D','8‑D','8D','=D','=3','B^D','c:',\n",
    "'C:','x‑D','xD','X‑D','XD',':-))',':‑(',':(',':‑c',':c',':‑<',':<',':‑[',':[',':-||','>:[',':{',':@',':('',;(',\":'‑(\",\":'(\",':=(',\":'‑)\",\":')\",\n",
    "':\"D',\"D‑':\",'D:<','D:','D8','D;','D=','DX',':‑O',':O',':‑o',':o',':-0','8‑0','>:O','=O','=o','=0',':-3',':3','=3','x3','X3','>:3',':-*',\n",
    "':*',':×',';‑)',';)','*-)','*)',';‑]',';]',';^)',';>',':‑,',';D',';3',':‑P',':P','X‑P','XP','x‑p','xp',':‑p',':p',':‑Þ',':Þ',':‑þ',':þ',':‑b',':b',\n",
    "'d:','=p','>:P',':-/',':/',':‑.',\">:/\",'=/',':L','=L',':S',':‑|',':|',':$','://)','://3',':‑X',':X',':‑#',':#',':‑&',':&','O:‑)','O:)','0:‑3'\n",
    ",'0:3','0:‑)','0:)','0;^)','>:‑)','>:)','}:‑)','}:)','3:‑)','3:)','>;‑)','>;)','>:3',';3','|;‑)','|‑O','B-)',':‑J','#‑)','%‑)','%)',':‑###..'\n",
    ",':###..','<:‑|',\"',:-|\",\"',:-l\",':E','8-X','8=X','x-3','x=3','~:>','v.v','._.','._.;','QQ  ','qq','Qq','X_X','x_x','+_+','X_x','x_X'\n",
    "'<_<','>_>','<.<','>.>','O_O','o_o','O-O','o‑o','O_o','o_O','>.<','>_<','^5','o/\\o','>_>^ ^<_<','V.v.V','V=(° °)=V','(^^^)','(::[]::)',\n",
    "'(o)(o)','( • )( • )','[̲̅$̲̅(̲̅1̲̅)̲̅$̲̅]','[̲̅$̲̅(̲̅5̲̅)̲̅$̲̅]','[̲̅$̲̅(̲̅10)̲̅$̲̅]','[̲̅$̲̅(̲̅100)̲̅$̲̅]','( ͡° ͜ʖ ͡°)','ヽ༼ຈل͜ຈ༽ﾉ','(๑ˇεˇ๑)','(◕‿◕✿)','( ༎ຶ ۝ ༎ຶ )','(=ʘᆽʘ=)∫','ʕ •ᴥ•ʔ',\n",
    "'(>_<)','(>_<)>','(>w<)',\"(';')\",'(^_^;)','(-_-;)','(~_~;)','(・.・;)','(・_・;)','^^;','(・・;)','^_^;','(#^.^#)'\n",
    ",'(^^;)','(⁄ ⁄•⁄ω⁄•⁄ ⁄)','(-.-)y-°°°','(^.^)y-.o○','(-_-)zzz','(^_-)-☆','(^_-)','((+_+))','(+o+)','(°°)','(°-°) (°.°)',\n",
    "'(°_°)','(°_°>)','(°レ°)','(o|o)','<(｀^´)>','^_^','(°o°)','(^_^)/','(^O^)／','(^o^)／','(^^)/','(≧∇≦)/','(/◕ヮ◕)/','(^o^)丿','∩(·ω·)∩',\n",
    "'(·ω·)','^ω^','_(._.)_','_(_^_)_','<(_ _)>','<m(__)m>','m(__)m','m(_ _)m','(凸ಠ益ಠ)凸',\"('_')\",'(/_;)','(T_T)','(;_;)','(;_;','(;_:)','(;O;)'\n",
    ",'(:_;)','(ToT)','(Ｔ▽Ｔ)',';_;',';-;',';n;',';;','Q.Q','T.T','TnT','QQ','Q_Q',\n",
    "'(ー_ー)!!','(-.-)','(-_-)','(一一)','(；一_一)','(=_=)','(=^・^=)','(=^・・^=)','=^_^=','(..)','(._.)',\n",
    "'.o○','○o.','_旦~~','( ^^)','_U~~','( ^^)','☆ミ','☆彡','>°)))彡','(Q))',\n",
    "'><ヨヨ (°))<<','>°))))彡','<°)))彡','>°))彡','<+','))><<','<*))','>=<','<コ:彡',\n",
    "'Ｃ:.ミ','~>°)～～～','～°·_·°～','(°°)～','●～*','￣|○',':3ミ','^m^','(・・?','(?_?)','>^_^<','<^!^>',\n",
    "'^/^','（*^_^*','§^.^§','(^<^)','(^.^)','(^ム^)','(^·^)','(^.^)','(^_^.)','(^_^)','(^^)','(^J^)',\n",
    "'(*^.^*)','^_^','(#^.^#)','（^—^）','(^^)/~~~','(^_^)/~','(;_;)/~~~','(^.^)/~~~',\n",
    "'(-_-)/~~~','($··)/~~~','(@^^)/~~~','(T_T)/~~~','(V)o￥o(V)','＼(~o~)／','＼(^o^)／','＼(-o-)／','ヽ(^。^)ノ',\n",
    "'ヽ(^o^)丿','(*^0^*)','(*_*)','(*_*;','(+_+) (@_@)','(@_@。','(＠_＠;)',\n",
    " '＼(◎o◎)／！','(*^^)v','(^^)v','(^_^)v','（’-’*) (＾ｖ＾)','(＾▽＾)','(・∀・)','(´∀`)','(⌒▽⌒）',\n",
    " '＼(^o^)／','\\(^o^)/','(~o~)','(~_~)','(^^ゞ','ˊ＿>ˋ','(p_-)', '(-_q)','((d[-_-]b))','(-\"-)','(ーー゛)',\n",
    " '(^_^メ)', '(-_-メ)','(~_~メ)','(－－〆)','(・へ・)','(｀´)'\n",
    " , '<`～´>','<`ヘ´>','(ーー;)','(^0_0^)','(＾ｖ＾)', '(＾ｕ＾)','(＾◇＾)','( ^)o(^ )','(^O^)','(^o^)','(^○^)',')^o^(',\n",
    " '(*^▽^*)','(￣ー￣)','(￣□￣;)', '°o°','°O°',':O','o_O','o_0', 'o.O','(o.o)','oO','(°◇°)','（ ﾟ Дﾟ)','(*￣m￣)',\n",
    " 'ヽ(´ー｀)┌','¯\\_(ツ)_/¯','¯\\(°_o)/¯','(´･ω･`)', '(‘A`)','(づ￣ ³￣)づ','(*^3^)/~☆','(︶｡︶✽)','(-_-) zzz'\n",
    " , 'uwu','UwU','OWO','OwO','～°·_·°～', '(°°)～',':3ミ','(´･ω･`)','(`･ω･´)','(｀-´)>', '（　´_ゝ`）'] \n",
    "\n",
    "puntuaction = ['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '.',\n",
    "           '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', \n",
    "           '`', '{', '|', '}', '~', '»', '«', '“', '”', \"-\", \"...\",\"..\",\".,\",\",.\"]\n",
    "\n",
    "\n",
    "mongoClient = pymongo.MongoClient(\"mongodb://admin:admin@localhost:27017/\")\n",
    "mydb = mongoClient[\"maadb\"]\n",
    "collectionTwitter = mydb[\"Twitter\"] #Questa é per la collezione dei emoji emoticon hashtag per ogni emozione\n",
    "angerCollections = mydb[\"anger\"] #Collezioni per solo i twitt di anger\n",
    "anticipationCollections = mydb[\"anticipation\"]\n",
    "disgustCollections = mydb[\"disgust\"]\n",
    "fearCollections = mydb[\"fear\"]\n",
    "joyCollections = mydb[\"joy\"]\n",
    "sadnessCollections = mydb[\"sadness\"]\n",
    "surpriseCollections = mydb[\"surprise\"]\n",
    "trustCollections = mydb[\"trust\"]\n",
    "resourcesCollection = mydb[\"Resources\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trattamento dataset Anger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lista di righe, ogni riga una tweet anger\n",
    "file = open(\"Dataset/dataset_dt_anger_60k.txt\", 'r',encoding=\"utf8\")\n",
    "anger = []\n",
    "hashtagAnger = []\n",
    "emoticonAnger = []\n",
    "emojiAnger = []\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    #Eliminazione URL e USERNAME\n",
    "    riga = re.sub(r'URL', '',riga)\n",
    "    riga = re.sub(r'USERNAME', '',riga)\n",
    "\n",
    "    #array di hashtag singoli\n",
    "    hashTag = (re.findall('#[a-zA-Z0-9]+', riga))\n",
    "\n",
    "     #generiamo l'array di hashtag senza hashtag\n",
    "    for h in hashTag:\n",
    "        hashtagAnger.append(h.replace(\"#\",\"\"))\n",
    "\n",
    "    #Rimuoviamo glin hashtag dalla riga\n",
    "    riga = riga.replace('#',\"\")\n",
    "\n",
    "    #Salvataggio frase\n",
    "    anger.append(riga)\n",
    "\n",
    "\n",
    "file.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(anger)):\n",
    "    #Controllo se l'emoticons c'è e in caso la rimuovo\n",
    "    for singleEmoticon in emoticons:\n",
    "        emoticonsFind = re.findall(re.escape(singleEmoticon),anger[i])\n",
    "        if(len(emoticonsFind) != 0):\n",
    "            emoticonAnger.extend(emoticonsFind)\n",
    "            anger[i] = anger[i].replace(singleEmoticon,\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(anger)):\n",
    "    #preleviamo le emoji e le rimuoviamo dal testo\n",
    "    data = regex.findall(r'\\X', anger[i])\n",
    "    for word in data:\n",
    "        if any(char in emoji.UNICODE_EMOJI['en'] for char in word):\n",
    "            emojiAnger.append(word)\n",
    "            anger[i] = anger[i].replace(word, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processamento slang \n",
    "with open(\"Slangs/slangs_before.json\") as f:\n",
    "    slangsBefore = json.load(f)\n",
    "    for i in range(len(anger)):\n",
    "        for word in anger[i].split():\n",
    "            indexOfWord = anger[i].find(word)\n",
    "            if slangsBefore.get(word) is None:\n",
    "                if slangsBefore.get(word.lower()) is None:\n",
    "                    if slangsBefore.get(word.upper()) is None:\n",
    "                        continue\n",
    "                    else:\n",
    "                        anger[i] = anger[i][:indexOfWord] + slangsBefore.get(word.upper()) + anger[i][indexOfWord+len(word):]\n",
    "                else:\n",
    "                    anger[i] = anger[i][:indexOfWord] + slangsBefore.get(word.lower()) + anger[i][indexOfWord+len(word):]\n",
    "            else:\n",
    "                anger[i] = anger[i][:indexOfWord] + slangsBefore.get(word) + anger[i][indexOfWord+len(word):]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizzazione di tutte le frasi\n",
    "angerTokenized = []\n",
    "for linea in anger:\n",
    "    result = word_tokenize(linea)\n",
    "    angerTokenized.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Esecuzione della POS tagging per ogni riga della matrice\n",
    "for i in range(len(angerTokenized)):\n",
    "    angerTokenized[i] = nltk.pos_tag(angerTokenized[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trattamento punteggiatura\n",
    "for line in angerTokenized:\n",
    "    newLine = line\n",
    "    listToRemove = []\n",
    "    for i in range(len(line)):\n",
    "        if line[i][0] in puntuaction:\n",
    "            listToRemove.append(line[i])\n",
    "    for element in listToRemove:\n",
    "        newLine.remove(element)\n",
    "    line = newLine\n",
    "\n",
    "linePuntuaction = re.compile(\"[\" + re.escape(\"\".join(puntuaction)) + \"]\")\n",
    "for line in angerTokenized:\n",
    "    for i in range(len(line)):\n",
    "        tupla = [(re.sub(linePuntuaction, \"\", line[i][0])),line[i][1]]\n",
    "        line[i] = tupla\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming\n",
    "#ps = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for line in angerTokenized:\n",
    "    for i in range(len(line)):\n",
    "        tupla = [lemmatizer.lemmatize(line[i][0]),line[i][1]]\n",
    "        line[i] = tupla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trattamnento Stop word dopo Stemming\n",
    "stop_words = set(stopwords.words('english'))\n",
    "angerTokStoped = []\n",
    "for line in angerTokenized:\n",
    "    filtred_word = []\n",
    "    for row in line:\n",
    "        if  row[0] not in stop_words:\n",
    "            filtred_word.append(row) \n",
    "    angerTokStoped.append(filtred_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#liste singole di parole e token\n",
    "angerListWord = []\n",
    "angerListTok = []\n",
    "for line in angerTokStoped:\n",
    "    for row in line:\n",
    "        angerListWord.append(row[0])\n",
    "        angerListTok.append(row[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lavorazione risorse lessicali\n",
    "resourceAngerEmoSN = []\n",
    "resourceAngerNRC = []\n",
    "resourceAngerSentisense = []\n",
    "\n",
    "#Lista di righe, ogni riga una tweet anger\n",
    "file = open(\"Risorse lessicali\\Anger\\EmoSN_anger.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        resourceAngerEmoSN.append(riga)\n",
    "\n",
    "file.close()\n",
    "\n",
    "file = open(\"Risorse lessicali\\\\Anger\\\\NRC_anger.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        resourceAngerNRC.append(riga)\n",
    "\n",
    "file.close()\n",
    "\n",
    "file = open(\"Risorse lessicali\\Anger\\sentisense_anger.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        resourceAngerSentisense.append(riga)\n",
    "\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Caricare le risorse lessicali \n",
    "\n",
    "AngerEmoSN = {\n",
    "    '_id': 'EmoSN_anger',\n",
    "    'sentiment': 'anger',\n",
    "    'totNumberWords': len(resourceAngerEmoSN),  \n",
    "    'wordList': resourceAngerEmoSN\n",
    "}\n",
    "\n",
    "AngerNRC = {\n",
    "    '_id': 'NRC_anger',\n",
    "    'sentiment': 'anger',\n",
    "    'totNumberWords': len(resourceAngerNRC),  \n",
    "    'wordList': resourceAngerNRC\n",
    "}\n",
    "\n",
    "AngerSentisense = {\n",
    "    '_id': 'sentisense_anger',\n",
    "    'sentiment': 'anger',\n",
    "    'totNumberWords': len(resourceAngerSentisense),  \n",
    "    'wordList': resourceAngerSentisense\n",
    "}\n",
    "\n",
    "resourcesCollection.insert_one(AngerEmoSN)\n",
    "resourcesCollection.insert_one(AngerNRC)\n",
    "resourcesCollection.insert_one(AngerSentisense)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conteggio delle frequenze delle parole\n",
    "countWordAnger = Counter(angerListWord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione database e collezioni\n",
    "# Inserimento singoli documenti contententi i tweet anger\n",
    "angerMongo = []\n",
    "wordsString = {}\n",
    "\n",
    "for i in range(len(angerListWord)):\n",
    "    wordsString = {\n",
    "        \"lemma\": angerListWord[i],\n",
    "        \"POS\": angerListTok[i]\n",
    "    }\n",
    "    angerTwit={\"word\":wordsString}\n",
    "    angerCollections.insert_one(angerTwit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using a JSON string\n",
    "#with open('jsonAnger_data.txt', 'w') as outfile:\n",
    "#    outfile.write(json.dumps(angerInsert))\n",
    "\n",
    "#Inserimento Documento \n",
    "angerInsert = {\n",
    "    \"sentiment\":\"anger\",\n",
    "    \"hashtags\": hashtagAnger,\n",
    "    \"emoticons\": emoticonAnger,\n",
    "    \"emoji\": emojiAnger\n",
    "}\n",
    "\n",
    "#inserimento\n",
    "x = collectionTwitter.insert_one(angerInsert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# match --> { $match: { <query> } } se si soddisfa una query\n",
    "# project --> { $project: { <specification(s)> } } prende un documento che può specificare l'inclusione dei campi, \n",
    "# la soppressione del campo _id, l'aggiunta di nuovi campi e il ripristino dei valori dei campi esistenti\n",
    "# addfield --> { $addFields: { <newField>: <expression>, ... } } Specificare il nome di ogni campo da aggiungere e impostarne il valore su un'espressione di aggregazione\n",
    "# lookup --> {$lookup: {from , localField, foreignkey, as   } } Per eseguire una corrispondenza di uguaglianza tra un campo dei documenti di input\n",
    "# group --> {$groud: {_id:<expression> , <field> : {<accumulator1> : <expression1>}} } \n",
    "\n",
    "# Raggruppa i documenti di input in base all'espressione _id specificata e per ciascun raggruppamento distinto genera un documento\n",
    "pipeline = [\n",
    "    {\"$group\": {\"_id\": \"$word.lemma\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"word\": \"$_id\",\n",
    "      \"freq\": \"$count\"\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"freq\": -1\n",
    "    }},\n",
    "    {\"$out\": \"anger\"}\n",
    "]\n",
    "\n",
    "#post processing per renderlo valido \n",
    "angerCollections.aggregate(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creazione Word Clouds per le parole\n",
    "data = {}\n",
    "for document in angerCollections.find().limit(50):\n",
    "    data[document['word']] = document['freq']\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2)\n",
    "wc.fit_words(data)\n",
    "wc.to_file('Word Clouds/angerWords.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conteggio degli hashtags\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$hashtags\"},\n",
    "    {\"$group\": {\"_id\": \"$hashtags\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"word\": \"$_id\",\n",
    "      \"count\": 1\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"count\": -1\n",
    "    }}\n",
    "]\n",
    "\n",
    "hashtagsCount = collectionTwitter.aggregate(pipeline)\n",
    "\n",
    "data = {}\n",
    "for document in hashtagsCount:\n",
    "    data[document['word']] = document['count']\n",
    "\n",
    "collectionTwitter.update_one(\n",
    "  {\"sentiment\": \"anger\"},\n",
    "  {\"$set\": {\"hashtags\": data}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creazione Word Clouds per gli hashtags\n",
    "\n",
    "hashtags = collectionTwitter.distinct(\"hashtags\")\n",
    "\n",
    "hashtagsLimited = {}\n",
    "for (word,count) in [x for x in hashtags[0].items()][:50]:\n",
    "    hashtagsLimited[word] = count\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2)\n",
    "wc.fit_words(hashtagsLimited)\n",
    "wc.to_file('Word Clouds/angerHashtags.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conteggio delle emoji\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$emoji\"},\n",
    "    {\"$group\": {\"_id\": \"$emoji\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"emoji\": \"$_id\",\n",
    "      \"count\": 1\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"count\": -1\n",
    "    }}\n",
    "]\n",
    "\n",
    "emojiCount = collectionTwitter.aggregate(pipeline)\n",
    "\n",
    "data = {}\n",
    "for document in emojiCount:\n",
    "    data[document['emoji']] = document['count']\n",
    "\n",
    "collectionTwitter.update_one(\n",
    "  {\"sentiment\": \"anger\"},\n",
    "  {\"$set\": {\"emoji\": data}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creazione Word Clouds per le emoji\n",
    "\n",
    "emojis = collectionTwitter.distinct(\"emoji\")\n",
    "\n",
    "emojisLimited = {}\n",
    "for (word,count) in [x for x in emojis[0].items()][:50]:\n",
    "    emojisLimited[word] = count\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2, font_path='Font Emoji\\Symbola.otf')\n",
    "wc.fit_words(emojisLimited)\n",
    "wc.to_file('Word Clouds/angerEmojis.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conteggio degli emoticons\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$emoticons\"},\n",
    "    {\"$group\": {\"_id\": \"$emoticons\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"emoticons\": \"$_id\",\n",
    "      \"count\": 1\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"count\": -1\n",
    "    }}\n",
    "]\n",
    "\n",
    "emoticonsCount = collectionTwitter.aggregate(pipeline)\n",
    "\n",
    "data = {}\n",
    "for document in emoticonsCount:\n",
    "    data[document['emoticons']] = document['count']\n",
    "\n",
    "collectionTwitter.update_one(\n",
    "  {\"sentiment\": \"anger\"},\n",
    "  {\"$set\": {\"emoticons\": data}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creazione Word Clouds per le emoticons\n",
    "\n",
    "emoticons = collectionTwitter.distinct(\"emoticons\")\n",
    "\n",
    "emoticonsLimited = {}\n",
    "for (word,count) in [x for x in emoticons[0].items()][:50]:\n",
    "    emoticonsLimited[word] = count\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2)\n",
    "wc.fit_words(emoticonsLimited)\n",
    "wc.to_file('Word Clouds/angerEmoticons.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline di aggregazione per inserire la risorsa lessicale a cui appartiene la parola\n",
    "\n",
    "pipeline = [\n",
    "    {\"$lookup\": {\n",
    "        \"from\": \"Resources\",\n",
    "        \"localField\": \"word\",\n",
    "        \"foreignField\": \"wordList\",\n",
    "        \"as\": \"LexResources\"\n",
    "    }},\n",
    "    {\"$project\": {\n",
    "        \"resources.wordList\": 0,\n",
    "        \"resources.totNumberWords\": 0,\n",
    "        \"resources.sentiment\": 0,\n",
    "        \"LexResources.sentiment\": 0,\n",
    "        \"LexResources.totNumberWords\": 0,\n",
    "        \"LexResources.wordList\": 0\n",
    "    }},\n",
    "    {\"$out\": \"anger\"}\n",
    "]\n",
    "\n",
    "angerCollections.aggregate(pipeline)\n",
    "       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe Percentuali delle risorse Lessicali nei tweet anger\n",
      "            Risorsa  Percentuale\n",
      "0         NRC_anger     0.485164\n",
      "1       EmoSN_anger     0.567797\n",
      "2  sentisense_anger     0.545455\n",
      "Dataframe Percentuali dei tweet anger nelle risorse Lessicali\n",
      "            Risorsa  Percentuale\n",
      "0         NRC_anger     0.001179\n",
      "1       EmoSN_anger     0.000392\n",
      "2  sentisense_anger     0.000058\n"
     ]
    }
   ],
   "source": [
    "#Pipeline di aggregazione per calcolare le percentuali richieste\n",
    "\n",
    "pipelineCount = [\n",
    "    {\"$group\": {\"_id\": \"null\", \"count\": { \"$sum\": \"$freq\" }}},\n",
    "    {\"$project\": {\n",
    "        \"_id\": 0,\n",
    "        \"count\": \"$count\"\n",
    "    }}\n",
    "]\n",
    "\n",
    "results = angerCollections.aggregate(pipelineCount)\n",
    "\n",
    "countWords = 0\n",
    "for data in results:\n",
    "    countWords = data['count']\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$LexResources\"},\n",
    "    {\"$lookup\": {\n",
    "        \"from\": \"Resources\",\n",
    "        \"localField\": \"LexResources._id\",\n",
    "        \"foreignField\": \"_id\",\n",
    "        \"as\": \"RefResource\"\n",
    "    }},\n",
    "    {\"$group\": {\"_id\": { \"lex\": \"$LexResources._id\", \"RefResource\": \"$RefResource\" }, \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "        \"_id\": \"$_id.lex\",\n",
    "        \"totWordsLex\": \"$_id.RefResource.totNumberWords\",\n",
    "        \"commonWords\": \"$count\",\n",
    "        \"sum_val\": \"$sum_val\"\n",
    "    }},\n",
    "    {\"$unwind\": \"$totWordsLex\"},\n",
    "    {\"$addFields\": {\"totalWordsTwitter\": countWords}},\n",
    "    {\"$project\": {\n",
    "        \"perc_presence_lex_res\": { \"$round\": [{\"$divide\": [ \"$commonWords\", \"$totWordsLex\" ] }, 6] },\n",
    "        \"perc_presence_twitter\": { \"$round\": [{\"$divide\": [ \"$commonWords\", \"$totalWordsTwitter\" ]}, 6] }\n",
    "    }}\n",
    "]\n",
    "\n",
    "results = angerCollections.aggregate(pipeline)\n",
    "\n",
    "#Genero l'isotagramma\n",
    "istogramAnger_lexRes = []\n",
    "istogramAnger_twitter = []\n",
    "\n",
    "for data in results:\n",
    "    #print(data)\n",
    "    log =math.log(data['perc_presence_lex_res'],10)\n",
    "    istogramAnger_lexRes.append([data['_id'], data['perc_presence_lex_res']])\n",
    "    log =math.log(data['perc_presence_twitter'],10)\n",
    "    istogramAnger_twitter.append([data['_id'], data['perc_presence_twitter']])\n",
    " \n",
    "# Create the pandas DataFrame\n",
    "dfAnger_lexRes = pd.DataFrame(istogramAnger_lexRes, columns = ['Risorsa', 'Percentuale'])\n",
    "dfAnger_twitter = pd.DataFrame(istogramAnger_twitter,columns = ['Risorsa', 'Percentuale'])\n",
    "\n",
    "# print dataframe.\n",
    "print(\"Dataframe Percentuali delle risorse Lessicali nei tweet anger\")  \n",
    "print(dfAnger_lexRes)  \n",
    "print(\"Dataframe Percentuali dei tweet anger nelle risorse Lessicali\")    \n",
    "print(dfAnger_twitter)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'\\nn, bins, patches=plt.hist(istogramAnger_lexRes)\\nplt.xlabel(\"Values\")\\nplt.ylabel(\"Probability\")\\nplt.title(\"Histogram perc_presence_lex_res\")\\nplt.show()\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfAnger_lexRes.plot.bar(x='Risorsa')\n",
    "plt.show()\n",
    "\"\"\"\n",
    "n, bins, patches=plt.hist(istogramAnger_lexRes)\n",
    "plt.xlabel(\"Values\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.title(\"Histogram perc_presence_lex_res\")\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'\\nn, bins, patches=plt.hist(istogramAnger_twitter)\\nplt.xlabel(\"Values\")\\nplt.ylabel(\"Probability\")\\nplt.title(\"Histogram perc_presence_twitter\")\\nplt.show()\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfAnger_twitter.plot.bar(x='Risorsa')\n",
    "plt.show()\n",
    "\"\"\"\n",
    "n, bins, patches=plt.hist(istogramAnger_twitter)\n",
    "plt.xlabel(\"Values\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.title(\"Histogram perc_presence_twitter\")\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(database = \"maadb\", user = \"postgres\", password = \"admin\", host = \"localhost\", port = \"5432\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'conn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\gabri\\Desktop\\Uni\\progettoMaadb\\Code\\main.ipynb Cell 31'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/gabri/Desktop/Uni/progettoMaadb/Code/main.ipynb#ch0000029?line=0'>1</a>\u001b[0m \u001b[39m# Connessione, creazione tabella risorse lessicali anger e inserimento\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/gabri/Desktop/Uni/progettoMaadb/Code/main.ipynb#ch0000029?line=2'>3</a>\u001b[0m cur \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39mcursor()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/gabri/Desktop/Uni/progettoMaadb/Code/main.ipynb#ch0000029?line=3'>4</a>\u001b[0m cur\u001b[39m.\u001b[39mexecute(\u001b[39m'''\u001b[39m\u001b[39mCREATE TABLE RISORSEANGER\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/gabri/Desktop/Uni/progettoMaadb/Code/main.ipynb#ch0000029?line=4'>5</a>\u001b[0m \u001b[39m                (   ID SERIAL NOT NULL PRIMARY KEY,\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/gabri/Desktop/Uni/progettoMaadb/Code/main.ipynb#ch0000029?line=5'>6</a>\u001b[0m \u001b[39m                    NOME             TEXT       NOT NULL,\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/gabri/Desktop/Uni/progettoMaadb/Code/main.ipynb#ch0000029?line=8'>9</a>\u001b[0m \u001b[39m            \u001b[39m\u001b[39m'''\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/gabri/Desktop/Uni/progettoMaadb/Code/main.ipynb#ch0000029?line=9'>10</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/gabri/Desktop/Uni/progettoMaadb/Code/main.ipynb#ch0000029?line=10'>11</a>\u001b[0m nome \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mEmoSN_anger\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'conn' is not defined"
     ]
    }
   ],
   "source": [
    "# Connessione, creazione tabella risorse lessicali anger e inserimento\n",
    "\n",
    "cur = conn.cursor()\n",
    "cur.execute('''CREATE TABLE RISORSEANGER\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    NOME             TEXT       NOT NULL,\n",
    "                    PAROLA           TEXT       NOT NULL\n",
    "                );\n",
    "            '''\n",
    ")\n",
    "nome = \"EmoSN_anger\"\n",
    "for word in resourceAngerEmoSN:\n",
    "    cur.execute(\"INSERT INTO RISORSEANGER (NOME, PAROLA) VALUES(%s, %s)\",(nome,word))\n",
    "\n",
    "nome = \"NRC_anger\"\n",
    "for word in resourceAngerNRC:\n",
    "    cur.execute(\"INSERT INTO RISORSEANGER (NOME, PAROLA) VALUES(%s, %s)\",(nome,word))\n",
    "\n",
    "nome = \"sentisense_anger\"\n",
    "for word in resourceAngerSentisense:\n",
    "    cur.execute(\"INSERT INTO RISORSEANGER (NOME, PAROLA) VALUES(%s, %s)\",(nome,word))\n",
    "\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur = conn.cursor()\n",
    "cur.execute('''CREATE TABLE PAROLEANGER\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    PAROLA           TEXT       NOT NULL, \n",
    "                    EmoSN_anger      BOOL       NOT NULL DEFAULT FALSE,\n",
    "                    NRC_anger      BOOL       NOT NULL DEFAULT FALSE,\n",
    "                    sentisense_anger      BOOL       NOT NULL DEFAULT FALSE,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ")   \n",
    "\n",
    "cur = conn.cursor()\n",
    "cur.execute('''CREATE TABLE EMOJIANGER\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    EMOJI       TEXT       NOT NULL,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ") \n",
    "\n",
    "cur = conn.cursor()\n",
    "cur.execute('''CREATE TABLE EMOTICONSANGER\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    EMOTICONS       TEXT       NOT NULL,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ") \n",
    "\n",
    "cur = conn.cursor()\n",
    "cur.execute('''CREATE TABLE HASHTAGANGER\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    HASHTAG     TEXT       NOT NULL,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ") \n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur = conn.cursor()\n",
    "\n",
    "for hashTag in hashtagAnger:\n",
    "    cur.execute(\"INSERT INTO HASHTAGANGER (HASHTAG,FREQUENCY) VALUES(%s,%s)\",(hashTag,0))\n",
    "\n",
    "for emoticons in emoticonAnger:\n",
    "    cur.execute(\"INSERT INTO EMOTICONSANGER (EMOTICONS,FREQUENCY) VALUES(%s,%s)\",(emoticons,0))\n",
    "\n",
    "for emojis in emojiAnger:\n",
    "    cur.execute(\"INSERT INTO EMOJIANGER (EMOJI,FREQUENCY) VALUES(%s,%s)\",(emojis,0))\n",
    "\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inserimento hashtag emoji emoticons\n",
    "cur = conn.cursor()\n",
    "\n",
    "for hashTag in hashtagAnger:\n",
    "    cur.execute(\"INSERT INTO HASHTAGANGER (HASHTAG,FREQUENCY) VALUES(%s,%s)\",(hashTag,0))\n",
    "\n",
    "for emoticons in emoticonAnger:\n",
    "    cur.execute(\"INSERT INTO EMOTICONSANGER (EMOTICONS,FREQUENCY) VALUES(%s,%s)\",(emoticons,0))\n",
    "\n",
    "for emojis in emojiAnger:\n",
    "    cur.execute(\"INSERT INTO EMOJIANGER (EMOJI,FREQUENCY) VALUES(%s,%s)\",(emojis,0))\n",
    "\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inserimento parole in db\n",
    "cur = conn.cursor()\n",
    "\n",
    "for word in angerListWord:\n",
    "    cur.execute(\"INSERT INTO PAROLEANGER (PAROLA,EmoSN_anger,NRC_anger,sentisense_anger,FREQUENCY) VALUES(%s,%s,%s,%s,%s)\",(word,False,False,False,0))\n",
    "\n",
    "conn.commit()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trattamento dataset Anticipation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lista di righe, ogni riga una tweet anticipation\n",
    "file = open(\"Dataset/dataset_dt_anticipation_60k.txt\", 'r',encoding=\"utf8\")\n",
    "anticipation = []\n",
    "hashtagAnticipation = []\n",
    "emoticonAnticipation = []\n",
    "emojiAnticipation = []\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "    riga = re.sub(r'URL', '',riga)\n",
    "    riga = re.sub(r'USERNAME', '',riga)\n",
    "\n",
    "    #array di hashtag singoli\n",
    "    hashTag = (re.findall('#[a-zA-Z]+', riga))\n",
    "\n",
    "    #generiamo l'array di hashtag senza hashtag\n",
    "    for h in hashTag:\n",
    "        hashtagAnticipation.append(h.replace(\"#\",\"\"))\n",
    "\n",
    "    #Rimuoviamo glin hashtag dalla riga\n",
    "    riga = riga.replace('#',\"\")\n",
    "    \n",
    "    anticipation.append(riga)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(anticipation)):\n",
    "    #Controllo se l'emoticons c'è e in caso la rimuovo\n",
    "    for singleEmoticon in emoticons:\n",
    "        emoticonsFind = re.findall(re.escape(singleEmoticon),anticipation[i])\n",
    "        if(len(emoticonsFind) != 0):\n",
    "            emoticonAnticipation.extend(emoticonsFind)\n",
    "            anticipation[i] = anticipation[i].replace(singleEmoticon,\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(anticipation)):\n",
    "    #preleviamo le emoji e le rimuoviamo dal testo\n",
    "    data = regex.findall(r'\\X', anticipation[i])\n",
    "    for word in data:\n",
    "        if any(char in emoji.UNICODE_EMOJI['en'] for char in word):\n",
    "            emojiAnticipation.append(word)\n",
    "            anticipation[i] = anticipation[i].replace(word, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processamento slang prima di punteggiatura e lowercase\n",
    "with open(\"Slangs/slangs_before.json\") as f:\n",
    "    slangsBefore = json.load(f)\n",
    "    for i in range(len(anticipation)):\n",
    "        for word in anticipation[i].split():\n",
    "            indexOfWord = anticipation[i].find(word)\n",
    "            if slangsBefore.get(word) is None:\n",
    "                if slangsBefore.get(word.lower()) is None:\n",
    "                    if slangsBefore.get(word.upper()) is None:\n",
    "                        continue\n",
    "                    else:\n",
    "                        anticipation[i] = anticipation[i][:indexOfWord] + slangsBefore.get(word.upper()) + anticipation[i][indexOfWord+len(word):]\n",
    "                else:\n",
    "                    anticipation[i] = anticipation[i][:indexOfWord] + slangsBefore.get(word.lower()) + anticipation[i][indexOfWord+len(word):]\n",
    "            else:\n",
    "                anticipation[i] = anticipation[i][:indexOfWord] + slangsBefore.get(word) + anticipation[i][indexOfWord+len(word):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(anticipation)):\n",
    "    #Trattamento punteggiatura\n",
    "    line = re.compile(\"[\" + re.escape(\"\".join(puntuaction)) + \"]\")\n",
    "    anticipation[i] = re.sub(line, \"\", anticipation[i])\n",
    "\n",
    "    #Rendiamo la farse in minuscolo\n",
    "    anticipation[i] = anticipation[i].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizzazione di tutte le frasi\n",
    "anticipationTokenized = []\n",
    "for linea in anticipation:\n",
    "    result = word_tokenize(linea)\n",
    "    anticipationTokenized.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Esecuzione della POS tagging per ogni riga della matrice\n",
    "for i in range(len(anticipationTokenized)):\n",
    "    anticipationTokenized[i] = nltk.pos_tag(anticipationTokenized[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming\n",
    "#ps = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for line in anticipationTokenized:\n",
    "    for i in range(len(line)):\n",
    "        tupla = [lemmatizer.lemmatize(line[i][0]),line[i][1]]\n",
    "        line[i] = tupla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trattamnento Stop word dopo Stemming\n",
    "stop_words = set(stopwords.words('english'))\n",
    "anticipationTokStoped = []\n",
    "for line in anticipationTokStoped:\n",
    "    filtred_word = []\n",
    "    for row in line:\n",
    "        if  row[0] not in stop_words:\n",
    "            filtred_word.append(row) \n",
    "    anticipationTokStoped.append(filtred_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lavorazione risorse lessicali\n",
    "resourceAnticipation = []\n",
    "#Lista di righe, ogni riga una tweet anger\n",
    "file = open(\"Risorse lessicali\\\\Anticipation\\\\NRC_anticipation.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        if(riga not in resourceAnticipation):\n",
    "            resourceAnticipation.append(line)\n",
    "\n",
    "file.close()\n",
    "\n",
    "file = open(\"Risorse lessicali\\Anticipation\\sentisense_anticipation.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        if(riga not in resourceAnticipation):\n",
    "            resourceAnticipation.append(line)\n",
    "\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anticipationListWord = []\n",
    "anticipationListTok = []\n",
    "for line in anticipationTokStoped:\n",
    "    for row in line:\n",
    "        anticipationListWord.append(row[0])\n",
    "        anticipationListTok.append(row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conteggio delle frequenze delle parole\n",
    "countWordAnticipation = Counter(anticipationListWord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione database e collezioni\n",
    "# Inserimento singoli documenti contententi i tweet anger\n",
    "anticipationMongo = []\n",
    "wordsString = {}\n",
    "\n",
    "for i in range(len(anticipationMongo)):\n",
    "    wordsString = {\n",
    "        \"lemma\": anticipationListWord[i],\n",
    "        \"POS\": anticipationListTok[i]\n",
    "    }\n",
    "    anticipationTwit={\"word\":wordsString}\n",
    "    anticipationCollections.insert_one(anticipationTwit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inserimento Documento\n",
    "anticipationInsert = {\n",
    "    \"sentiment\":\"anticipation\",\n",
    "    \"hashtags\": hashtagAnticipation,\n",
    "    \"emoticons\": emoticonAnticipation,\n",
    "    \"emoji\": emojiAnticipation\n",
    "}\n",
    "\n",
    "#inserimento\n",
    "x = collectionTwitter.insert_one(anticipationInsert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# match --> { $match: { <query> } } se si soddisfa una query\n",
    "# project --> { $project: { <specification(s)> } } prende un documento che può specificare l'inclusione dei campi, \n",
    "# la soppressione del campo _id, l'aggiunta di nuovi campi e il ripristino dei valori dei campi esistenti\n",
    "# addfield --> { $addFields: { <newField>: <expression>, ... } } Specificare il nome di ogni campo da aggiungere e impostarne il valore su un'espressione di aggregazione\n",
    "# lookup --> {$lookup: {from , localField, foreignkey, as   } } Per eseguire una corrispondenza di uguaglianza tra un campo dei documenti di input\n",
    "# group --> {$groud: {_id:<expression> , <field> : {<accumulator1> : <expression1>}} } \n",
    "#   Raggruppa i documenti di input in base all'espressione _id specificata e per ciascun raggruppamento distinto genera un documento\n",
    "\n",
    "pipeline = [\n",
    "    {\"$group\": {\"_id\": \"$word.lemma\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"word\": \"$_id\",\n",
    "      \"freq\": \"$count\"\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"freq\": -1\n",
    "    }},\n",
    "    {\"$out\": \"anticipation\"}\n",
    "]\n",
    "\n",
    "\n",
    "wordCount = anticipationCollections.aggregate(pipeline)\n",
    "#Creazione Word Clouds per le parole\n",
    "data = {}\n",
    "for document in anticipationCollections.find().limit(50):\n",
    "    data[document['word']] = document['freq']\n",
    "\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2)\n",
    "wc.fit_words(data)\n",
    "wc.to_file('Word Clouds/anticipationWords.png')\n",
    "# Conteggio degli hashtags\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$hashtags\"},\n",
    "    {\"$group\": {\"_id\": \"$hashtags\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"word\": \"$_id\",\n",
    "      \"count\": 1\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"count\": -1\n",
    "    }}\n",
    "]\n",
    "\n",
    "hashtagsCount = collectionTwitter.aggregate(pipeline)\n",
    "\n",
    "data = {}\n",
    "for document in hashtagsCount:\n",
    "    data[document['word']] = document['count']\n",
    "\n",
    "collectionTwitter.update_one(\n",
    "  {\"sentiment\": \"anticipation\"},\n",
    "  {\"$set\": {\"hashtags\": data}}\n",
    ")\n",
    "#Creazione Word Clouds per gli hashtags\n",
    "\n",
    "hashtags = collectionTwitter.distinct(\"hashtags\")\n",
    "\n",
    "hashtagsLimited = {}\n",
    "for (word,count) in [x for x in hashtags[0].items()][:50]:\n",
    "    hashtagsLimited[word] = count\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2)\n",
    "wc.fit_words(hashtagsLimited)\n",
    "wc.to_file('Word Clouds/anticipationHashtags.png')\n",
    "# Conteggio delle emoji\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$emoji\"},\n",
    "    {\"$group\": {\"_id\": \"$emoji\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"emoji\": \"$_id\",\n",
    "      \"count\": 1\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"count\": -1\n",
    "    }}\n",
    "]\n",
    "\n",
    "emojiCount = collectionTwitter.aggregate(pipeline)\n",
    "\n",
    "data = {}\n",
    "for document in emojiCount:\n",
    "    data[document['emoji']] = document['count']\n",
    "\n",
    "collectionTwitter.update_one(\n",
    "  {\"sentiment\": \"anticipation\"},\n",
    "  {\"$set\": {\"emoji\": data}}\n",
    ")\n",
    "#Creazione Word Clouds per le emoji\n",
    "\n",
    "emojis = collectionTwitter.distinct(\"emoji\")\n",
    "\n",
    "emojisLimited = {}\n",
    "for (word,count) in [x for x in emojis[0].items()][:50]:\n",
    "    emojisLimited[word] = count\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2)\n",
    "wc.fit_words(emojisLimited)\n",
    "wc.to_file('Word Clouds/anticipationEmojis.png')\n",
    "# Conteggio degli emoticons\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$emoticons\"},\n",
    "    {\"$group\": {\"_id\": \"$emoticons\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"emoticons\": \"$_id\",\n",
    "      \"count\": 1\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"count\": -1\n",
    "    }}\n",
    "]\n",
    "\n",
    "emoticonsCount = collectionTwitter.aggregate(pipeline)\n",
    "\n",
    "data = {}\n",
    "for document in emoticonsCount:\n",
    "    data[document['emoticons']] = document['count']\n",
    "\n",
    "collectionTwitter.update_one(\n",
    "  {\"sentiment\": \"anticipation\"},\n",
    "  {\"$set\": {\"emoticons\": data}}\n",
    ")\n",
    "#Creazione Word Clouds per le emoticons\n",
    "\n",
    "emoticons = collectionTwitter.distinct(\"emoticons\")\n",
    "\n",
    "emoticonsLimited = {}\n",
    "for (word,count) in [x for x in emoticons[0].items()][:50]:\n",
    "    emoticonsLimited[word] = count\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2)\n",
    "wc.fit_words(emoticonsLimited)\n",
    "wc.to_file('Word Clouds/anticipationEmoticons.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trattamento dataset Disgust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lista di righe, ogni riga una tweet disgust\n",
    "file = open(\"Dataset/dataset_dt_disgust_60k.txt\", 'r',encoding=\"utf8\")\n",
    "disgust = []\n",
    "hashtagDisgust = []\n",
    "emoticonDisgust = []\n",
    "emojiDisgust = []\n",
    "\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "    riga = re.sub(r'URL', '',riga)\n",
    "    riga = re.sub(r'USERNAME', '',riga)\n",
    "\n",
    "    #array di hashtag singoli\n",
    "    hashTag = (re.findall('#[a-zA-Z]+', riga))\n",
    "    \n",
    "    #generiamo l'array di hashtag senza hashtag\n",
    "    for h in hashTag:\n",
    "        hashtagDisgust.append(h.replace(\"#\",\"\"))\n",
    "\n",
    "    #Rimuoviamo glin hashtag dalla riga\n",
    "    riga = riga.replace('#',\"\")\n",
    "\n",
    "    disgust.append(riga)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(disgust)):\n",
    "    #Controllo se l'emoticons c'è e in caso la rimuovo\n",
    "    for singleEmoticon in emoticons:\n",
    "        emoticonsFind = re.findall(re.escape(singleEmoticon),disgust[i])\n",
    "        if(len(emoticonsFind) != 0):\n",
    "            emoticonDisgust.extend(emoticonsFind)\n",
    "            disgust[i] = disgust[i].replace(singleEmoticon,\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(disgust)):\n",
    "    #preleviamo le emoji e le rimuoviamo dal testo\n",
    "    data = regex.findall(r'\\X', disgust[i])\n",
    "    for word in data:\n",
    "        if any(char in emoji.UNICODE_EMOJI['en'] for char in word):\n",
    "            emojiDisgust = [].append(word)\n",
    "            disgust[i] = disgust[i].replace(word, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processamento slang prima di punteggiatura e lowercase\n",
    "with open(\"Slangs/slangs_before.json\") as f:\n",
    "    slangsBefore = json.load(f)\n",
    "    for i in range(len(disgust)):\n",
    "        for word in disgust[i].split():\n",
    "            indexOfWord = disgust[i].find(word)\n",
    "            if slangsBefore.get(word) is None:\n",
    "                if slangsBefore.get(word.lower()) is None:\n",
    "                    if slangsBefore.get(word.upper()) is None:\n",
    "                        continue\n",
    "                    else:\n",
    "                        disgust[i] = disgust[i][:indexOfWord] + slangsBefore.get(word.upper()) + disgust[i][indexOfWord+len(word):]\n",
    "                else:\n",
    "                    disgust[i] = disgust[i][:indexOfWord] + slangsBefore.get(word.lower()) + disgust[i][indexOfWord+len(word):]\n",
    "            else:\n",
    "                disgust[i] = disgust[i][:indexOfWord] + slangsBefore.get(word) + disgust[i][indexOfWord+len(word):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(disgust)):\n",
    "    #Trattamento punteggiatura\n",
    "    line = re.compile(\"[\" + re.escape(\"\".join(puntuaction)) + \"]\")\n",
    "    disgust[i] = re.sub(line, \"\", disgust[i])\n",
    "\n",
    "    #Rendiamo la farse in minuscolo\n",
    "    disgust[i] = disgust[i].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizzazione di tutte le frasi\n",
    "disgustTokenized = []\n",
    "for linea in disgust:\n",
    "    result = word_tokenize(linea)\n",
    "    disgustTokenized.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Esecuzione della POS tagging per ogni riga della matrice\n",
    "for i in range(len(disgustTokenized)):\n",
    "    disgustTokenized[i] = nltk.pos_tag(disgustTokenized[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming\n",
    "#ps = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for line in disgustTokenized:\n",
    "    for i in range(len(line)):\n",
    "        tupla = [lemmatizer.lemmatize(line[i][0]),line[i][1]]\n",
    "        line[i] = tupla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trattamnento Stop word dopo Stemming\n",
    "stop_words = set(stopwords.words('english'))\n",
    "disgustTokStoped = []\n",
    "for line in disgustTokStoped:\n",
    "    filtred_word = []\n",
    "    for row in line:\n",
    "        if  row[0] not in stop_words:\n",
    "            filtred_word.append(row) \n",
    "    disgustTokStoped.append(filtred_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lavorazione risorse lessicali\n",
    "resourceDisgust = []\n",
    "#Lista di righe, ogni riga una tweet anger\n",
    "file = open(\"Risorse lessicali\\Disgust-Hate\\sentisense_disgust.txt.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        if(riga not in resourceDisgust):\n",
    "            resourceDisgust.append(riga)\n",
    "\n",
    "file.close()\n",
    "\n",
    "file = open(\"C:Risorse lessicali\\\\Disgust-Hate\\\\NRC_disgust.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        if(riga not in resourceDisgust):\n",
    "            resourceDisgust.append(line)\n",
    "\n",
    "file.close()\n",
    "\n",
    "file = open(\"Risorse lessicali\\Disgust-Hate\\sentisense_hate.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        if(riga not in resourceDisgust):\n",
    "            resourceDisgust.append(line)\n",
    "\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disgustListWord = []\n",
    "disgustListTok = []\n",
    "for line in disgustTokStoped:\n",
    "    for row in line:\n",
    "        disgustListWord.append(row[0])\n",
    "        disgustListTok.append(row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Slangs vengono modificati con la loro definizione\n",
    "# with open(\"Slangs/slangs.json\", \"r\", encoding='utf-8') as f:\n",
    "#     slangs = json.load(f)\n",
    "#     for row in range(len(disgustTokenized)):\n",
    "#         for column in range(len(disgustTokenized[row])):\n",
    "#             if disgustTokenized[row][column] in slangs:\n",
    "#                 disgustTokenized[row][column] = slangs.get(disgustTokenized[row][column])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trattamento dataset Fear "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lista di righe, ogni riga una tweet fear\n",
    "file = open(\"Dataset/dataset_dt_fear_60k.txt\", 'r',encoding=\"utf8\")\n",
    "fear = []\n",
    "hashtagFear = []\n",
    "emoticonFear = []\n",
    "emojiFear = []\n",
    "\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "    riga = re.sub(r'URL', '',riga)\n",
    "    riga = re.sub(r'USERNAME', '',riga)\n",
    "\n",
    "     #array di hashtag singoli\n",
    "    hashTag = (re.findall('#[a-zA-Z]+', riga))\n",
    "\n",
    "    #generiamo l'array di hashtag senza hashtag\n",
    "    for h in hashTag:\n",
    "        hashtagFear.append(h.replace(\"#\",\"\"))\n",
    "\n",
    "    #Rimuoviamo glin hashtag dalla riga\n",
    "    riga = riga.replace('#',\"\")\n",
    "\n",
    "    fear.append(riga)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(fear)):\n",
    "    #Controllo se l'emoticons c'è e in caso la rimuovo\n",
    "    for singleEmoticon in emoticons:\n",
    "        emoticonsFind = re.findall(re.escape(singleEmoticon),fear[i])\n",
    "        if(len(emoticonsFind) != 0):\n",
    "            emoticonFear.extend(emoticonsFind)\n",
    "            fear[i] = fear[i].replace(singleEmoticon,\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(fear)):\n",
    "    #preleviamo le emoji e le rimuoviamo dal testo\n",
    "    data = regex.findall(r'\\X', fear[i])\n",
    "    for word in data:\n",
    "        if any(char in emoji.UNICODE_EMOJI['en'] for char in word):\n",
    "            emojiFear = [].append(word)\n",
    "            fear[i] = fear[i].replace(word, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processamento slang prima di punteggiatura e lowercase\n",
    "with open(\"Slangs/slangs_before.json\") as f:\n",
    "    slangsBefore = json.load(f)\n",
    "    for i in range(len(fear)):\n",
    "        for word in fear[i].split():\n",
    "            indexOfWord = fear[i].find(word)\n",
    "            if slangsBefore.get(word) is None:\n",
    "                if slangsBefore.get(word.lower()) is None:\n",
    "                    if slangsBefore.get(word.upper()) is None:\n",
    "                        continue\n",
    "                    else:\n",
    "                        fear[i] = fear[i][:indexOfWord] + slangsBefore.get(word.upper()) + fear[i][indexOfWord+len(word):]\n",
    "                else:\n",
    "                    fear[i] = fear[i][:indexOfWord] + slangsBefore.get(word.lower()) + fear[i][indexOfWord+len(word):]\n",
    "            else:\n",
    "                fear[i] = fear[i][:indexOfWord] + slangsBefore.get(word) + fear[i][indexOfWord+len(word):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(fear)):\n",
    "    #Trattamento punteggiatura\n",
    "    line = re.compile(\"[\" + re.escape(\"\".join(puntuaction)) + \"]\")\n",
    "    fear[i] = re.sub(line, \"\", fear[i])\n",
    "\n",
    "    #Rendiamo la farse in minuscolo\n",
    "    fear[i] = fear[i].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizzazione di tutte le frasi\n",
    "fearTokenized = []\n",
    "for linea in fear:\n",
    "    result = word_tokenize(linea)\n",
    "    fearTokenized.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Esecuzione della POS tagging per ogni riga della matrice\n",
    "for i in range(len(fearTokenized)):\n",
    "    fearTokenized[i] = nltk.pos_tag(fearTokenized[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming\n",
    "#ps = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for line in fearTokenized:\n",
    "    for i in range(len(line)):\n",
    "        tupla = [lemmatizer.lemmatize(line[i][0]),line[i][1]]\n",
    "        line[i] = tupla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trattamnento Stop word dopo Stemming\n",
    "stop_words = set(stopwords.words('english'))\n",
    "fearTokStoped = []\n",
    "for line in fearTokStoped:\n",
    "    filtred_word = []\n",
    "    for row in line:\n",
    "        if  row[0] not in stop_words:\n",
    "            filtred_word.append(row) \n",
    "    fearTokStoped.append(filtred_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lavorazione risorse lessicali\n",
    "resourceFear = []\n",
    "#Lista di righe, ogni riga una tweet anger\n",
    "file = open(\"Risorse lessicali\\\\Fear\\\\NRC_fear.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        if(riga not in resourceFear):\n",
    "            resourceFear.append(line)\n",
    "\n",
    "file.close()\n",
    "\n",
    "file = open(\"Risorse lessicali\\Fear\\sentisense_fear.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        if(riga not in resourceFear):\n",
    "            resourceFear.append(line)\n",
    "\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fearListWord = []\n",
    "fearListToken = []\n",
    "for line in fearTokStoped:\n",
    "    for row in line:\n",
    "        fearListWord.append(row[0])\n",
    "        fearListToken.append(row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Slangs vengono modificati con la loro definizione\n",
    "# with open(\"Slangs/slangs.json\", \"r\", encoding='utf-8') as f:\n",
    "#     slangs = json.load(f)\n",
    "#     for row in range(len(fearTokenized)):\n",
    "#         for column in range(len(fearTokenized[row])):\n",
    "#             if fearTokenized[row][column] in slangs:\n",
    "#                 fearTokenized[row][column] = slangs.get(fearTokenized[row][column])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trattamento dataset Joy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lista di righe, ogni riga una tweet joy\n",
    "file = open(\"Dataset/dataset_dt_joy_60k.txt\", 'r',encoding=\"utf8\")\n",
    "joy = []\n",
    "hashtagJoy = []\n",
    "emoticonJoy = []\n",
    "emojiJoy = []\n",
    "\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "    riga = re.sub(r'URL', '',riga)\n",
    "    riga = re.sub(r'USERNAME', '',riga)\n",
    "    \n",
    "    #array di hashtag singoli\n",
    "    hashTag = (re.findall('#[a-zA-Z]+', riga))\n",
    "\n",
    "    #generiamo l'array di hashtag senza hashtag\n",
    "    for h in hashTag:\n",
    "        hashtagJoy.append(h.replace(\"#\",\"\"))\n",
    "\n",
    "    #Rimuoviamo glin hashtag dalla riga\n",
    "    riga = riga.replace('#',\"\")\n",
    "\n",
    "    joy.append(riga)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(joy)):\n",
    "    #Controllo se l'emoticons c'è e in caso la rimuovo\n",
    "    for singleEmoticon in emoticons:\n",
    "        emoticonJoy = re.findall(re.escape(singleEmoticon),joy[i])\n",
    "        if(len(emoticonsFind) != 0):\n",
    "            emoticonFear.extend(emoticonsFind)\n",
    "            joy[i] = joy[i].replace(singleEmoticon,\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(joy)):\n",
    "    #preleviamo le emoji e le rimuoviamo dal testo\n",
    "    data = regex.findall(r'\\X', joy[i])\n",
    "    for word in data:\n",
    "        if any(char in emoji.UNICODE_EMOJI['en'] for char in word):\n",
    "            emojiJoy = [].append(word)\n",
    "            joy[i] = joy[i].replace(word, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processamento slang prima di punteggiatura e lowercase\n",
    "with open(\"Slangs/slangs_before.json\") as f:\n",
    "    slangsBefore = json.load(f)\n",
    "    for i in range(len(joy)):\n",
    "        for word in joy[i].split():\n",
    "            indexOfWord = joy[i].find(word)\n",
    "            if slangsBefore.get(word) is None:\n",
    "                if slangsBefore.get(word.lower()) is None:\n",
    "                    if slangsBefore.get(word.upper()) is None:\n",
    "                        continue\n",
    "                    else:\n",
    "                        joy[i] = joy[i][:indexOfWord] + slangsBefore.get(word.upper()) + joy[i][indexOfWord+len(word):]\n",
    "                else:\n",
    "                    joy[i] = joy[i][:indexOfWord] + slangsBefore.get(word.lower()) + joy[i][indexOfWord+len(word):]\n",
    "            else:\n",
    "                joy[i] = joy[i][:indexOfWord] + slangsBefore.get(word) + joy[i][indexOfWord+len(word):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(joy)):\n",
    "    #Trattamento punteggiatura\n",
    "    line = re.compile(\"[\" + re.escape(\"\".join(puntuaction)) + \"]\")\n",
    "    joy[i] = re.sub(line, \"\", joy[i])\n",
    "\n",
    "    #Rendiamo la farse in minuscolo\n",
    "    joy[i] = joy[i].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizzazione di tutte le frasi\n",
    "joyTokenized = []\n",
    "for linea in joy:\n",
    "    result = word_tokenize(linea)\n",
    "    joyTokenized.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Esecuzione della POS tagging per ogni riga della matrice\n",
    "for i in range(len(joyTokenized)):\n",
    "    joyTokenized[i] = nltk.pos_tag(joyTokenized[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Slangs vengono modificati con la loro definizione\n",
    "# with open(\"Slangs/slangs.json\", \"r\", encoding='utf-8') as f:\n",
    "#     slangs = json.load(f)\n",
    "#     for row in range(len(joyTokenized)):\n",
    "#         for column in range(len(joyTokenized[row])):\n",
    "#             if joyTokenized[row][column] in slangs:\n",
    "#                 joyTokenized[row][column] = slangs.get(joyTokenized[row][column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming\n",
    "#ps = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for line in joyTokenized:\n",
    "    for i in range(len(line)):\n",
    "        tupla = [lemmatizer.lemmatize(line[i][0]),line[i][1]]\n",
    "        line[i] = tupla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trattamnento Stop word dopo Stemming\n",
    "stop_words = set(stopwords.words('english'))\n",
    "joyTokStoped = []\n",
    "for line in joyTokenized:\n",
    "    filtred_word = []\n",
    "    for row in line:\n",
    "        if  row[0] not in stop_words:\n",
    "            filtred_word.append(row) \n",
    "    joyTokStoped.append(filtred_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joyListWord = []\n",
    "joyListTok = []\n",
    "for line in joyTokStoped:\n",
    "    for row in line:\n",
    "        joyListWord.append(row[0])\n",
    "        joyListTok.append(row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lavorazione risorse lessicali\n",
    "resourceJoy = []\n",
    "#Lista di righe, ogni riga una tweet anger\n",
    "file = open(\"Risorse lessicali\\Joy\\EmoSN_joy.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        if(riga not in resourceJoy):\n",
    "            resourceJoy.append(riga)\n",
    "\n",
    "file.close()\n",
    "\n",
    "file = open(\"Risorse lessicali\\\\Joy\\\\NRC_joy.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        if(riga not in resourceJoy):\n",
    "            resourceJoy.append(line)\n",
    "\n",
    "file.close()\n",
    "\n",
    "file = open(\"Risorse lessicali\\Joy\\sentisense_joy.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        if(riga not in resourceJoy):\n",
    "            resourceJoy.append(line)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trattamento dataset Sadness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lista di righe, ogni riga una tweet sadness\n",
    "file = open(\"Dataset/dataset_dt_sadness_60k.txt\", 'r',encoding=\"utf8\")\n",
    "sadness = []\n",
    "hashtagSadness = []\n",
    "emoticonSadness = []\n",
    "emojiSadness = []\n",
    "\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "    riga = re.sub(r'URL', '',riga)\n",
    "    riga = re.sub(r'USERNAME', '',riga)\n",
    "\n",
    "    #array di hashtag singoli\n",
    "    hashTag = (re.findall('#[a-zA-Z]+', riga))\n",
    "\n",
    "    #Rimuoviamo glin hashtag dalla riga\n",
    "    riga = riga.replace('#',\"\")\n",
    "\n",
    "    sadness.append(riga)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sadness)):\n",
    "    #Controllo se l'emoticons c'è e in caso la rimuovo\n",
    "    for singleEmoticon in emoticons:\n",
    "        emoticonJoy = re.findall(re.escape(singleEmoticon),sadness[i])\n",
    "        if(len(emoticonsFind) != 0):\n",
    "            emoticonSadness.extend(emoticonsFind)\n",
    "            sadness[i] = sadness[i].replace(singleEmoticon,\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sadness)):\n",
    "    #preleviamo le emoji e le rimuoviamo dal testo\n",
    "    data = regex.findall(r'\\X', sadness[i])\n",
    "    for word in data:\n",
    "        if any(char in emoji.UNICODE_EMOJI['en'] for char in word):\n",
    "            emojiSadness = [].append(word)\n",
    "            sadness[i] = sadness[i].replace(word, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processamento slang prima di punteggiatura e lowercase\n",
    "with open(\"Slangs/slangs_before.json\") as f:\n",
    "    slangsBefore = json.load(f)\n",
    "    for i in range(len(sadness)):\n",
    "        for word in sadness[i].split():\n",
    "            indexOfWord = sadness[i].find(word)\n",
    "            if slangsBefore.get(word) is None:\n",
    "                if slangsBefore.get(word.lower()) is None:\n",
    "                    if slangsBefore.get(word.upper()) is None:\n",
    "                        continue\n",
    "                    else:\n",
    "                        sadness[i] = sadness[i][:indexOfWord] + slangsBefore.get(word.upper()) + sadness[i][indexOfWord+len(word):]\n",
    "                else:\n",
    "                    sadness[i] = sadness[i][:indexOfWord] + slangsBefore.get(word.lower()) + sadness[i][indexOfWord+len(word):]\n",
    "            else:\n",
    "                sadness[i] = sadness[i][:indexOfWord] + slangsBefore.get(word) + sadness[i][indexOfWord+len(word):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sadness)):\n",
    "    #Trattamento punteggiatura\n",
    "    line = re.compile(\"[\" + re.escape(\"\".join(puntuaction)) + \"]\")\n",
    "    sadness[i] = re.sub(line, \"\", sadness[i])\n",
    "\n",
    "    #Rendiamo la farse in minuscolo\n",
    "    sadness[i] = sadness[i].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizzazione di tutte le frasi\n",
    "sadnessTokenized = []\n",
    "for linea in sadness:\n",
    "    result = word_tokenize(linea)\n",
    "    sadnessTokenized.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Esecuzione della POS tagging per ogni riga della matrice\n",
    "for i in range(len(sadnessTokenized)):\n",
    "    sadnessTokenized[i] = nltk.pos_tag(sadnessTokenized[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Slangs vengono modificati con la loro definizione\n",
    "# with open(\"Slangs/slangs.json\", \"r\", encoding='utf-8') as f:\n",
    "#     slangs = json.load(f)\n",
    "#     for row in range(len(sadnessTokenized)):\n",
    "#         for column in range(len(sadnessTokenized[row])):\n",
    "#             if sadnessTokenized[row][column] in slangs:\n",
    "#                 sadnessTokenized[row][column] = slangs.get(sadnessTokenized[row][column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming\n",
    "#ps = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for line in sadnessTokenized:\n",
    "    for i in range(len(line)):\n",
    "        tupla = [lemmatizer.lemmatize(line[i][0]),line[i][1]]\n",
    "        line[i] = tupla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trattamnento Stop word dopo Stemming\n",
    "stop_words = set(stopwords.words('english'))\n",
    "sadnessTokStoped = []\n",
    "for line in sadnessTokenized:\n",
    "    filtred_word = []\n",
    "    for row in line:\n",
    "        if  row[0] not in stop_words:\n",
    "            filtred_word.append(row) \n",
    "    sadnessTokStoped.append(filtred_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sadnessListWord = []\n",
    "sadnessListTok = []\n",
    "for line in sadnessTokStoped:\n",
    "    for row in line:\n",
    "        sadnessListWord.append(row[0])\n",
    "        sadnessListTok.append(row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lavorazione risorse lessicali\n",
    "resourceSadness = []\n",
    "file = open(\"Risorse lessicali\\\\Sadness\\\\NRC_sadness.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        if(riga not in resourceSadness):\n",
    "            resourceSadness.append(line)\n",
    "\n",
    "file.close()\n",
    "\n",
    "file = open(\"Risorse lessicali\\Sadness\\sentisense_joy.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        if(riga not in resourceSadness):\n",
    "            resourceSadness.append(line)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trattamento dataset Surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lista di righe, ogni riga una tweet surprise\n",
    "file = open(\"Dataset/dataset_dt_surprise_60k.txt\", 'r',encoding=\"utf8\")\n",
    "surprise = []\n",
    "hashtagSurprise = []\n",
    "emoticonSurprise = []\n",
    "emojiSurprise = []\n",
    "\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "    riga = re.sub(r'URL', '',riga)\n",
    "    riga = re.sub(r'USERNAME', '',riga)\n",
    "\n",
    "     #array di hashtag singoli\n",
    "    hashTag = (re.findall('#[a-zA-Z]+', riga))\n",
    "\n",
    "    #generiamo l'array di hashtag senza hashtag\n",
    "    for h in hashTag:\n",
    "        hashtagSurprise.append(h.replace(\"#\",\"\"))\n",
    "\n",
    "    #Rimuoviamo glin hashtag dalla riga\n",
    "    riga = riga.replace('#',\"\")\n",
    "\n",
    "    surprise.append(riga)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(surprise)):\n",
    "    #Controllo se l'emoticons c'è e in caso la rimuovo\n",
    "    for singleEmoticon in emoticons:\n",
    "        emoticonJoy = re.findall(re.escape(singleEmoticon),surprise[i])\n",
    "        if(len(emoticonsFind) != 0):\n",
    "            emoticonSurprise.extend(emoticonsFind)\n",
    "            surprise[i] = surprise[i].replace(singleEmoticon,\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(surprise)):\n",
    "    #preleviamo le emoji e le rimuoviamo dal testo\n",
    "    data = regex.findall(r'\\X', surprise[i])\n",
    "    for word in data:\n",
    "        if any(char in emoji.UNICODE_EMOJI['en'] for char in word):\n",
    "            emojiSurprise = [].append(word)\n",
    "            surprise[i] = surprise[i].replace(word, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processamento slang prima di punteggiatura e lowercase\n",
    "with open(\"Slangs/slangs_before.json\") as f:\n",
    "    slangsBefore = json.load(f)\n",
    "    for i in range(len(surprise)):\n",
    "        for word in surprise[i].split():\n",
    "            indexOfWord = surprise[i].find(word)\n",
    "            if slangsBefore.get(word) is None:\n",
    "                if slangsBefore.get(word.lower()) is None:\n",
    "                    if slangsBefore.get(word.upper()) is None:\n",
    "                        continue\n",
    "                    else:\n",
    "                        surprise[i] = surprise[i][:indexOfWord] + slangsBefore.get(word.upper()) + surprise[i][indexOfWord+len(word):]\n",
    "                else:\n",
    "                    surprise[i] = surprise[i][:indexOfWord] + slangsBefore.get(word.lower()) + surprise[i][indexOfWord+len(word):]\n",
    "            else:\n",
    "                surprise[i] = surprise[i][:indexOfWord] + slangsBefore.get(word) + surprise[i][indexOfWord+len(word):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(surprise)):\n",
    "    #Trattamento punteggiatura\n",
    "    line = re.compile(\"[\" + re.escape(\"\".join(puntuaction)) + \"]\")\n",
    "    surprise[i] = re.sub(line, \"\", surprise[i])\n",
    "\n",
    "    #Rendiamo la farse in minuscolo\n",
    "    surprise[i] = surprise[i].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizzazione di tutte le frasi\n",
    "surpriseTokenized = []\n",
    "for linea in surprise:\n",
    "    result = word_tokenize(linea)\n",
    "    surpriseTokenized.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Esecuzione della POS tagging per ogni riga della matrice\n",
    "for i in range(len(surpriseTokenized)):\n",
    "    surpriseTokenized[i] = nltk.pos_tag(surpriseTokenized[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Slangs vengono modificati con la loro definizione\n",
    "# with open(\"Slangs/slangs.json\", \"r\", encoding='utf-8') as f:\n",
    "#     slangs = json.load(f)\n",
    "#     for row in range(len(surpriseTokenized)):\n",
    "#         for column in range(len(surpriseTokenized[row])):\n",
    "#             if surpriseTokenized[row][column] in slangs:\n",
    "#                 surpriseTokenized[row][column] = slangs.get(surpriseTokenized[row][column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming\n",
    "#ps = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for line in surpriseTokenized:\n",
    "    for i in range(len(line)):\n",
    "        tupla = [lemmatizer.lemmatize(line[i][0]),line[i][1]]\n",
    "        line[i] = tupla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trattamnento Stop word dopo Stemming\n",
    "stop_words = set(stopwords.words('english'))\n",
    "surpriseTokStoped = []\n",
    "for line in surpriseTokenized:\n",
    "    filtred_word = []\n",
    "    for row in line:\n",
    "        if  row[0] not in stop_words:\n",
    "            filtred_word.append(row) \n",
    "    surpriseTokStoped.append(filtred_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surpriseListWord = []\n",
    "surpriseListTok = []\n",
    "for line in surpriseTokStoped:\n",
    "    for row in line:\n",
    "        surpriseListWord.append(row[0])\n",
    "        surpriseListTok.append(row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lavorazione risorse lessicali\n",
    "resourceSurprise = []\n",
    "file = open(\"Risorse lessicali\\\\Surprise\\\\NRC_surprise.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        if(riga not in resourceSurprise):\n",
    "            resourceSurprise.append(line)\n",
    "\n",
    "file.close()\n",
    "\n",
    "file = open(\"Risorse lessicali\\Surprise\\sentisense_surprise.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        if(riga not in resourceSurprise):\n",
    "            resourceSurprise.append(line)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trattamento dataset Trust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lista di righe, ogni riga una tweet trust\n",
    "file = open(\"Dataset/dataset_dt_trust_60k.txt\", 'r',encoding=\"utf8\")\n",
    "trust = []\n",
    "hashtagTrust = []\n",
    "emoticonTrust = []\n",
    "emojiTrust = []\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "    riga = re.sub(r'URL', '',riga)\n",
    "    riga = re.sub(r'USERNAME', '',riga)\n",
    "    \n",
    "    #array di hashtag singoli\n",
    "    hashTag = (re.findall('#[a-zA-Z]+', riga))\n",
    "    \n",
    "    #generiamo l'array di hashtag senza hashtag\n",
    "    for h in hashTag:\n",
    "        hashtagTrust.append(h.replace(\"#\",\"\"))\n",
    "\n",
    "    #Rimuoviamo glin hashtag dalla riga\n",
    "    riga = riga.replace('#',\"\")\n",
    "\n",
    "    trust.append(riga)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(trust)):\n",
    "    #Controllo se l'emoticons c'è e in caso la rimuovo\n",
    "    for singleEmoticon in emoticons:\n",
    "        emoticonJoy = re.findall(re.escape(singleEmoticon),trust[i])\n",
    "        if(len(emoticonsFind) != 0):\n",
    "            emoticonTrust.extend(emoticonsFind)\n",
    "            trust[i] = trust[i].replace(singleEmoticon,\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(trust)):\n",
    "    #preleviamo le emoji e le rimuoviamo dal testo\n",
    "    data = regex.findall(r'\\X', trust[i])\n",
    "    for word in data:\n",
    "        if any(char in emoji.UNICODE_EMOJI['en'] for char in word):\n",
    "            emojiTrust = [].append(word)\n",
    "            trust[i] = trust[i].replace(word, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processamento slang prima di punteggiatura e lowercase\n",
    "with open(\"Slangs/slangs_before.json\") as f:\n",
    "    slangsBefore = json.load(f)\n",
    "    for i in range(len(trust)):\n",
    "        for word in trust[i].split():\n",
    "            indexOfWord = trust[i].find(word)\n",
    "            if slangsBefore.get(word) is None:\n",
    "                if slangsBefore.get(word.lower()) is None:\n",
    "                    if slangsBefore.get(word.upper()) is None:\n",
    "                        continue\n",
    "                    else:\n",
    "                        trust[i] = trust[i][:indexOfWord] + slangsBefore.get(word.upper()) + trust[i][indexOfWord+len(word):]\n",
    "                else:\n",
    "                    trust[i] = trust[i][:indexOfWord] + slangsBefore.get(word.lower()) + trust[i][indexOfWord+len(word):]\n",
    "            else:\n",
    "                trust[i] = trust[i][:indexOfWord] + slangsBefore.get(word) + trust[i][indexOfWord+len(word):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(trust)):\n",
    "    #Trattamento punteggiatura\n",
    "    line = re.compile(\"[\" + re.escape(\"\".join(puntuaction)) + \"]\")\n",
    "    trust[i] = re.sub(line, \"\", trust[i])\n",
    "\n",
    "    #Rendiamo la farse in minuscolo\n",
    "    trust[i] = trust[i].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizzazione di tutte le frasi\n",
    "trustTokenized = []\n",
    "for linea in trust:\n",
    "    result = word_tokenize(linea)\n",
    "    trustTokenized.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Esecuzione della POS tagging per ogni riga della matrice\n",
    "for i in range(len(trustTokenized)):\n",
    "    trustTokenized[i] = nltk.pos_tag(trustTokenized[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming\n",
    "#ps = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for line in trustTokenized:\n",
    "    for i in range(len(line)):\n",
    "        tupla = [lemmatizer.lemmatize(line[i][0]),line[i][1]]\n",
    "        line[i] = tupla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trattamnento Stop word dopo Stemming\n",
    "stop_words = set(stopwords.words('english'))\n",
    "trustTokStoped = []\n",
    "for line in trustTokenized:\n",
    "    filtred_word = []\n",
    "    for row in line:\n",
    "        if  row[0] not in stop_words:\n",
    "            filtred_word.append(row) \n",
    "    trustTokStoped.append(filtred_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trustListWord = []\n",
    "trustListTok = []\n",
    "for line in trustTokStoped:\n",
    "    for row in line:\n",
    "        trustListWord.append(row[0])\n",
    "        trustListTok.append(row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lavorazione risorse lessicali\n",
    "resourceTrust = []\n",
    "file = open(\"Risorse lessicali\\\\Trust\\\\NRC_trust.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        if(riga not in resourceTrust):\n",
    "            resourceTrust.append(line)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Slangs vengono modificati con la loro definizione\n",
    "# with open(\"Slangs/slangs.json\", \"r\", encoding='utf-8') as f:\n",
    "#     slangs = json.load(f)\n",
    "#     for row in range(len(trustTokenized)):\n",
    "#         for column in range(len(trustTokenized[row])):\n",
    "#             if trustTokenized[row][column] in slangs:\n",
    "#                 trustTokenized[row][column] = slangs.get(trustTokenized[row][column])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "250b308242d36e83629721c7244075b381b1b678cf3b391cdc434a91845cedac"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
