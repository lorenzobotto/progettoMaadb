{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lores\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\lores\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\lores\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\lores\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\lores\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Import per il resto delle celle\n",
    "import re\n",
    "import emoji\n",
    "import regex\n",
    "import nltk\n",
    "import json\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from collections import Counter\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "import pymongo\n",
    "import json\n",
    "\n",
    "posemoticons = [ 'B-)',':)', ':-)', \":')\", \":'-)\", ':D', ':-D',':\\'-)',\":')\",':o)',':]',':3',':c)',':>','=]','8)','=)',\\\n",
    "\n",
    "    ':}',':^)','8-D','8D','x-D','xD','X-D','XD','=-D','=D','=-3','=3','B^D',':-))',':*',':^*','( \\'}{\\' )',\\\n",
    "\n",
    "    '^^','(^_^)','^-^',\"^.^\",\"^3\\^\",\"\\^L\\^\"]\n",
    "\n",
    "\n",
    "\n",
    "negemoticons = [':(', ':-(', \":'(\", \":'-(\",'>:[', ':-c', ':c', ':-<', ':<', ':-[', ':[', ':{',':\\'-(', ':\\'(',\\\n",
    "\n",
    "                ' _( ',':\\'[',\"='(\",\"' [\",\"='[\",\\\n",
    "\n",
    "                \":'-<\", \":' <\", \":'<\", \"=' <\", \"='<\", \"T_T\", \"T.T\",\"(T_T)\", \"y_y\",\"y.y\",\"(Y_Y)\",\\\n",
    "\n",
    "                \";-;\", \";_;\",\";.;\",\":_:\",\"o .__. o\",\".-.\"]\n",
    "\n",
    "emoticons = [':‑)',':)',':-]',':]',':->',':>','8-)','8)',':-}',':}',':o)',':c)',':^)','=]','=)',':‑D',':D','8‑D','8D','=D','=3','B^D','c:',\n",
    "'C:','x‑D','xD','X‑D','XD',':-))',':‑(',':(',':‑c',':c',':‑<',':<',':‑[',':[',':-||','>:[',':{',':@',':('',;(',\":'‑(\",\":'(\",':=(',\":'‑)\",\":')\",\n",
    "':\"D',\"D‑':\",'D:<','D:','D8','D;','D=','DX',':‑O',':O',':‑o',':o',':-0','8‑0','>:O','=O','=o','=0',':-3',':3','=3','x3','X3','>:3',':-*',\n",
    "':*',':×',';‑)',';)','*-)','*)',';‑]',';]',';^)',';>',':‑,',';D',';3',':‑P',':P','X‑P','XP','x‑p','xp',':‑p',':p',':‑Þ',':Þ',':‑þ',':þ',':‑b',':b',\n",
    "'d:','=p','>:P',':-/',':/',':‑.',\">:/\",'=/',':L','=L',':S',':‑|',':|',':$','://)','://3',':‑X',':X',':‑#',':#',':‑&',':&','O:‑)','O:)','0:‑3'\n",
    ",'0:3','0:‑)','0:)','0;^)','>:‑)','>:)','}:‑)','}:)','3:‑)','3:)','>;‑)','>;)','>:3',';3','|;‑)','|‑O','B-)',':‑J','#‑)','%‑)','%)',':‑###..'\n",
    ",':###..','<:‑|',\"',:-|\",\"',:-l\",':E','8-X','8=X','x-3','x=3','~:>','v.v','._.','._.;','QQ  ','qq','Qq','X_X','x_x','+_+','X_x','x_X'\n",
    "'<_<','>_>','<.<','>.>','O_O','o_o','O-O','o‑o','O_o','o_O','>.<','>_<','^5','o/\\o','>_>^ ^<_<','V.v.V','V=(° °)=V','(^^^)','(::[]::)',\n",
    "'(o)(o)','( • )( • )','[̲̅$̲̅(̲̅1̲̅)̲̅$̲̅]','[̲̅$̲̅(̲̅5̲̅)̲̅$̲̅]','[̲̅$̲̅(̲̅10)̲̅$̲̅]','[̲̅$̲̅(̲̅100)̲̅$̲̅]','( ͡° ͜ʖ ͡°)','ヽ༼ຈل͜ຈ༽ﾉ','(๑ˇεˇ๑)','(◕‿◕✿)','( ༎ຶ ۝ ༎ຶ )','(=ʘᆽʘ=)∫','ʕ •ᴥ•ʔ',\n",
    "'(>_<)','(>_<)>','(>w<)',\"(';')\",'(^_^;)','(-_-;)','(~_~;)','(・.・;)','(・_・;)','^^;','(・・;)','^_^;','(#^.^#)'\n",
    ",'(^^;)','(⁄ ⁄•⁄ω⁄•⁄ ⁄)','(-.-)y-°°°','(^.^)y-.o○','(-_-)zzz','(^_-)-☆','(^_-)','((+_+))','(+o+)','(°°)','(°-°) (°.°)',\n",
    "'(°_°)','(°_°>)','(°レ°)','(o|o)','<(｀^´)>','^_^','(°o°)','(^_^)/','(^O^)／','(^o^)／','(^^)/','(≧∇≦)/','(/◕ヮ◕)/','(^o^)丿','∩(·ω·)∩',\n",
    "'(·ω·)','^ω^','_(._.)_','_(_^_)_','<(_ _)>','<m(__)m>','m(__)m','m(_ _)m','(凸ಠ益ಠ)凸',\"('_')\",'(/_;)','(T_T)','(;_;)','(;_;','(;_:)','(;O;)'\n",
    ",'(:_;)','(ToT)','(Ｔ▽Ｔ)',';_;',';-;',';n;',';;','Q.Q','T.T','TnT','QQ','Q_Q',\n",
    "'(ー_ー)!!','(-.-)','(-_-)','(一一)','(；一_一)','(=_=)','(=^・^=)','(=^・・^=)','=^_^=','(..)','(._.)',\n",
    "'.o○','○o.','_旦~~','( ^^)','_U~~','( ^^)','☆ミ','☆彡','>°)))彡','(Q))',\n",
    "'><ヨヨ (°))<<','>°))))彡','<°)))彡','>°))彡','<+','))><<','<*))','>=<','<コ:彡',\n",
    "'Ｃ:.ミ','~>°)～～～','～°·_·°～','(°°)～','●～*','￣|○',':3ミ','^m^','(・・?','(?_?)','>^_^<','<^!^>',\n",
    "'^/^','（*^_^*','§^.^§','(^<^)','(^.^)','(^ム^)','(^·^)','(^.^)','(^_^.)','(^_^)','(^^)','(^J^)',\n",
    "'(*^.^*)','^_^','(#^.^#)','（^—^）','(^^)/~~~','(^_^)/~','(;_;)/~~~','(^.^)/~~~',\n",
    "'(-_-)/~~~','($··)/~~~','(@^^)/~~~','(T_T)/~~~','(V)o￥o(V)','＼(~o~)／','＼(^o^)／','＼(-o-)／','ヽ(^。^)ノ',\n",
    "'ヽ(^o^)丿','(*^0^*)','(*_*)','(*_*;','(+_+) (@_@)','(@_@。','(＠_＠;)',\n",
    " '＼(◎o◎)／！','(*^^)v','(^^)v','(^_^)v','（’-’*) (＾ｖ＾)','(＾▽＾)','(・∀・)','(´∀`)','(⌒▽⌒）',\n",
    " '＼(^o^)／','\\(^o^)/','(~o~)','(~_~)','(^^ゞ','ˊ＿>ˋ','(p_-)', '(-_q)','((d[-_-]b))','(-\"-)','(ーー゛)',\n",
    " '(^_^メ)', '(-_-メ)','(~_~メ)','(－－〆)','(・へ・)','(｀´)'\n",
    " , '<`～´>','<`ヘ´>','(ーー;)','(^0_0^)','(＾ｖ＾)', '(＾ｕ＾)','(＾◇＾)','( ^)o(^ )','(^O^)','(^o^)','(^○^)',')^o^(',\n",
    " '(*^▽^*)','(￣ー￣)','(￣□￣;)', '°o°','°O°',':O','o_O','o_0', 'o.O','(o.o)','oO','(°◇°)','（ ﾟ Дﾟ)','(*￣m￣)',\n",
    " 'ヽ(´ー｀)┌','¯\\_(ツ)_/¯','¯\\(°_o)/¯','(´･ω･`)', '(‘A`)','(づ￣ ³￣)づ','(*^3^)/~☆','(︶｡︶✽)','(-_-) zzz'\n",
    " , 'uwu','UwU','OWO','OwO','～°·_·°～', '(°°)～',':3ミ','(´･ω･`)','(`･ω･´)','(｀-´)>', '（　´_ゝ`）'] \n",
    "\n",
    "puntuaction = ['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '.',\n",
    "           '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', \n",
    "           '`', '{', '|', '}', '~', '»', '«', '“', '”', \"-\"]\n",
    "\n",
    "\n",
    "mongoClient = pymongo.MongoClient(\"mongodb://admin:admin@localhost:27017/\")\n",
    "mydb = mongoClient[\"maadb\"]\n",
    "collectionTwitter = mydb[\"Twitter\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trattamento dataset Anger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lista di righe, ogni riga una tweet anger\n",
    "file = open(\"Dataset/dataset_dt_anger_60k.txt\", 'r',encoding=\"utf8\")\n",
    "anger = []\n",
    "hashtagAnger = []\n",
    "emoticonAnger = []\n",
    "emojiAnger = []\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    #Eliminazione URL e USERNAME\n",
    "    riga = re.sub(r'URL', '',riga)\n",
    "    riga = re.sub(r'USERNAME', '',riga)\n",
    "\n",
    "    #array di hashtag singoli\n",
    "    hashTag = (re.findall('#[a-zA-Z]+', riga))\n",
    "\n",
    "     #generiamo l'array di hashtag senza hashtag\n",
    "    for h in hashTag:\n",
    "        hashtagAnger.append(h.replace(\"#\",\"\"))\n",
    "\n",
    "    #Rimuoviamo glin hashtag dalla riga\n",
    "    riga = riga.replace('#',\"\")\n",
    "\n",
    "    #Salvataggio frase\n",
    "    anger.append(riga)\n",
    "\n",
    "\n",
    "file.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(anger)):\n",
    "    #Controllo se l'emoticons c'è e in caso la rimuovo\n",
    "    for singleEmoticon in emoticons:\n",
    "        emoticonsFind = re.findall(re.escape(singleEmoticon),anger[i])\n",
    "        if(len(emoticonsFind) != 0):\n",
    "            emoticonAnger.extend(emoticonsFind)\n",
    "            anger[i] = anger[i].replace(singleEmoticon,\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(anger)):\n",
    "    #preleviamo le emoji e le rimuoviamo dal testo\n",
    "    data = regex.findall(r'\\X', anger[i])\n",
    "    for word in data:\n",
    "        if any(char in emoji.UNICODE_EMOJI['en'] for char in word):\n",
    "            emojiAnger.append(word)\n",
    "            anger[i] = anger[i].replace(word, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processamento slang \n",
    "with open(\"Slangs/slangs_before.json\") as f:\n",
    "    slangsBefore = json.load(f)\n",
    "    for i in range(len(anger)):\n",
    "        for word in anger[i].split():\n",
    "            indexOfWord = anger[i].find(word)\n",
    "            if slangsBefore.get(word) is None:\n",
    "                if slangsBefore.get(word.lower()) is None:\n",
    "                    if slangsBefore.get(word.upper()) is None:\n",
    "                        continue\n",
    "                    else:\n",
    "                        anger[i] = anger[i][:indexOfWord] + slangsBefore.get(word.upper()) + anger[i][indexOfWord+len(word):]\n",
    "                else:\n",
    "                    anger[i] = anger[i][:indexOfWord] + slangsBefore.get(word.lower()) + anger[i][indexOfWord+len(word):]\n",
    "            else:\n",
    "                anger[i] = anger[i][:indexOfWord] + slangsBefore.get(word) + anger[i][indexOfWord+len(word):]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(anger)):\n",
    "    #Trattamento punteggiatura\n",
    "    line = re.compile(\"[\" + re.escape(\"\".join(puntuaction)) + \"]\")\n",
    "    anger[i] = re.sub(line, \"\", anger[i])\n",
    "\n",
    "    #Rendiamo la farse in minuscolo\n",
    "    anger[i] = anger[i].lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizzazione di tutte le frasi\n",
    "angerTokenized = []\n",
    "for linea in anger:\n",
    "    result = word_tokenize(linea)\n",
    "    angerTokenized.append(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Esecuzione della POS tagging per ogni riga della matrice\n",
    "for i in range(len(angerTokenized)):\n",
    "    angerTokenized[i] = nltk.pos_tag(angerTokenized[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming\n",
    "#ps = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for line in angerTokenized:\n",
    "    for i in range(len(line)):\n",
    "        tupla = [lemmatizer.lemmatize(line[i][0]),line[i][1]]\n",
    "        line[i] = tupla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trattamnento Stop word dopo Stemming\n",
    "stop_words = set(stopwords.words('english'))\n",
    "angerTokStoped = []\n",
    "for line in angerTokenized:\n",
    "    filtred_word = []\n",
    "    for row in line:\n",
    "        if  row[0] not in stop_words:\n",
    "            filtred_word.append(row) \n",
    "    angerTokStoped.append(filtred_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "angerListWord = []\n",
    "angerListTok = []\n",
    "for line in angerTokStoped:\n",
    "    for row in line:\n",
    "        angerListWord.append(row[0])\n",
    "        angerListTok.append(row[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lavorazione risorse lessicali\n",
    "resourceAnger = []\n",
    "#Lista di righe, ogni riga una tweet anger\n",
    "file = open(\"Risorse lessicali\\Anger\\EmoSN_anger.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        if(riga not in resourceAnger):\n",
    "            resourceAnger.append(riga)\n",
    "\n",
    "file.close()\n",
    "\n",
    "file = open(\"Risorse lessicali\\\\Anger\\\\NRC_anger.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        if(riga not in resourceAnger):\n",
    "            resourceAnger.append(line)\n",
    "\n",
    "file.close()\n",
    "\n",
    "file = open(\"Risorse lessicali\\Anger\\sentisense_anger.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        if(riga not in resourceAnger):\n",
    "            resourceAnger.append(line)\n",
    "\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024\n"
     ]
    }
   ],
   "source": [
    "#Conteggio delle frequenze delle parole\n",
    "countWordAnger = Counter(angerListWord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione database e collezioni\n",
    "# Inserimento risorse lessicali e parole\n",
    "angerMongo = []\n",
    "wordsString = {}\n",
    "\n",
    "for i in range(len(angerListWord)):\n",
    "    wordsString = {   \n",
    "        \"lemma\": angerListWord[i],\n",
    "        \"POS\": angerListTok[i],\n",
    "        \"freq\": countWordAnger[angerListWord[i]]\n",
    "    }\n",
    "    banana={\"word\":wordsString}\n",
    "    angerMongo.append(banana)\n",
    "\n",
    "angerInsert = {\n",
    "    \"sentiment\":\"anger\",\n",
    "    \"listWords\": angerMongo,\n",
    "    \"hashtags\": hashtagAnger,\n",
    "    \"emoticons\": emoticonAnger,\n",
    "    \"emoji\": emojiAnger\n",
    "}\n",
    "\n",
    "# Using a JSON string\n",
    "with open('jsonAnger_data.json', 'w') as outfile:\n",
    "    outfile.write(json.dumps(angerInsert))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "angerInsert = {\n",
    "    \"sentiment\":\"anger\",\n",
    "    \"listWords\": angerMongo,\n",
    "    \"hashtags\": hashtagAnger,\n",
    "    \"emoticons\": emoticonAnger,\n",
    "    \"emoji\": emojiAnger\n",
    "}\n",
    "\n",
    "x = collectionTwitter.insert_one(angerInsert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = collectionTwitter.delete_one(angerInsert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Slangs vengono modificati con la loro definizione, dopo la punteggiatura e il lower\n",
    "# with open(\"Slangs/slangs.json\", \"r\", encoding='utf-8') as f:\n",
    "#     slangs = json.load(f)\n",
    "#     for row in range(len(angerTokenized)):\n",
    "#         for column in range(len(angerTokenized[row])):\n",
    "#             if angerTokenized[row][column] in slangs:\n",
    "#                 angerTokenized[row][column] = slangs.get(angerTokenized[row][column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRATTAMENTO STOP WORD FORSE EVITABILE\n",
    "#stop_words = set(stopwords.words('english'))\n",
    "#angerTokStoped = []\n",
    "#for line in angerTokenized:\n",
    "#    filtred_word = []\n",
    "#    for row in line:\n",
    "#        if  row not in stop_words:\n",
    "#            filtred_word.append(row)\n",
    "#    angerTokStoped.append(filtred_word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conteggio delle frequenze delle emoji\n",
    "countEmojiAnger = Counter(emojiAnger)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conteggio delle frequenze delle emoticons\n",
    "countEmoticonAnger = Counter(emoticonAnger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conteggio delle frequenze degli hashtags\n",
    "countHashtagAnger = Counter(hashtagAnger) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5379\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conteggio delle frequenze degli token del post tagging\n",
    "countTokAnger = Counter(angerListTok) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trattamento dataset Anticipation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lista di righe, ogni riga una tweet anticipation\n",
    "file = open(\"Dataset/dataset_dt_anticipation_60k.txt\", 'r',encoding=\"utf8\")\n",
    "anticipation = []\n",
    "hashtagAnticipation = []\n",
    "emoticonAnticipation = []\n",
    "emojiAnticipation = []\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "    riga = re.sub(r'URL', '',riga)\n",
    "    riga = re.sub(r'USERNAME', '',riga)\n",
    "\n",
    "    #array di hashtag singoli\n",
    "    hashTag = (re.findall('#[a-zA-Z]+', riga))\n",
    "\n",
    "    #generiamo l'array di hashtag senza hashtag\n",
    "    for h in hashTag:\n",
    "        hashtagAnticipation.append(h.replace(\"#\",\"\"))\n",
    "\n",
    "    #Rimuoviamo glin hashtag dalla riga\n",
    "    riga = riga.replace('#',\"\")\n",
    "    \n",
    "    anticipation.append(riga)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(anticipation)):\n",
    "    #Controllo se l'emoticons c'è e in caso la rimuovo\n",
    "    for singleEmoticon in emoticons:\n",
    "        emoticonsFind = re.findall(re.escape(singleEmoticon),anticipation[i])\n",
    "        if(len(emoticonsFind) != 0):\n",
    "            emoticonAnticipation.extend(emoticonsFind)\n",
    "            anticipation[i] = anticipation[i].replace(singleEmoticon,\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(anticipation)):\n",
    "    #preleviamo le emoji e le rimuoviamo dal testo\n",
    "    data = regex.findall(r'\\X', anticipation[i])\n",
    "    for word in data:\n",
    "        if any(char in emoji.UNICODE_EMOJI['en'] for char in word):\n",
    "            emojiAnticipation.append(word)\n",
    "            anticipation[i] = anticipation[i].replace(word, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processamento slang prima di punteggiatura e lowercase\n",
    "with open(\"Slangs/slangs_before.json\") as f:\n",
    "    slangsBefore = json.load(f)\n",
    "    for i in range(len(anticipation)):\n",
    "        for word in anticipation[i].split():\n",
    "            indexOfWord = anticipation[i].find(word)\n",
    "            if slangsBefore.get(word) is None:\n",
    "                if slangsBefore.get(word.lower()) is None:\n",
    "                    if slangsBefore.get(word.upper()) is None:\n",
    "                        continue\n",
    "                    else:\n",
    "                        anticipation[i] = anticipation[i][:indexOfWord] + slangsBefore.get(word.upper()) + anticipation[i][indexOfWord+len(word):]\n",
    "                else:\n",
    "                    anticipation[i] = anticipation[i][:indexOfWord] + slangsBefore.get(word.lower()) + anticipation[i][indexOfWord+len(word):]\n",
    "            else:\n",
    "                anticipation[i] = anticipation[i][:indexOfWord] + slangsBefore.get(word) + anticipation[i][indexOfWord+len(word):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(anticipation)):\n",
    "    #Trattamento punteggiatura\n",
    "    line = re.compile(\"[\" + re.escape(\"\".join(puntuaction)) + \"]\")\n",
    "    anticipation[i] = re.sub(line, \"\", anticipation[i])\n",
    "\n",
    "    #Rendiamo la farse in minuscolo\n",
    "    anticipation[i] = anticipation[i].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizzazione di tutte le frasi\n",
    "anticipationTokenized = []\n",
    "for linea in anticipation:\n",
    "    result = word_tokenize(linea)\n",
    "    anticipationTokenized.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Esecuzione della POS tagging per ogni riga della matrice\n",
    "for i in range(len(anticipationTokenized)):\n",
    "    anticipationTokenized[i] = nltk.pos_tag(anticipationTokenized[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming\n",
    "#ps = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for line in anticipationTokenized:\n",
    "    for i in range(len(line)):\n",
    "        tupla = [lemmatizer.lemmatize(line[i][0]),line[i][1]]\n",
    "        line[i] = tupla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trattamnento Stop word dopo Stemming\n",
    "stop_words = set(stopwords.words('english'))\n",
    "anticipationTokStoped = []\n",
    "for line in anticipationTokStoped:\n",
    "    filtred_word = []\n",
    "    for row in line:\n",
    "        if  row[0] not in stop_words:\n",
    "            filtred_word.append(row) \n",
    "    anticipationTokStoped.append(filtred_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lavorazione risorse lessicali\n",
    "resourceAnticipation = []\n",
    "#Lista di righe, ogni riga una tweet anger\n",
    "file = open(\"Risorse lessicali\\\\Anticipation\\\\NRC_anticipation.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        if(riga not in resourceAnticipation):\n",
    "            resourceAnticipation.append(line)\n",
    "\n",
    "file.close()\n",
    "\n",
    "file = open(\"Risorse lessicali\\Anticipation\\sentisense_anticipation.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        if(riga not in resourceAnticipation):\n",
    "            resourceAnticipation.append(line)\n",
    "\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anticipationListWord = []\n",
    "anticipationListTok = []\n",
    "for line in anticipationTokStoped:\n",
    "    for row in line:\n",
    "        anticipationListWord.append(row[0])\n",
    "        anticipationListTok.append(row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Slangs vengono modificati con la loro definizione\n",
    "# with open(\"Slangs/slangs.json\", \"r\", encoding='utf-8') as f:\n",
    "#     slangs = json.load(f)\n",
    "#     for row in range(len(anticipationTokenized)):\n",
    "#         for column in range(len(anticipationTokenized[row])):\n",
    "#             if anticipationTokenized[row][column] in slangs:\n",
    "#                 anticipationTokenized[row][column] = slangs.get(anticipationTokenized[row][column])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trattamento dataset Disgust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lista di righe, ogni riga una tweet disgust\n",
    "file = open(\"Dataset/dataset_dt_disgust_60k.txt\", 'r',encoding=\"utf8\")\n",
    "disgust = []\n",
    "hashtagDisgust = []\n",
    "emoticonDisgust = []\n",
    "emojiDisgust = []\n",
    "\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "    riga = re.sub(r'URL', '',riga)\n",
    "    riga = re.sub(r'USERNAME', '',riga)\n",
    "\n",
    "    #array di hashtag singoli\n",
    "    hashTag = (re.findall('#[a-zA-Z]+', riga))\n",
    "    \n",
    "    #generiamo l'array di hashtag senza hashtag\n",
    "    for h in hashTag:\n",
    "        hashtagDisgust.append(h.replace(\"#\",\"\"))\n",
    "\n",
    "    #Rimuoviamo glin hashtag dalla riga\n",
    "    riga = riga.replace('#',\"\")\n",
    "\n",
    "    disgust.append(riga)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(disgust)):\n",
    "    #Controllo se l'emoticons c'è e in caso la rimuovo\n",
    "    for singleEmoticon in emoticons:\n",
    "        emoticonsFind = re.findall(re.escape(singleEmoticon),disgust[i])\n",
    "        if(len(emoticonsFind) != 0):\n",
    "            emoticonDisgust.extend(emoticonsFind)\n",
    "            disgust[i] = disgust[i].replace(singleEmoticon,\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(disgust)):\n",
    "    #preleviamo le emoji e le rimuoviamo dal testo\n",
    "    data = regex.findall(r'\\X', disgust[i])\n",
    "    for word in data:\n",
    "        if any(char in emoji.UNICODE_EMOJI['en'] for char in word):\n",
    "            emojiDisgust = [].append(word)\n",
    "            disgust[i] = disgust[i].replace(word, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processamento slang prima di punteggiatura e lowercase\n",
    "with open(\"Slangs/slangs_before.json\") as f:\n",
    "    slangsBefore = json.load(f)\n",
    "    for i in range(len(disgust)):\n",
    "        for word in disgust[i].split():\n",
    "            indexOfWord = disgust[i].find(word)\n",
    "            if slangsBefore.get(word) is None:\n",
    "                if slangsBefore.get(word.lower()) is None:\n",
    "                    if slangsBefore.get(word.upper()) is None:\n",
    "                        continue\n",
    "                    else:\n",
    "                        disgust[i] = disgust[i][:indexOfWord] + slangsBefore.get(word.upper()) + disgust[i][indexOfWord+len(word):]\n",
    "                else:\n",
    "                    disgust[i] = disgust[i][:indexOfWord] + slangsBefore.get(word.lower()) + disgust[i][indexOfWord+len(word):]\n",
    "            else:\n",
    "                disgust[i] = disgust[i][:indexOfWord] + slangsBefore.get(word) + disgust[i][indexOfWord+len(word):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(disgust)):\n",
    "    #Trattamento punteggiatura\n",
    "    line = re.compile(\"[\" + re.escape(\"\".join(puntuaction)) + \"]\")\n",
    "    disgust[i] = re.sub(line, \"\", disgust[i])\n",
    "\n",
    "    #Rendiamo la farse in minuscolo\n",
    "    disgust[i] = disgust[i].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizzazione di tutte le frasi\n",
    "disgustTokenized = []\n",
    "for linea in disgust:\n",
    "    result = word_tokenize(linea)\n",
    "    disgustTokenized.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Esecuzione della POS tagging per ogni riga della matrice\n",
    "for i in range(len(disgustTokenized)):\n",
    "    disgustTokenized[i] = nltk.pos_tag(disgustTokenized[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming\n",
    "#ps = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for line in disgustTokenized:\n",
    "    for i in range(len(line)):\n",
    "        tupla = [lemmatizer.lemmatize(line[i][0]),line[i][1]]\n",
    "        line[i] = tupla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trattamnento Stop word dopo Stemming\n",
    "stop_words = set(stopwords.words('english'))\n",
    "disgustTokStoped = []\n",
    "for line in disgustTokStoped:\n",
    "    filtred_word = []\n",
    "    for row in line:\n",
    "        if  row[0] not in stop_words:\n",
    "            filtred_word.append(row) \n",
    "    disgustTokStoped.append(filtred_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lavorazione risorse lessicali\n",
    "resourceDisgust = []\n",
    "#Lista di righe, ogni riga una tweet anger\n",
    "file = open(\"Risorse lessicali\\Disgust-Hate\\sentisense_disgust.txt.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        if(riga not in resourceDisgust):\n",
    "            resourceDisgust.append(riga)\n",
    "\n",
    "file.close()\n",
    "\n",
    "file = open(\"C:Risorse lessicali\\\\Disgust-Hate\\\\NRC_disgust.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        if(riga not in resourceDisgust):\n",
    "            resourceDisgust.append(line)\n",
    "\n",
    "file.close()\n",
    "\n",
    "file = open(\"Risorse lessicali\\Disgust-Hate\\sentisense_hate.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        if(riga not in resourceDisgust):\n",
    "            resourceDisgust.append(line)\n",
    "\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disgustListWord = []\n",
    "disgustListTok = []\n",
    "for line in disgustTokStoped:\n",
    "    for row in line:\n",
    "        disgustListWord.append(row[0])\n",
    "        disgustListTok.append(row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Slangs vengono modificati con la loro definizione\n",
    "# with open(\"Slangs/slangs.json\", \"r\", encoding='utf-8') as f:\n",
    "#     slangs = json.load(f)\n",
    "#     for row in range(len(disgustTokenized)):\n",
    "#         for column in range(len(disgustTokenized[row])):\n",
    "#             if disgustTokenized[row][column] in slangs:\n",
    "#                 disgustTokenized[row][column] = slangs.get(disgustTokenized[row][column])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trattamento dataset Fear "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lista di righe, ogni riga una tweet fear\n",
    "file = open(\"Dataset/dataset_dt_fear_60k.txt\", 'r',encoding=\"utf8\")\n",
    "fear = []\n",
    "hashtagFear = []\n",
    "emoticonFear = []\n",
    "emojiFear = []\n",
    "\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "    riga = re.sub(r'URL', '',riga)\n",
    "    riga = re.sub(r'USERNAME', '',riga)\n",
    "\n",
    "     #array di hashtag singoli\n",
    "    hashTag = (re.findall('#[a-zA-Z]+', riga))\n",
    "\n",
    "    #generiamo l'array di hashtag senza hashtag\n",
    "    for h in hashTag:\n",
    "        hashtagFear.append(h.replace(\"#\",\"\"))\n",
    "\n",
    "    #Rimuoviamo glin hashtag dalla riga\n",
    "    riga = riga.replace('#',\"\")\n",
    "\n",
    "    fear.append(riga)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(fear)):\n",
    "    #Controllo se l'emoticons c'è e in caso la rimuovo\n",
    "    for singleEmoticon in emoticons:\n",
    "        emoticonsFind = re.findall(re.escape(singleEmoticon),fear[i])\n",
    "        if(len(emoticonsFind) != 0):\n",
    "            emoticonFear.extend(emoticonsFind)\n",
    "            fear[i] = fear[i].replace(singleEmoticon,\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(fear)):\n",
    "    #preleviamo le emoji e le rimuoviamo dal testo\n",
    "    data = regex.findall(r'\\X', fear[i])\n",
    "    for word in data:\n",
    "        if any(char in emoji.UNICODE_EMOJI['en'] for char in word):\n",
    "            emojiFear = [].append(word)\n",
    "            fear[i] = fear[i].replace(word, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processamento slang prima di punteggiatura e lowercase\n",
    "with open(\"Slangs/slangs_before.json\") as f:\n",
    "    slangsBefore = json.load(f)\n",
    "    for i in range(len(fear)):\n",
    "        for word in fear[i].split():\n",
    "            indexOfWord = fear[i].find(word)\n",
    "            if slangsBefore.get(word) is None:\n",
    "                if slangsBefore.get(word.lower()) is None:\n",
    "                    if slangsBefore.get(word.upper()) is None:\n",
    "                        continue\n",
    "                    else:\n",
    "                        fear[i] = fear[i][:indexOfWord] + slangsBefore.get(word.upper()) + fear[i][indexOfWord+len(word):]\n",
    "                else:\n",
    "                    fear[i] = fear[i][:indexOfWord] + slangsBefore.get(word.lower()) + fear[i][indexOfWord+len(word):]\n",
    "            else:\n",
    "                fear[i] = fear[i][:indexOfWord] + slangsBefore.get(word) + fear[i][indexOfWord+len(word):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(fear)):\n",
    "    #Trattamento punteggiatura\n",
    "    line = re.compile(\"[\" + re.escape(\"\".join(puntuaction)) + \"]\")\n",
    "    fear[i] = re.sub(line, \"\", fear[i])\n",
    "\n",
    "    #Rendiamo la farse in minuscolo\n",
    "    fear[i] = fear[i].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizzazione di tutte le frasi\n",
    "fearTokenized = []\n",
    "for linea in fear:\n",
    "    result = word_tokenize(linea)\n",
    "    fearTokenized.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Esecuzione della POS tagging per ogni riga della matrice\n",
    "for i in range(len(fearTokenized)):\n",
    "    fearTokenized[i] = nltk.pos_tag(fearTokenized[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming\n",
    "#ps = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for line in fearTokenized:\n",
    "    for i in range(len(line)):\n",
    "        tupla = [lemmatizer.lemmatize(line[i][0]),line[i][1]]\n",
    "        line[i] = tupla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trattamnento Stop word dopo Stemming\n",
    "stop_words = set(stopwords.words('english'))\n",
    "fearTokStoped = []\n",
    "for line in fearTokStoped:\n",
    "    filtred_word = []\n",
    "    for row in line:\n",
    "        if  row[0] not in stop_words:\n",
    "            filtred_word.append(row) \n",
    "    fearTokStoped.append(filtred_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lavorazione risorse lessicali\n",
    "resourceFear = []\n",
    "#Lista di righe, ogni riga una tweet anger\n",
    "file = open(\"Risorse lessicali\\\\Fear\\\\NRC_fear.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        if(riga not in resourceFear):\n",
    "            resourceFear.append(line)\n",
    "\n",
    "file.close()\n",
    "\n",
    "file = open(\"Risorse lessicali\\Fear\\sentisense_fear.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        if(riga not in resourceFear):\n",
    "            resourceFear.append(line)\n",
    "\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fearListWord = []\n",
    "fearListToken = []\n",
    "for line in fearTokStoped:\n",
    "    for row in line:\n",
    "        fearListWord.append(row[0])\n",
    "        fearListToken.append(row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Slangs vengono modificati con la loro definizione\n",
    "# with open(\"Slangs/slangs.json\", \"r\", encoding='utf-8') as f:\n",
    "#     slangs = json.load(f)\n",
    "#     for row in range(len(fearTokenized)):\n",
    "#         for column in range(len(fearTokenized[row])):\n",
    "#             if fearTokenized[row][column] in slangs:\n",
    "#                 fearTokenized[row][column] = slangs.get(fearTokenized[row][column])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trattamento dataset Joy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lista di righe, ogni riga una tweet joy\n",
    "file = open(\"Dataset/dataset_dt_joy_60k.txt\", 'r',encoding=\"utf8\")\n",
    "joy = []\n",
    "hashtagJoy = []\n",
    "emoticonJoy = []\n",
    "emojiJoy = []\n",
    "\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "    riga = re.sub(r'URL', '',riga)\n",
    "    riga = re.sub(r'USERNAME', '',riga)\n",
    "    \n",
    "    #array di hashtag singoli\n",
    "    hashTag = (re.findall('#[a-zA-Z]+', riga))\n",
    "\n",
    "    #generiamo l'array di hashtag senza hashtag\n",
    "    for h in hashTag:\n",
    "        hashtagJoy.append(h.replace(\"#\",\"\"))\n",
    "\n",
    "    #Rimuoviamo glin hashtag dalla riga\n",
    "    riga = riga.replace('#',\"\")\n",
    "\n",
    "    joy.append(riga)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(joy)):\n",
    "    #Controllo se l'emoticons c'è e in caso la rimuovo\n",
    "    for singleEmoticon in emoticons:\n",
    "        emoticonJoy = re.findall(re.escape(singleEmoticon),joy[i])\n",
    "        if(len(emoticonsFind) != 0):\n",
    "            emoticonFear.extend(emoticonsFind)\n",
    "            joy[i] = joy[i].replace(singleEmoticon,\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(joy)):\n",
    "    #preleviamo le emoji e le rimuoviamo dal testo\n",
    "    data = regex.findall(r'\\X', joy[i])\n",
    "    for word in data:\n",
    "        if any(char in emoji.UNICODE_EMOJI['en'] for char in word):\n",
    "            emojiJoy = [].append(word)\n",
    "            joy[i] = joy[i].replace(word, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processamento slang prima di punteggiatura e lowercase\n",
    "with open(\"Slangs/slangs_before.json\") as f:\n",
    "    slangsBefore = json.load(f)\n",
    "    for i in range(len(joy)):\n",
    "        for word in joy[i].split():\n",
    "            indexOfWord = joy[i].find(word)\n",
    "            if slangsBefore.get(word) is None:\n",
    "                if slangsBefore.get(word.lower()) is None:\n",
    "                    if slangsBefore.get(word.upper()) is None:\n",
    "                        continue\n",
    "                    else:\n",
    "                        joy[i] = joy[i][:indexOfWord] + slangsBefore.get(word.upper()) + joy[i][indexOfWord+len(word):]\n",
    "                else:\n",
    "                    joy[i] = joy[i][:indexOfWord] + slangsBefore.get(word.lower()) + joy[i][indexOfWord+len(word):]\n",
    "            else:\n",
    "                joy[i] = joy[i][:indexOfWord] + slangsBefore.get(word) + joy[i][indexOfWord+len(word):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(joy)):\n",
    "    #Trattamento punteggiatura\n",
    "    line = re.compile(\"[\" + re.escape(\"\".join(puntuaction)) + \"]\")\n",
    "    joy[i] = re.sub(line, \"\", joy[i])\n",
    "\n",
    "    #Rendiamo la farse in minuscolo\n",
    "    joy[i] = joy[i].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizzazione di tutte le frasi\n",
    "joyTokenized = []\n",
    "for linea in joy:\n",
    "    result = word_tokenize(linea)\n",
    "    joyTokenized.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Esecuzione della POS tagging per ogni riga della matrice\n",
    "for i in range(len(joyTokenized)):\n",
    "    joyTokenized[i] = nltk.pos_tag(joyTokenized[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Slangs vengono modificati con la loro definizione\n",
    "# with open(\"Slangs/slangs.json\", \"r\", encoding='utf-8') as f:\n",
    "#     slangs = json.load(f)\n",
    "#     for row in range(len(joyTokenized)):\n",
    "#         for column in range(len(joyTokenized[row])):\n",
    "#             if joyTokenized[row][column] in slangs:\n",
    "#                 joyTokenized[row][column] = slangs.get(joyTokenized[row][column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming\n",
    "#ps = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for line in joyTokenized:\n",
    "    for i in range(len(line)):\n",
    "        tupla = [lemmatizer.lemmatize(line[i][0]),line[i][1]]\n",
    "        line[i] = tupla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trattamnento Stop word dopo Stemming\n",
    "stop_words = set(stopwords.words('english'))\n",
    "joyTokStoped = []\n",
    "for line in joyTokenized:\n",
    "    filtred_word = []\n",
    "    for row in line:\n",
    "        if  row[0] not in stop_words:\n",
    "            filtred_word.append(row) \n",
    "    joyTokStoped.append(filtred_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joyListWord = []\n",
    "joyListTok = []\n",
    "for line in joyTokStoped:\n",
    "    for row in line:\n",
    "        joyListWord.append(row[0])\n",
    "        joyListTok.append(row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lavorazione risorse lessicali\n",
    "resourceJoy = []\n",
    "#Lista di righe, ogni riga una tweet anger\n",
    "file = open(\"Risorse lessicali\\Joy\\EmoSN_joy.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        if(riga not in resourceJoy):\n",
    "            resourceJoy.append(riga)\n",
    "\n",
    "file.close()\n",
    "\n",
    "file = open(\"Risorse lessicali\\\\Joy\\\\NRC_joy.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        if(riga not in resourceJoy):\n",
    "            resourceJoy.append(line)\n",
    "\n",
    "file.close()\n",
    "\n",
    "file = open(\"Risorse lessicali\\Joy\\sentisense_joy.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        if(riga not in resourceJoy):\n",
    "            resourceJoy.append(line)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trattamento dataset Sadness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lista di righe, ogni riga una tweet sadness\n",
    "file = open(\"Dataset/dataset_dt_sadness_60k.txt\", 'r',encoding=\"utf8\")\n",
    "sadness = []\n",
    "hashtagSadness = []\n",
    "emoticonSadness = []\n",
    "emojiSadness = []\n",
    "\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "    riga = re.sub(r'URL', '',riga)\n",
    "    riga = re.sub(r'USERNAME', '',riga)\n",
    "\n",
    "    #array di hashtag singoli\n",
    "    hashTag = (re.findall('#[a-zA-Z]+', riga))\n",
    "\n",
    "    #Rimuoviamo glin hashtag dalla riga\n",
    "    riga = riga.replace('#',\"\")\n",
    "\n",
    "    sadness.append(riga)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sadness)):\n",
    "    #Controllo se l'emoticons c'è e in caso la rimuovo\n",
    "    for singleEmoticon in emoticons:\n",
    "        emoticonJoy = re.findall(re.escape(singleEmoticon),sadness[i])\n",
    "        if(len(emoticonsFind) != 0):\n",
    "            emoticonSadness.extend(emoticonsFind)\n",
    "            sadness[i] = sadness[i].replace(singleEmoticon,\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sadness)):\n",
    "    #preleviamo le emoji e le rimuoviamo dal testo\n",
    "    data = regex.findall(r'\\X', sadness[i])\n",
    "    for word in data:\n",
    "        if any(char in emoji.UNICODE_EMOJI['en'] for char in word):\n",
    "            emojiSadness = [].append(word)\n",
    "            sadness[i] = sadness[i].replace(word, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processamento slang prima di punteggiatura e lowercase\n",
    "with open(\"Slangs/slangs_before.json\") as f:\n",
    "    slangsBefore = json.load(f)\n",
    "    for i in range(len(sadness)):\n",
    "        for word in sadness[i].split():\n",
    "            indexOfWord = sadness[i].find(word)\n",
    "            if slangsBefore.get(word) is None:\n",
    "                if slangsBefore.get(word.lower()) is None:\n",
    "                    if slangsBefore.get(word.upper()) is None:\n",
    "                        continue\n",
    "                    else:\n",
    "                        sadness[i] = sadness[i][:indexOfWord] + slangsBefore.get(word.upper()) + sadness[i][indexOfWord+len(word):]\n",
    "                else:\n",
    "                    sadness[i] = sadness[i][:indexOfWord] + slangsBefore.get(word.lower()) + sadness[i][indexOfWord+len(word):]\n",
    "            else:\n",
    "                sadness[i] = sadness[i][:indexOfWord] + slangsBefore.get(word) + sadness[i][indexOfWord+len(word):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sadness)):\n",
    "    #Trattamento punteggiatura\n",
    "    line = re.compile(\"[\" + re.escape(\"\".join(puntuaction)) + \"]\")\n",
    "    sadness[i] = re.sub(line, \"\", sadness[i])\n",
    "\n",
    "    #Rendiamo la farse in minuscolo\n",
    "    sadness[i] = sadness[i].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizzazione di tutte le frasi\n",
    "sadnessTokenized = []\n",
    "for linea in sadness:\n",
    "    result = word_tokenize(linea)\n",
    "    sadnessTokenized.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Esecuzione della POS tagging per ogni riga della matrice\n",
    "for i in range(len(sadnessTokenized)):\n",
    "    sadnessTokenized[i] = nltk.pos_tag(sadnessTokenized[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Slangs vengono modificati con la loro definizione\n",
    "# with open(\"Slangs/slangs.json\", \"r\", encoding='utf-8') as f:\n",
    "#     slangs = json.load(f)\n",
    "#     for row in range(len(sadnessTokenized)):\n",
    "#         for column in range(len(sadnessTokenized[row])):\n",
    "#             if sadnessTokenized[row][column] in slangs:\n",
    "#                 sadnessTokenized[row][column] = slangs.get(sadnessTokenized[row][column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming\n",
    "#ps = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for line in sadnessTokenized:\n",
    "    for i in range(len(line)):\n",
    "        tupla = [lemmatizer.lemmatize(line[i][0]),line[i][1]]\n",
    "        line[i] = tupla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trattamnento Stop word dopo Stemming\n",
    "stop_words = set(stopwords.words('english'))\n",
    "sadnessTokStoped = []\n",
    "for line in sadnessTokenized:\n",
    "    filtred_word = []\n",
    "    for row in line:\n",
    "        if  row[0] not in stop_words:\n",
    "            filtred_word.append(row) \n",
    "    sadnessTokStoped.append(filtred_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sadnessListWord = []\n",
    "sadnessListTok = []\n",
    "for line in sadnessTokStoped:\n",
    "    for row in line:\n",
    "        sadnessListWord.append(row[0])\n",
    "        sadnessListTok.append(row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lavorazione risorse lessicali\n",
    "resourceSadness = []\n",
    "file = open(\"Risorse lessicali\\\\Sadness\\\\NRC_sadness.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        if(riga not in resourceSadness):\n",
    "            resourceSadness.append(line)\n",
    "\n",
    "file.close()\n",
    "\n",
    "file = open(\"Risorse lessicali\\Sadness\\sentisense_joy.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        if(riga not in resourceSadness):\n",
    "            resourceSadness.append(line)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trattamento dataset Surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lista di righe, ogni riga una tweet surprise\n",
    "file = open(\"Dataset/dataset_dt_surprise_60k.txt\", 'r',encoding=\"utf8\")\n",
    "surprise = []\n",
    "hashtagSurprise = []\n",
    "emoticonSurprise = []\n",
    "emojiSurprise = []\n",
    "\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "    riga = re.sub(r'URL', '',riga)\n",
    "    riga = re.sub(r'USERNAME', '',riga)\n",
    "\n",
    "     #array di hashtag singoli\n",
    "    hashTag = (re.findall('#[a-zA-Z]+', riga))\n",
    "\n",
    "    #generiamo l'array di hashtag senza hashtag\n",
    "    for h in hashTag:\n",
    "        hashtagSurprise.append(h.replace(\"#\",\"\"))\n",
    "\n",
    "    #Rimuoviamo glin hashtag dalla riga\n",
    "    riga = riga.replace('#',\"\")\n",
    "\n",
    "    surprise.append(riga)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(surprise)):\n",
    "    #Controllo se l'emoticons c'è e in caso la rimuovo\n",
    "    for singleEmoticon in emoticons:\n",
    "        emoticonJoy = re.findall(re.escape(singleEmoticon),surprise[i])\n",
    "        if(len(emoticonsFind) != 0):\n",
    "            emoticonSurprise.extend(emoticonsFind)\n",
    "            surprise[i] = surprise[i].replace(singleEmoticon,\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(surprise)):\n",
    "    #preleviamo le emoji e le rimuoviamo dal testo\n",
    "    data = regex.findall(r'\\X', surprise[i])\n",
    "    for word in data:\n",
    "        if any(char in emoji.UNICODE_EMOJI['en'] for char in word):\n",
    "            emojiSurprise = [].append(word)\n",
    "            surprise[i] = surprise[i].replace(word, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processamento slang prima di punteggiatura e lowercase\n",
    "with open(\"Slangs/slangs_before.json\") as f:\n",
    "    slangsBefore = json.load(f)\n",
    "    for i in range(len(surprise)):\n",
    "        for word in surprise[i].split():\n",
    "            indexOfWord = surprise[i].find(word)\n",
    "            if slangsBefore.get(word) is None:\n",
    "                if slangsBefore.get(word.lower()) is None:\n",
    "                    if slangsBefore.get(word.upper()) is None:\n",
    "                        continue\n",
    "                    else:\n",
    "                        surprise[i] = surprise[i][:indexOfWord] + slangsBefore.get(word.upper()) + surprise[i][indexOfWord+len(word):]\n",
    "                else:\n",
    "                    surprise[i] = surprise[i][:indexOfWord] + slangsBefore.get(word.lower()) + surprise[i][indexOfWord+len(word):]\n",
    "            else:\n",
    "                surprise[i] = surprise[i][:indexOfWord] + slangsBefore.get(word) + surprise[i][indexOfWord+len(word):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(surprise)):\n",
    "    #Trattamento punteggiatura\n",
    "    line = re.compile(\"[\" + re.escape(\"\".join(puntuaction)) + \"]\")\n",
    "    surprise[i] = re.sub(line, \"\", surprise[i])\n",
    "\n",
    "    #Rendiamo la farse in minuscolo\n",
    "    surprise[i] = surprise[i].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizzazione di tutte le frasi\n",
    "surpriseTokenized = []\n",
    "for linea in surprise:\n",
    "    result = word_tokenize(linea)\n",
    "    surpriseTokenized.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Esecuzione della POS tagging per ogni riga della matrice\n",
    "for i in range(len(surpriseTokenized)):\n",
    "    surpriseTokenized[i] = nltk.pos_tag(surpriseTokenized[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Slangs vengono modificati con la loro definizione\n",
    "# with open(\"Slangs/slangs.json\", \"r\", encoding='utf-8') as f:\n",
    "#     slangs = json.load(f)\n",
    "#     for row in range(len(surpriseTokenized)):\n",
    "#         for column in range(len(surpriseTokenized[row])):\n",
    "#             if surpriseTokenized[row][column] in slangs:\n",
    "#                 surpriseTokenized[row][column] = slangs.get(surpriseTokenized[row][column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming\n",
    "#ps = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for line in surpriseTokenized:\n",
    "    for i in range(len(line)):\n",
    "        tupla = [lemmatizer.lemmatize(line[i][0]),line[i][1]]\n",
    "        line[i] = tupla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trattamnento Stop word dopo Stemming\n",
    "stop_words = set(stopwords.words('english'))\n",
    "surpriseTokStoped = []\n",
    "for line in surpriseTokenized:\n",
    "    filtred_word = []\n",
    "    for row in line:\n",
    "        if  row[0] not in stop_words:\n",
    "            filtred_word.append(row) \n",
    "    surpriseTokStoped.append(filtred_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surpriseListWord = []\n",
    "surpriseListTok = []\n",
    "for line in surpriseTokStoped:\n",
    "    for row in line:\n",
    "        surpriseListWord.append(row[0])\n",
    "        surpriseListTok.append(row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lavorazione risorse lessicali\n",
    "resourceSurprise = []\n",
    "file = open(\"Risorse lessicali\\\\Surprise\\\\NRC_surprise.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        if(riga not in resourceSurprise):\n",
    "            resourceSurprise.append(line)\n",
    "\n",
    "file.close()\n",
    "\n",
    "file = open(\"Risorse lessicali\\Surprise\\sentisense_surprise.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        if(riga not in resourceSurprise):\n",
    "            resourceSurprise.append(line)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trattamento dataset Trust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lista di righe, ogni riga una tweet trust\n",
    "file = open(\"Dataset/dataset_dt_trust_60k.txt\", 'r',encoding=\"utf8\")\n",
    "trust = []\n",
    "hashtagTrust = []\n",
    "emoticonTrust = []\n",
    "emojiTrust = []\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "    riga = re.sub(r'URL', '',riga)\n",
    "    riga = re.sub(r'USERNAME', '',riga)\n",
    "    \n",
    "    #array di hashtag singoli\n",
    "    hashTag = (re.findall('#[a-zA-Z]+', riga))\n",
    "    \n",
    "    #generiamo l'array di hashtag senza hashtag\n",
    "    for h in hashTag:\n",
    "        hashtagTrust.append(h.replace(\"#\",\"\"))\n",
    "\n",
    "    #Rimuoviamo glin hashtag dalla riga\n",
    "    riga = riga.replace('#',\"\")\n",
    "\n",
    "    trust.append(riga)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(trust)):\n",
    "    #Controllo se l'emoticons c'è e in caso la rimuovo\n",
    "    for singleEmoticon in emoticons:\n",
    "        emoticonJoy = re.findall(re.escape(singleEmoticon),trust[i])\n",
    "        if(len(emoticonsFind) != 0):\n",
    "            emoticonTrust.extend(emoticonsFind)\n",
    "            trust[i] = trust[i].replace(singleEmoticon,\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(trust)):\n",
    "    #preleviamo le emoji e le rimuoviamo dal testo\n",
    "    data = regex.findall(r'\\X', trust[i])\n",
    "    for word in data:\n",
    "        if any(char in emoji.UNICODE_EMOJI['en'] for char in word):\n",
    "            emojiTrust = [].append(word)\n",
    "            trust[i] = trust[i].replace(word, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processamento slang prima di punteggiatura e lowercase\n",
    "with open(\"Slangs/slangs_before.json\") as f:\n",
    "    slangsBefore = json.load(f)\n",
    "    for i in range(len(trust)):\n",
    "        for word in trust[i].split():\n",
    "            indexOfWord = trust[i].find(word)\n",
    "            if slangsBefore.get(word) is None:\n",
    "                if slangsBefore.get(word.lower()) is None:\n",
    "                    if slangsBefore.get(word.upper()) is None:\n",
    "                        continue\n",
    "                    else:\n",
    "                        trust[i] = trust[i][:indexOfWord] + slangsBefore.get(word.upper()) + trust[i][indexOfWord+len(word):]\n",
    "                else:\n",
    "                    trust[i] = trust[i][:indexOfWord] + slangsBefore.get(word.lower()) + trust[i][indexOfWord+len(word):]\n",
    "            else:\n",
    "                trust[i] = trust[i][:indexOfWord] + slangsBefore.get(word) + trust[i][indexOfWord+len(word):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(trust)):\n",
    "    #Trattamento punteggiatura\n",
    "    line = re.compile(\"[\" + re.escape(\"\".join(puntuaction)) + \"]\")\n",
    "    trust[i] = re.sub(line, \"\", trust[i])\n",
    "\n",
    "    #Rendiamo la farse in minuscolo\n",
    "    trust[i] = trust[i].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizzazione di tutte le frasi\n",
    "trustTokenized = []\n",
    "for linea in trust:\n",
    "    result = word_tokenize(linea)\n",
    "    trustTokenized.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Esecuzione della POS tagging per ogni riga della matrice\n",
    "for i in range(len(trustTokenized)):\n",
    "    trustTokenized[i] = nltk.pos_tag(trustTokenized[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming\n",
    "#ps = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for line in trustTokenized:\n",
    "    for i in range(len(line)):\n",
    "        tupla = [lemmatizer.lemmatize(line[i][0]),line[i][1]]\n",
    "        line[i] = tupla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trattamnento Stop word dopo Stemming\n",
    "stop_words = set(stopwords.words('english'))\n",
    "trustTokStoped = []\n",
    "for line in trustTokenized:\n",
    "    filtred_word = []\n",
    "    for row in line:\n",
    "        if  row[0] not in stop_words:\n",
    "            filtred_word.append(row) \n",
    "    trustTokStoped.append(filtred_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trustListWord = []\n",
    "trustListTok = []\n",
    "for line in trustTokStoped:\n",
    "    for row in line:\n",
    "        trustListWord.append(row[0])\n",
    "        trustListTok.append(row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lavorazione risorse lessicali\n",
    "resourceTrust = []\n",
    "file = open(\"Risorse lessicali\\\\Trust\\\\NRC_trust.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        if(riga not in resourceTrust):\n",
    "            resourceTrust.append(line)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Slangs vengono modificati con la loro definizione\n",
    "# with open(\"Slangs/slangs.json\", \"r\", encoding='utf-8') as f:\n",
    "#     slangs = json.load(f)\n",
    "#     for row in range(len(trustTokenized)):\n",
    "#         for column in range(len(trustTokenized[row])):\n",
    "#             if trustTokenized[row][column] in slangs:\n",
    "#                 trustTokenized[row][column] = slangs.get(trustTokenized[row][column])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "250b308242d36e83629721c7244075b381b1b678cf3b391cdc434a91845cedac"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
