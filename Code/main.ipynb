{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\gabri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\gabri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\gabri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\gabri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\gabri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Import per il resto delle celle\n",
    "import re\n",
    "import emoji\n",
    "import regex\n",
    "import nltk\n",
    "import json\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from collections import Counter\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "import pymongo\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import psycopg2\n",
    "%matplotlib inline\n",
    "#from wordcloud import WordCloud\n",
    "\n",
    "emoticons = [':‑)',':)',':-]',':]',':->',':>','8-)','8)',':-}',':}',':o)',':c)',':^)','=]','=)',':‑D',':D','8‑D','8D','=D','=3','B^D','c:',\n",
    "'C:','x‑D','xD','X‑D','XD',':-))',':‑(',':(',':‑c',':c',':‑<',':<',':‑[',':[',':-||','>:[',':{',':@',':('',;(',\":'‑(\",\":'(\",':=(',\":'‑)\",\":')\",\n",
    "':\"D',\"D‑':\",'D:<','D:','D8','D;','D=','DX',':‑O',':O',':‑o',':o',':-0','8‑0','>:O','=O','=o','=0',':-3',':3','=3','x3','X3','>:3',':-*',\n",
    "':*',':×',';‑)',';)','*-)','*)',';‑]',';]',';^)',';>',':‑,',';D',';3',':‑P',':P','X‑P','XP','x‑p','xp',':‑p',':p',':‑Þ',':Þ',':‑þ',':þ',':‑b',':b',\n",
    "'d:','=p','>:P',':-/',':/',':‑.',\">:/\",'=/',':L','=L',':S',':‑|',':|',':$','://)','://3',':‑X',':X',':‑#',':#',':‑&',':&','O:‑)','O:)','0:‑3'\n",
    ",'0:3','0:‑)','0:)','0;^)','>:‑)','>:)','}:‑)','}:)','3:‑)','3:)','>;‑)','>;)','>:3',';3','|;‑)','|‑O','B-)',':‑J','#‑)','%‑)','%)',':‑###..'\n",
    ",':###..','<:‑|',\"',:-|\",\"',:-l\",':E','8-X','8=X','x-3','x=3','~:>','v.v','._.','._.;','QQ  ','qq','Qq','X_X','x_x','+_+','X_x','x_X'\n",
    "'<_<','>_>','<.<','>.>','O_O','o_o','O-O','o‑o','O_o','o_O','>.<','>_<','^5','o/\\o','>_>^ ^<_<','V.v.V','V=(° °)=V','(^^^)','(::[]::)',\n",
    "'(o)(o)','( • )( • )','[̲̅$̲̅(̲̅1̲̅)̲̅$̲̅]','[̲̅$̲̅(̲̅5̲̅)̲̅$̲̅]','[̲̅$̲̅(̲̅10)̲̅$̲̅]','[̲̅$̲̅(̲̅100)̲̅$̲̅]','( ͡° ͜ʖ ͡°)','ヽ༼ຈل͜ຈ༽ﾉ','(๑ˇεˇ๑)','(◕‿◕✿)','( ༎ຶ ۝ ༎ຶ )','(=ʘᆽʘ=)∫','ʕ •ᴥ•ʔ',\n",
    "'(>_<)','(>_<)>','(>w<)',\"(';')\",'(^_^;)','(-_-;)','(~_~;)','(・.・;)','(・_・;)','^^;','(・・;)','^_^;','(#^.^#)'\n",
    ",'(^^;)','(⁄ ⁄•⁄ω⁄•⁄ ⁄)','(-.-)y-°°°','(^.^)y-.o○','(-_-)zzz','(^_-)-☆','(^_-)','((+_+))','(+o+)','(°°)','(°-°) (°.°)',\n",
    "'(°_°)','(°_°>)','(°レ°)','(o|o)','<(｀^´)>','^_^','(°o°)','(^_^)/','(^O^)／','(^o^)／','(^^)/','(≧∇≦)/','(/◕ヮ◕)/','(^o^)丿','∩(·ω·)∩',\n",
    "'(·ω·)','^ω^','_(._.)_','_(_^_)_','<(_ _)>','<m(__)m>','m(__)m','m(_ _)m','(凸ಠ益ಠ)凸',\"('_')\",'(/_;)','(T_T)','(;_;)','(;_;','(;_:)','(;O;)'\n",
    ",'(:_;)','(ToT)','(Ｔ▽Ｔ)',';_;',';-;',';n;',';;','Q.Q','T.T','TnT','QQ','Q_Q',\n",
    "'(ー_ー)!!','(-.-)','(-_-)','(一一)','(；一_一)','(=_=)','(=^・^=)','(=^・・^=)','=^_^=','(..)','(._.)',\n",
    "'.o○','○o.','_旦~~','( ^^)','_U~~','( ^^)','☆ミ','☆彡','>°)))彡','(Q))',\n",
    "'><ヨヨ (°))<<','>°))))彡','<°)))彡','>°))彡','<+','))><<','<*))','>=<','<コ:彡',\n",
    "'Ｃ:.ミ','~>°)～～～','～°·_·°～','(°°)～','●～*','￣|○',':3ミ','^m^','(・・?','(?_?)','>^_^<','<^!^>',\n",
    "'^/^','（*^_^*','§^.^§','(^<^)','(^.^)','(^ム^)','(^·^)','(^.^)','(^_^.)','(^_^)','(^^)','(^J^)',\n",
    "'(*^.^*)','^_^','(#^.^#)','（^—^）','(^^)/~~~','(^_^)/~','(;_;)/~~~','(^.^)/~~~',\n",
    "'(-_-)/~~~','($··)/~~~','(@^^)/~~~','(T_T)/~~~','(V)o￥o(V)','＼(~o~)／','＼(^o^)／','＼(-o-)／','ヽ(^。^)ノ',\n",
    "'ヽ(^o^)丿','(*^0^*)','(*_*)','(*_*;','(+_+) (@_@)','(@_@。','(＠_＠;)',\n",
    " '＼(◎o◎)／！','(*^^)v','(^^)v','(^_^)v','（’-’*) (＾ｖ＾)','(＾▽＾)','(・∀・)','(´∀`)','(⌒▽⌒）',\n",
    " '＼(^o^)／','\\(^o^)/','(~o~)','(~_~)','(^^ゞ','ˊ＿>ˋ','(p_-)', '(-_q)','((d[-_-]b))','(-\"-)','(ーー゛)',\n",
    " '(^_^メ)', '(-_-メ)','(~_~メ)','(－－〆)','(・へ・)','(｀´)'\n",
    " , '<`～´>','<`ヘ´>','(ーー;)','(^0_0^)','(＾ｖ＾)', '(＾ｕ＾)','(＾◇＾)','( ^)o(^ )','(^O^)','(^o^)','(^○^)',')^o^(',\n",
    " '(*^▽^*)','(￣ー￣)','(￣□￣;)', '°o°','°O°',':O','o_O','o_0', 'o.O','(o.o)','oO','(°◇°)','（ ﾟ Дﾟ)','(*￣m￣)',\n",
    " 'ヽ(´ー｀)┌','¯\\_(ツ)_/¯','¯\\(°_o)/¯','(´･ω･`)', '(‘A`)','(づ￣ ³￣)づ','(*^3^)/~☆','(︶｡︶✽)','(-_-) zzz'\n",
    " , 'uwu','UwU','OWO','OwO','～°·_·°～', '(°°)～',':3ミ','(´･ω･`)','(`･ω･´)','(｀-´)>', '（　´_ゝ`）'] \n",
    "\n",
    "puntuaction = ['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '.',\n",
    "           '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', \n",
    "           '`', '{', '|', '}', '~', '»', '«', '“', '”', \"-\", \"...\",\"..\",\".,\",\",.\"]\n",
    "\n",
    "\n",
    "mongoClient = pymongo.MongoClient(\"mongodb://admin:admin@localhost:27017/\")\n",
    "mydb = mongoClient[\"maadb\"]\n",
    "collectionTwitter = mydb[\"Twitter\"] #Questa é per la collezione dei emoji emoticon hashtag per ogni emozione\n",
    "angerCollections = mydb[\"anger\"] #Collezioni per solo i twitt di anger\n",
    "anticipationCollections = mydb[\"anticipation\"]\n",
    "disgustCollections = mydb[\"disgust\"]\n",
    "fearCollections = mydb[\"fear\"]\n",
    "joyCollections = mydb[\"joy\"]\n",
    "sadnessCollections = mydb[\"sadness\"]\n",
    "surpriseCollections = mydb[\"surprise\"]\n",
    "trustCollections = mydb[\"trust\"]\n",
    "resourcesCollection = mydb[\"Resources\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trattamento dataset Anger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lista di righe, ogni riga una tweet anger\n",
    "file = open(\"Dataset/dataset_dt_anger_60k.txt\", 'r',encoding=\"utf8\")\n",
    "anger = []\n",
    "hashtagAnger = []\n",
    "emoticonAnger = []\n",
    "emojiAnger = []\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    #Eliminazione URL e USERNAME\n",
    "    riga = re.sub(r'URL', '',riga)\n",
    "    riga = re.sub(r'USERNAME', '',riga)\n",
    "\n",
    "    #array di hashtag singoli\n",
    "    hashTag = (re.findall('#[a-zA-Z0-9]+', riga))\n",
    "\n",
    "     #generiamo l'array di hashtag senza hashtag\n",
    "    for h in hashTag:\n",
    "        hashtagAnger.append(h.replace(\"#\",\"\"))\n",
    "\n",
    "    #Rimuoviamo glin hashtag dalla riga\n",
    "    riga = riga.replace('#',\"\")\n",
    "\n",
    "    #Salvataggio frase\n",
    "    anger.append(riga)\n",
    "\n",
    "\n",
    "file.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(anger)):\n",
    "    #Controllo se l'emoticons c'è e in caso la rimuovo\n",
    "    for singleEmoticon in emoticons:\n",
    "        emoticonsFind = re.findall(re.escape(singleEmoticon),anger[i])\n",
    "        if(len(emoticonsFind) != 0):\n",
    "            emoticonAnger.extend(emoticonsFind)\n",
    "            anger[i] = anger[i].replace(singleEmoticon,\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(anger)):\n",
    "    #preleviamo le emoji e le rimuoviamo dal testo\n",
    "    data = regex.findall(r'\\X', anger[i])\n",
    "    for word in data:\n",
    "        if any(char in emoji.UNICODE_EMOJI['en'] for char in word):\n",
    "            emojiAnger.append(word)\n",
    "            anger[i] = anger[i].replace(word, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processamento slang \n",
    "with open(\"Slangs/slangs_before.json\") as f:\n",
    "    slangsBefore = json.load(f)\n",
    "    for i in range(len(anger)):\n",
    "        for word in anger[i].split():\n",
    "            indexOfWord = anger[i].find(word)\n",
    "            if slangsBefore.get(word) is None:\n",
    "                if slangsBefore.get(word.lower()) is None:\n",
    "                    if slangsBefore.get(word.upper()) is None:\n",
    "                        continue\n",
    "                    else:\n",
    "                        anger[i] = anger[i][:indexOfWord] + slangsBefore.get(word.upper()) + anger[i][indexOfWord+len(word):]\n",
    "                else:\n",
    "                    anger[i] = anger[i][:indexOfWord] + slangsBefore.get(word.lower()) + anger[i][indexOfWord+len(word):]\n",
    "            else:\n",
    "                anger[i] = anger[i][:indexOfWord] + slangsBefore.get(word) + anger[i][indexOfWord+len(word):]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizzazione di tutte le frasi\n",
    "angerTokenized = []\n",
    "for linea in anger:\n",
    "    result = word_tokenize(linea)\n",
    "    angerTokenized.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Esecuzione della POS tagging per ogni riga della matrice\n",
    "for i in range(len(angerTokenized)):\n",
    "    angerTokenized[i] = nltk.pos_tag(angerTokenized[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trattamento punteggiatura\n",
    "for line in angerTokenized:\n",
    "    newLine = line\n",
    "    listToRemove = []\n",
    "    for i in range(len(line)):\n",
    "        if line[i][0] in puntuaction:\n",
    "            listToRemove.append(line[i])\n",
    "    for element in listToRemove:\n",
    "        newLine.remove(element)\n",
    "    line = newLine\n",
    "\n",
    "linePuntuaction = re.compile(\"[\" + re.escape(\"\".join(puntuaction)) + \"]\")\n",
    "for line in angerTokenized:\n",
    "    for i in range(len(line)):\n",
    "        tupla = [(re.sub(linePuntuaction, \"\", line[i][0])),line[i][1]]\n",
    "        line[i] = tupla\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming\n",
    "#ps = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for line in angerTokenized:\n",
    "    for i in range(len(line)):\n",
    "        tupla = [lemmatizer.lemmatize(line[i][0]),line[i][1]]\n",
    "        line[i] = tupla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trattamnento Stop word dopo Stemming\n",
    "stop_words = set(stopwords.words('english'))\n",
    "angerTokStoped = []\n",
    "for line in angerTokenized:\n",
    "    filtred_word = []\n",
    "    for row in line:\n",
    "        if  row[0] not in stop_words:\n",
    "            filtred_word.append(row) \n",
    "    angerTokStoped.append(filtred_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#liste singole di parole e token\n",
    "angerListWord = []\n",
    "angerListTok = []\n",
    "for line in angerTokStoped:\n",
    "    for row in line:\n",
    "        if row[0] != '':\n",
    "            angerListWord.append(row[0])\n",
    "        angerListTok.append(row[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lavorazione risorse lessicali\n",
    "resourceAngerEmoSN = []\n",
    "resourceAngerNRC = []\n",
    "resourceAngerSentisense = []\n",
    "\n",
    "#Lista di righe, ogni riga una tweet anger\n",
    "file = open(\"Risorse lessicali\\Anger\\EmoSN_anger.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        resourceAngerEmoSN.append(riga)\n",
    "\n",
    "file.close()\n",
    "\n",
    "file = open(\"Risorse lessicali\\\\Anger\\\\NRC_anger.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        resourceAngerNRC.append(riga)\n",
    "\n",
    "file.close()\n",
    "\n",
    "file = open(\"Risorse lessicali\\Anger\\sentisense_anger.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        resourceAngerSentisense.append(riga)\n",
    "\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Caricare le risorse lessicali \n",
    "\n",
    "AngerEmoSN = {\n",
    "    '_id': 'EmoSN_anger',\n",
    "    'sentiment': 'anger', \n",
    "    'wordList': resourceAngerEmoSN\n",
    "}\n",
    "\n",
    "AngerNRC = {\n",
    "    '_id': 'NRC_anger',\n",
    "    'sentiment': 'anger',  \n",
    "    'wordList': resourceAngerNRC\n",
    "}\n",
    "\n",
    "AngerSentisense = {\n",
    "    '_id': 'sentisense_anger',\n",
    "    'sentiment': 'anger',\n",
    "    'wordList': resourceAngerSentisense\n",
    "}\n",
    "\n",
    "resourcesCollection.insert_one(AngerEmoSN)\n",
    "resourcesCollection.insert_one(AngerNRC)\n",
    "resourcesCollection.insert_one(AngerSentisense)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conteggio delle frequenze delle parole\n",
    "countWordAnger = Counter(angerListWord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione database e collezioni\n",
    "# Inserimento singoli documenti contententi i tweet anger\n",
    "wordsString = {}\n",
    "\n",
    "for i in range(len(angerListWord)):\n",
    "    wordsString = {\n",
    "        \"lemma\": angerListWord[i],\n",
    "        \"POS\": angerListTok[i]\n",
    "    }\n",
    "    angerTwit={\"word\":wordsString}\n",
    "    angerCollections.insert_one(angerTwit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inserimento collezione Twitter con hashtags, emoticons ed emoji \n",
    "angerInsert = {\n",
    "    \"sentiment\":\"anger\",\n",
    "    \"hashtags\": hashtagAnger,\n",
    "    \"emoticons\": emoticonAnger,\n",
    "    \"emoji\": emojiAnger\n",
    "}\n",
    "\n",
    "#inserimento\n",
    "x = collectionTwitter.insert_one(angerInsert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raggruppa i documenti di input in base all'espressione _id specificata e per ciascun raggruppamento distinto genera un documento\n",
    "pipeline = [\n",
    "    {\"$group\": {\"_id\": \"$word.lemma\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"word\": \"$_id\",\n",
    "      \"freq\": \"$count\"\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"freq\": -1\n",
    "    }},\n",
    "    {\"$out\": \"anger\"}\n",
    "]\n",
    "\n",
    "#post processing per renderlo valido \n",
    "angerCollections.aggregate(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creazione Word Clouds per le parole\n",
    "data = {}\n",
    "for document in angerCollections.find().limit(50):\n",
    "    data[document['word']] = document['freq']\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2)\n",
    "wc.fit_words(data)\n",
    "wc.to_file('Word Clouds/angerWords.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conteggio degli hashtags\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$hashtags\"},\n",
    "    {\"$group\": {\"_id\": \"$hashtags\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"word\": \"$_id\",\n",
    "      \"count\": 1\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"count\": -1\n",
    "    }}\n",
    "]\n",
    "\n",
    "hashtagsCount = collectionTwitter.aggregate(pipeline)\n",
    "\n",
    "data = {}\n",
    "for document in hashtagsCount:\n",
    "    data[document['word']] = document['count']\n",
    "\n",
    "collectionTwitter.update_one(\n",
    "  {\"sentiment\": \"anger\"},\n",
    "  {\"$set\": {\"hashtags\": data}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creazione Word Clouds per gli hashtags\n",
    "\n",
    "hashtags = collectionTwitter.distinct(\"hashtags\")\n",
    "\n",
    "hashtagsLimited = {}\n",
    "for (word,count) in [x for x in hashtags[0].items()][:50]:\n",
    "    hashtagsLimited[word] = count\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2)\n",
    "wc.fit_words(hashtagsLimited)\n",
    "wc.to_file('Word Clouds/angerHashtags.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conteggio delle emoji\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$emoji\"},\n",
    "    {\"$group\": {\"_id\": \"$emoji\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"emoji\": \"$_id\",\n",
    "      \"count\": 1\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"count\": -1\n",
    "    }}\n",
    "]\n",
    "\n",
    "emojiCount = collectionTwitter.aggregate(pipeline)\n",
    "\n",
    "data = {}\n",
    "for document in emojiCount:\n",
    "    data[document['emoji']] = document['count']\n",
    "\n",
    "collectionTwitter.update_one(\n",
    "  {\"sentiment\": \"anger\"},\n",
    "  {\"$set\": {\"emoji\": data}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creazione Word Clouds per le emoji\n",
    "\n",
    "emojis = collectionTwitter.distinct(\"emoji\")\n",
    "\n",
    "emojisLimited = {}\n",
    "for (word,count) in [x for x in emojis[0].items()][:50]:\n",
    "    emojisLimited[word] = count\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2, font_path='Font Emoji\\Symbola.otf')\n",
    "wc.fit_words(emojisLimited)\n",
    "wc.to_file('Word Clouds/angerEmojis.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conteggio degli emoticons\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$emoticons\"},\n",
    "    {\"$group\": {\"_id\": \"$emoticons\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"emoticons\": \"$_id\",\n",
    "      \"count\": 1\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"count\": -1\n",
    "    }}\n",
    "]\n",
    "\n",
    "emoticonsCount = collectionTwitter.aggregate(pipeline)\n",
    "\n",
    "data = {}\n",
    "for document in emoticonsCount:\n",
    "    data[document['emoticons']] = document['count']\n",
    "\n",
    "collectionTwitter.update_one(\n",
    "  {\"sentiment\": \"anger\"},\n",
    "  {\"$set\": {\"emoticons\": data}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creazione Word Clouds per le emoticons\n",
    "\n",
    "emoticons = collectionTwitter.distinct(\"emoticons\")\n",
    "\n",
    "emoticonsLimited = {}\n",
    "for (word,count) in [x for x in emoticons[0].items()][:50]:\n",
    "    emoticonsLimited[word] = count\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2)\n",
    "wc.fit_words(emoticonsLimited)\n",
    "wc.to_file('Word Clouds/angerEmoticons.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline di aggregazione per inserire la risorsa lessicale a cui appartiene la parola\n",
    "\n",
    "pipeline = [\n",
    "    {\"$lookup\": {\n",
    "        \"from\": \"Resources\",\n",
    "        \"localField\": \"word\",\n",
    "        \"foreignField\": \"wordList\",\n",
    "        \"as\": \"LexResources\"\n",
    "    }},\n",
    "    {\"$project\": {\n",
    "        \"resources.wordList\": 0,\n",
    "        \"resources.totNumberWords\": 0,\n",
    "        \"resources.sentiment\": 0,\n",
    "        \"LexResources.sentiment\": 0,\n",
    "        \"LexResources.totNumberWords\": 0,\n",
    "        \"LexResources.wordList\": 0\n",
    "    }},\n",
    "    {\"$out\": \"anger\"}\n",
    "]\n",
    "\n",
    "angerCollections.aggregate(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline di aggregazione per calcolare le percentuali richieste\n",
    "\n",
    "pipelineCount = [\n",
    "    {\"$group\": {\"_id\": \"null\", \"count\": { \"$sum\": \"$freq\" }}},\n",
    "    {\"$project\": {\n",
    "        \"_id\": 0,\n",
    "        \"count\": \"$count\"\n",
    "    }}\n",
    "]\n",
    "\n",
    "results = angerCollections.aggregate(pipelineCount)\n",
    "\n",
    "countWords = 0\n",
    "for data in results:\n",
    "    countWords = data['count']\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$LexResources\"},\n",
    "    {\"$lookup\": {\n",
    "        \"from\": \"Resources\",\n",
    "        \"localField\": \"LexResources._id\",\n",
    "        \"foreignField\": \"_id\",\n",
    "        \"as\": \"RefResource\"\n",
    "    }},\n",
    "    {\"$group\": {\"_id\": { \"lex\": \"$LexResources._id\", \"RefResource\": \"$RefResource\" }, \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "        \"_id\": \"$_id.lex\",\n",
    "        \"wordList\": \"$_id.RefResource.wordList\",\n",
    "        \"commonWords\": \"$count\"\n",
    "    }},\n",
    "    {\"$unwind\": \"$wordList\"},\n",
    "    {\"$project\": {\n",
    "        \"_id\": 1,\n",
    "        \"commonWords\": 1,\n",
    "        \"totWordsLex\": {\"$size\": \"$wordList\"}\n",
    "    }},\n",
    "    {\"$addFields\": {\"totalWordsTwitter\": countWords}},\n",
    "    {\"$project\": {\n",
    "        \"perc_presence_lex_res\": { \"$round\": [{\"$divide\": [ \"$commonWords\", \"$totWordsLex\" ] }, 6] },\n",
    "        \"perc_presence_twitter\": { \"$round\": [{\"$divide\": [ \"$commonWords\", \"$totalWordsTwitter\" ]}, 6] }\n",
    "    }}\n",
    "]\n",
    "\n",
    "results = angerCollections.aggregate(pipeline)\n",
    "\n",
    "# Genero i dati per l'istogramma\n",
    "istogramAnger_lexRes = []\n",
    "istogramAnger_twitter = []\n",
    "\n",
    "for data in results:\n",
    "    print(data)\n",
    "    log =math.log(data['perc_presence_lex_res'],10)\n",
    "    istogramAnger_lexRes.append([data['_id'], data['perc_presence_lex_res']])\n",
    "    log =math.log(data['perc_presence_twitter'],10)\n",
    "    istogramAnger_twitter.append([data['_id'], data['perc_presence_twitter']])\n",
    " \n",
    "# Create the pandas DataFrame\n",
    "dfAnger_lexRes = pd.DataFrame(istogramAnger_lexRes, columns = ['Risorsa', 'Percentuale'])\n",
    "dfAnger_twitter = pd.DataFrame(istogramAnger_twitter,columns = ['Risorsa', 'Percentuale'])      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione istogramma per presenza parole nelle risorse lessicali\n",
    "\n",
    "dfAnger_lexRes.plot.bar(x='Risorsa')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione istogramma per presenza parole nei twitter\n",
    "\n",
    "dfAnger_twitter.plot.bar(x='Risorsa')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connessione, creazione cursore e database\n",
    "\n",
    "conn = psycopg2.connect(database = \"maadb\", user = \"postgres\", password = \"admin\", host = \"localhost\", port = \"5432\")\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione tabella risorse lessicali anger e inserimento\n",
    "cur.execute('''CREATE TABLE RISORSEANGER\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    NOME             TEXT       NOT NULL,\n",
    "                    PAROLA           TEXT       NOT NULL\n",
    "                );\n",
    "            '''\n",
    ")\n",
    "nome = \"EmoSN_anger\"\n",
    "for word in resourceAngerEmoSN:\n",
    "    cur.execute(\"INSERT INTO RISORSEANGER (NOME, PAROLA) VALUES(%s, %s)\",(nome,word))\n",
    "\n",
    "nome = \"NRC_anger\"\n",
    "for word in resourceAngerNRC:\n",
    "    cur.execute(\"INSERT INTO RISORSEANGER (NOME, PAROLA) VALUES(%s, %s)\",(nome,word))\n",
    "\n",
    "nome = \"sentisense_anger\"\n",
    "for word in resourceAngerSentisense:\n",
    "    cur.execute(\"INSERT INTO RISORSEANGER (NOME, PAROLA) VALUES(%s, %s)\",(nome,word))\n",
    "\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione tabelle per le parole, emoji, emoticon e hashtag\n",
    "\n",
    "cur.execute('''CREATE TABLE PAROLEANGER\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    PAROLA           TEXT       NOT NULL, \n",
    "                    EmoSN_anger      BOOL       NOT NULL DEFAULT FALSE,\n",
    "                    NRC_anger      BOOL       NOT NULL DEFAULT FALSE,\n",
    "                    sentisense_anger      BOOL       NOT NULL DEFAULT FALSE,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ")   \n",
    "\n",
    "cur.execute('''CREATE TABLE EMOJIANGER\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    EMOJI       TEXT       NOT NULL,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ") \n",
    "\n",
    "cur.execute('''CREATE TABLE EMOTICONSANGER\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    EMOTICONS       TEXT       NOT NULL,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ") \n",
    "\n",
    "cur.execute('''CREATE TABLE HASHTAGANGER\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    HASHTAG     TEXT       NOT NULL,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ") \n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inserimento hashtag, emoticon ed emoji\n",
    "\n",
    "for hashTag in hashtagAnger:\n",
    "    cur.execute(\"INSERT INTO HASHTAGANGER (HASHTAG,FREQUENCY) VALUES(%s,%s)\",(hashTag,0))\n",
    "\n",
    "for emoticons in emoticonAnger:\n",
    "    cur.execute(\"INSERT INTO EMOTICONSANGER (EMOTICONS,FREQUENCY) VALUES(%s,%s)\",(emoticons,0))\n",
    "\n",
    "for emojis in emojiAnger:\n",
    "    cur.execute(\"INSERT INTO EMOJIANGER (EMOJI,FREQUENCY) VALUES(%s,%s)\",(emojis,0))\n",
    "\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inserimento parole in db\n",
    "for word in angerListWord:\n",
    "    cur.execute(\"INSERT INTO PAROLEANGER (PAROLA,EmoSN_anger,NRC_anger,sentisense_anger,FREQUENCY) VALUES(%s,%s,%s,%s,%s)\",(word,False,False,False,0))\n",
    "\n",
    "conn.commit()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze hashtag\n",
    "cur.execute(\"update hashtaganger as ha1 set frequency = (select count(*) from hashtaganger ha2 where ha1.hashtag = ha2.hashtag);\")\n",
    "cur.execute(\"delete from hashtaganger as ha1 where exists (Select 1 From hashtaganger as ha2 where ha1.hashtag = ha2.hashtag and ha1.frequency = ha2.frequency and ha2.ctid > ha1.ctid);\")\n",
    "\n",
    "conn.commit()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze emoji\n",
    "cur.execute(\"update EMOJIANGER as ha1 set frequency = (select count(*) from EMOJIANGER ha2 where ha1.emoji = ha2.emoji);\")\n",
    "cur.execute(\"delete from EMOJIANGER as ha1 where exists (Select 1 From EMOJIANGER as ha2 where ha1.emoji = ha2.emoji and ha1.frequency = ha2.frequency and ha2.ctid > ha1.ctid);\")\n",
    "\n",
    "conn.commit()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze emoticons\n",
    "cur.execute(\"update EMOTICONSANGER as ha1 set frequency = (select count(*) from EMOTICONSANGER ha2 where ha1.emoticons = ha2.emoticons);\")\n",
    "cur.execute(\"delete from EMOTICONSANGER as ha1 where exists (Select 1 From EMOTICONSANGER as ha2 where ha1.emoticons = ha2.emoticons and ha1.frequency = ha2.frequency and ha2.ctid > ha1.ctid);\")\n",
    "\n",
    "conn.commit()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze parole (ci mette più di 100 minuti)\n",
    "\"\"\"\n",
    "cur.execute(\"update PAROLEANGER as ha1 set frequency = (select count(*) from PAROLEANGER ha2 where ha1.parola = ha2.parola);\")\n",
    "cur.execute(\"delete from PAROLEANGER as ha1 where exists (Select 1 From PAROLEANGER as ha2 where ha1.parola = ha2.parola and ha1.frequency = ha2.frequency and ha2.ctid > ha1.ctid);\")\n",
    "\n",
    "conn.commit()  \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze parole\n",
    "cur.execute(\"select count(*),parola from paroleanger group by parola\")\n",
    "result = []\n",
    "for word in cur.fetchall():\n",
    "    result.append(word) \n",
    "\n",
    "cur.execute(\"truncate table paroleanger;\")\n",
    "cur.execute(\"ALTER SEQUENCE paroleanger_id_seq RESTART WITH 1;\")\n",
    "\n",
    "for couple in result:\n",
    "    cur.execute(\"INSERT INTO PAROLEANGER (PAROLA,EmoSN_anger,NRC_anger,sentisense_anger,FREQUENCY) VALUES(%s,%s,%s,%s,%s)\",(couple[1],False,False,False,couple[0]))\n",
    "\n",
    "conn.commit()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggiornamento presenza parola nelle risorse lessicali\n",
    "cur.execute(\"UPDATE paroleanger SET EmoSN_anger = TRUE WHERE parola IN (select p.parola from paroleanger as p LEFT OUTER JOIN risorseanger as r ON p.parola = r.parola WHERE r.nome = 'EmoSN_anger')\")\n",
    "cur.execute(\"UPDATE paroleanger SET NRC_ANGER = TRUE WHERE parola IN (select p.parola from paroleanger as p LEFT OUTER JOIN risorseanger as r ON p.parola = r.parola WHERE r.nome = 'NRC_anger')\")\n",
    "cur.execute(\"UPDATE paroleanger SET sentisense_anger = TRUE WHERE parola IN (select p.parola from paroleanger as p LEFT OUTER JOIN risorseanger as r ON p.parola = r.parola WHERE r.nome = 'sentisense_anger')\")\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvataggio in un txt delle parole non presenti in nessuna risorsa lessicale\n",
    "\n",
    "wordsMongo = angerCollections.find({\"LexResources\": []}, {\"_id\": 0, \"freq\": 0, \"LexResources\": 0})\n",
    "words = []\n",
    "\n",
    "for word in wordsMongo:\n",
    "    words.append(word['word'])\n",
    "\n",
    "with open('Nuove Parole/anger.txt', 'w',encoding=\"utf8\") as f:\n",
    "    for word in words:\n",
    "        f.write(\"%s\\n\" % word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trattamento dataset Anticipation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lista di righe, ogni riga una tweet anticipation\n",
    "file = open(\"Dataset/dataset_dt_anticipation_60k.txt\", 'r',encoding=\"utf8\")\n",
    "anticipation = []\n",
    "hashtagAnticipation = []\n",
    "emoticonAnticipation = []\n",
    "emojiAnticipation = []\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "    riga = re.sub(r'URL', '',riga)\n",
    "    riga = re.sub(r'USERNAME', '',riga)\n",
    "\n",
    "    #array di hashtag singoli\n",
    "    hashTag = (re.findall('#[a-zA-Z]+', riga))\n",
    "\n",
    "    #generiamo l'array di hashtag senza hashtag\n",
    "    for h in hashTag:\n",
    "        hashtagAnticipation.append(h.replace(\"#\",\"\"))\n",
    "\n",
    "    #Rimuoviamo glin hashtag dalla riga\n",
    "    riga = riga.replace('#',\"\")\n",
    "    \n",
    "    anticipation.append(riga)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(anticipation)):\n",
    "    #Controllo se l'emoticons c'è e in caso la rimuovo\n",
    "    for singleEmoticon in emoticons:\n",
    "        emoticonsFind = re.findall(re.escape(singleEmoticon),anticipation[i])\n",
    "        if(len(emoticonsFind) != 0):\n",
    "            emoticonAnticipation.extend(emoticonsFind)\n",
    "            anticipation[i] = anticipation[i].replace(singleEmoticon,\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(anticipation)):\n",
    "    #preleviamo le emoji e le rimuoviamo dal testo\n",
    "    data = regex.findall(r'\\X', anticipation[i])\n",
    "    for word in data:\n",
    "        if any(char in emoji.UNICODE_EMOJI['en'] for char in word):\n",
    "            emojiAnticipation.append(word)\n",
    "            anticipation[i] = anticipation[i].replace(word, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processamento slang prima di punteggiatura e lowercase\n",
    "with open(\"Slangs/slangs_before.json\") as f:\n",
    "    slangsBefore = json.load(f)\n",
    "    for i in range(len(anticipation)):\n",
    "        for word in anticipation[i].split():\n",
    "            indexOfWord = anticipation[i].find(word)\n",
    "            if slangsBefore.get(word) is None:\n",
    "                if slangsBefore.get(word.lower()) is None:\n",
    "                    if slangsBefore.get(word.upper()) is None:\n",
    "                        continue\n",
    "                    else:\n",
    "                        anticipation[i] = anticipation[i][:indexOfWord] + slangsBefore.get(word.upper()) + anticipation[i][indexOfWord+len(word):]\n",
    "                else:\n",
    "                    anticipation[i] = anticipation[i][:indexOfWord] + slangsBefore.get(word.lower()) + anticipation[i][indexOfWord+len(word):]\n",
    "            else:\n",
    "                anticipation[i] = anticipation[i][:indexOfWord] + slangsBefore.get(word) + anticipation[i][indexOfWord+len(word):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizzazione di tutte le frasi\n",
    "anticipationTokenized = []\n",
    "for linea in anticipation:\n",
    "    result = word_tokenize(linea)\n",
    "    anticipationTokenized.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Esecuzione della POS tagging per ogni riga della matrice\n",
    "for i in range(len(anticipationTokenized)):\n",
    "    anticipationTokenized[i] = nltk.pos_tag(anticipationTokenized[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trattamento punteggiatura\n",
    "for line in anticipationTokenized:\n",
    "    newLine = line\n",
    "    listToRemove = []\n",
    "    for i in range(len(line)):\n",
    "        if line[i][0] in puntuaction:\n",
    "            listToRemove.append(line[i])\n",
    "    for element in listToRemove:\n",
    "        newLine.remove(element)\n",
    "    line = newLine\n",
    "\n",
    "linePuntuaction = re.compile(\"[\" + re.escape(\"\".join(puntuaction)) + \"]\")\n",
    "for line in anticipationTokenized:\n",
    "    for i in range(len(line)):\n",
    "        tupla = [(re.sub(linePuntuaction, \"\", line[i][0])),line[i][1]]\n",
    "        line[i] = tupla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming\n",
    "#ps = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for line in anticipationTokenized:\n",
    "    for i in range(len(line)):\n",
    "        tupla = [lemmatizer.lemmatize(line[i][0]),line[i][1]]\n",
    "        line[i] = tupla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trattamnento Stop word dopo Stemming\n",
    "stop_words = set(stopwords.words('english'))\n",
    "anticipationTokStoped = []\n",
    "for line in anticipationTokenized:\n",
    "    filtred_word = []\n",
    "    for row in line:\n",
    "        if  row[0] not in stop_words:\n",
    "            filtred_word.append(row) \n",
    "    anticipationTokStoped.append(filtred_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lavorazione risorse lessicali\n",
    "resourceNRC_anticipation = []\n",
    "resourceSentisense_anticipation = []\n",
    "#Lista di righe, ogni riga una tweet anger\n",
    "file = open(\"Risorse lessicali\\\\Anticipation\\\\NRC_anticipation.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "            resourceNRC_anticipation.append(riga)\n",
    "\n",
    "file.close()\n",
    "\n",
    "file = open(\"Risorse lessicali\\Anticipation\\sentisense_anticipation.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "            resourceSentisense_anticipation.append(riga)\n",
    "\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.InsertOneResult at 0x1ffdace8400>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Caricare le risorse lessicali \n",
    "\n",
    "\n",
    "AnticipationNRC = {\n",
    "    '_id': 'NRC_anticipation',\n",
    "    'sentiment': 'anticipation',  \n",
    "    'wordList': resourceNRC_anticipation\n",
    "}\n",
    "\n",
    "AnticipationSentisense = {\n",
    "    '_id': 'sentisense_anticipation',\n",
    "    'sentiment': 'anticipation',\n",
    "    'wordList': resourceSentisense_anticipation\n",
    "}\n",
    "\n",
    "resourcesCollection.insert_one(AnticipationNRC)\n",
    "resourcesCollection.insert_one(AnticipationSentisense)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "anticipationListWord = []\n",
    "anticipationListTok = []\n",
    "for line in anticipationTokStoped:\n",
    "    for row in line:\n",
    "        if row[0] != '':\n",
    "            anticipationListWord.append(row[0])\n",
    "        anticipationListTok.append(row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conteggio delle frequenze delle parole\n",
    "countWordAnticipation = Counter(anticipationListWord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione database e collezioni\n",
    "# Inserimento singoli documenti contententi i tweet anger\n",
    "wordsString = {}\n",
    "\n",
    "for i in range(len(anticipationListWord)):\n",
    "    wordsString = {\n",
    "        \"lemma\": anticipationListWord[i],\n",
    "        \"POS\": anticipationListTok[i]\n",
    "    }\n",
    "    anticipationTwit={\"word\":wordsString}\n",
    "    anticipationCollections.insert_one(anticipationTwit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inserimento Documento\n",
    "anticipationInsert = {\n",
    "    \"sentiment\":\"anticipation\",\n",
    "    \"hashtags\": hashtagAnticipation,\n",
    "    \"emoticons\": emoticonAnticipation,\n",
    "    \"emoji\": emojiAnticipation\n",
    "}\n",
    "\n",
    "#inserimento\n",
    "x = collectionTwitter.insert_one(anticipationInsert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': 'NRC_anticipation', 'perc_presence_lex_res': 0.500596, 'perc_presence_twitter': 0.000767}\n",
      "{'_id': 'sentisense_anticipation', 'perc_presence_lex_res': 0.668966, 'perc_presence_twitter': 0.000177}\n"
     ]
    }
   ],
   "source": [
    "# match --> { $match: { <query> } } se si soddisfa una query\n",
    "# project --> { $project: { <specification(s)> } } prende un documento che può specificare l'inclusione dei campi, \n",
    "# la soppressione del campo _id, l'aggiunta di nuovi campi e il ripristino dei valori dei campi esistenti\n",
    "# addfield --> { $addFields: { <newField>: <expression>, ... } } Specificare il nome di ogni campo da aggiungere e impostarne il valore su un'espressione di aggregazione\n",
    "# lookup --> {$lookup: {from , localField, foreignkey, as   } } Per eseguire una corrispondenza di uguaglianza tra un campo dei documenti di input\n",
    "# group --> {$groud: {_id:<expression> , <field> : {<accumulator1> : <expression1>}} } \n",
    "#   Raggruppa i documenti di input in base all'espressione _id specificata e per ciascun raggruppamento distinto genera un documento\n",
    "\n",
    "pipeline = [\n",
    "    {\"$group\": {\"_id\": \"$word.lemma\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"word\": \"$_id\",\n",
    "      \"freq\": \"$count\"\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"freq\": -1\n",
    "    }},\n",
    "    {\"$out\": \"anticipation\"}\n",
    "]\n",
    "\n",
    "\n",
    "wordCount = anticipationCollections.aggregate(pipeline)\n",
    "\"\"\"\n",
    "#Creazione Word Clouds per le parole\n",
    "data = {}\n",
    "for document in anticipationCollections.find().limit(50):\n",
    "    data[document['word']] = document['freq']\n",
    "\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2)\n",
    "wc.fit_words(data)\n",
    "wc.to_file('Word Clouds/anticipationWords.png')\n",
    "\"\"\"\n",
    "# Conteggio degli hashtags\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$hashtags\"},\n",
    "    {\"$group\": {\"_id\": \"$hashtags\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"word\": \"$_id\",\n",
    "      \"count\": 1\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"count\": -1\n",
    "    }}\n",
    "]\n",
    "\n",
    "hashtagsCount = collectionTwitter.aggregate(pipeline)\n",
    "\n",
    "data = {}\n",
    "for document in hashtagsCount:\n",
    "    data[document['word']] = document['count']\n",
    "\n",
    "collectionTwitter.update_one(\n",
    "  {\"sentiment\": \"anticipation\"},\n",
    "  {\"$set\": {\"hashtags\": data}}\n",
    ")\n",
    "\n",
    "#Creazione Word Clouds per gli hashtag\n",
    "\"\"\"\n",
    "hashtags = collectionTwitter.distinct(\"hashtags\")\n",
    "\n",
    "hashtagsLimited = {}\n",
    "for (word,count) in [x for x in hashtags[0].items()][:50]:\n",
    "    hashtagsLimited[word] = count\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2)\n",
    "wc.fit_words(hashtagsLimited)\n",
    "wc.to_file('Word Clouds/anticipationHashtags.png')\n",
    "\"\"\"\n",
    "# Conteggio delle emoji\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$emoji\"},\n",
    "    {\"$group\": {\"_id\": \"$emoji\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"emoji\": \"$_id\",\n",
    "      \"count\": 1\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"count\": -1\n",
    "    }}\n",
    "]\n",
    "\n",
    "emojiCount = collectionTwitter.aggregate(pipeline)\n",
    "\n",
    "data = {}\n",
    "for document in emojiCount:\n",
    "    data[document['emoji']] = document['count']\n",
    "\n",
    "collectionTwitter.update_one(\n",
    "  {\"sentiment\": \"anticipation\"},\n",
    "  {\"$set\": {\"emoji\": data}}\n",
    ")\n",
    "\n",
    "#Creazione Word Clouds per le emoji\n",
    "\"\"\"\n",
    "emojis = collectionTwitter.distinct(\"emoji\")\n",
    "\n",
    "emojisLimited = {}\n",
    "for (word,count) in [x for x in emojis[0].items()][:50]:\n",
    "    emojisLimited[word] = count\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2)\n",
    "wc.fit_words(emojisLimited)\n",
    "wc.to_file('Word Clouds/anticipationEmojis.png')\n",
    "\"\"\"\n",
    "# Conteggio degli emoticons\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$emoticons\"},\n",
    "    {\"$group\": {\"_id\": \"$emoticons\", \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "      \"_id\": 0,\n",
    "      \"emoticons\": \"$_id\",\n",
    "      \"count\": 1\n",
    "    }\n",
    "    },\n",
    "    {\"$sort\": {\n",
    "      \"count\": -1\n",
    "    }}\n",
    "]\n",
    "\n",
    "emoticonsCount = collectionTwitter.aggregate(pipeline)\n",
    "\n",
    "data = {}\n",
    "for document in emoticonsCount:\n",
    "    data[document['emoticons']] = document['count']\n",
    "\n",
    "collectionTwitter.update_one(\n",
    "  {\"sentiment\": \"anticipation\"},\n",
    "  {\"$set\": {\"emoticons\": data}}\n",
    ")\n",
    "\"\"\"\n",
    "#Creazione Word Clouds per le emoticons\n",
    "emoticons = collectionTwitter.distinct(\"emoticons\")\n",
    "\n",
    "emoticonsLimited = {}\n",
    "for (word,count) in [x for x in emoticons[0].items()][:50]:\n",
    "    emoticonsLimited[word] = count\n",
    "\n",
    "wc = WordCloud(background_color='white', width = 1920, height=1080, margin=2)\n",
    "wc.fit_words(emoticonsLimited)\n",
    "wc.to_file('Word Clouds/anticipationEmoticons.png')\n",
    "\"\"\"\n",
    "#Pipeline di aggregazione per inserire la risorsa lessicale a cui appartiene la parola\n",
    "pipeline = [\n",
    "    {\"$lookup\": {\n",
    "        \"from\": \"Resources\",\n",
    "        \"localField\": \"word\",\n",
    "        \"foreignField\": \"wordList\",\n",
    "        \"as\": \"LexResources\"\n",
    "    }},\n",
    "    {\"$project\": {\n",
    "        \"resources.wordList\": 0,\n",
    "        \"resources.totNumberWords\": 0,\n",
    "        \"resources.sentiment\": 0,\n",
    "        \"LexResources.sentiment\": 0,\n",
    "        \"LexResources.totNumberWords\": 0,\n",
    "        \"LexResources.wordList\": 0\n",
    "    }},\n",
    "    {\"$out\": \"anticipation\"}\n",
    "]\n",
    "\n",
    "anticipationCollections.aggregate(pipeline)\n",
    "\n",
    "#Pipeline di aggregazione per calcolare le percentuali richieste\n",
    "\n",
    "pipelineCount = [\n",
    "    {\"$group\": {\"_id\": \"null\", \"count\": { \"$sum\": \"$freq\" }}},\n",
    "    {\"$project\": {\n",
    "        \"_id\": 0,\n",
    "        \"count\": \"$count\"\n",
    "    }}\n",
    "]\n",
    "\n",
    "results = anticipationCollections.aggregate(pipelineCount)\n",
    "\n",
    "countWords = 0\n",
    "for data in results:\n",
    "    countWords = data['count']\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$LexResources\"},\n",
    "    {\"$lookup\": {\n",
    "        \"from\": \"Resources\",\n",
    "        \"localField\": \"LexResources._id\",\n",
    "        \"foreignField\": \"_id\",\n",
    "        \"as\": \"RefResource\"\n",
    "    }},\n",
    "    {\"$group\": {\"_id\": { \"lex\": \"$LexResources._id\", \"RefResource\": \"$RefResource\" }, \"count\": { \"$sum\": 1 }}},\n",
    "    {\"$project\": {\n",
    "        \"_id\": \"$_id.lex\",\n",
    "        \"wordList\": \"$_id.RefResource.wordList\",\n",
    "        \"commonWords\": \"$count\"\n",
    "    }},\n",
    "    {\"$unwind\": \"$wordList\"},\n",
    "    {\"$project\": {\n",
    "        \"_id\": 1,\n",
    "        \"commonWords\": 1,\n",
    "        \"totWordsLex\": {\"$size\": \"$wordList\"}\n",
    "    }},\n",
    "    {\"$addFields\": {\"totalWordsTwitter\": countWords}},\n",
    "    {\"$project\": {\n",
    "        \"perc_presence_lex_res\": { \"$round\": [{\"$divide\": [ \"$commonWords\", \"$totWordsLex\" ] }, 6] },\n",
    "        \"perc_presence_twitter\": { \"$round\": [{\"$divide\": [ \"$commonWords\", \"$totalWordsTwitter\" ]}, 6] }\n",
    "    }}\n",
    "]\n",
    "\n",
    "results = anticipationCollections.aggregate(pipeline)\n",
    "\n",
    "# Genero i dati per l'istogramma\n",
    "istogramAnticipation_lexRes = []\n",
    "istogramAnticipation_twitter = []\n",
    "\n",
    "for data in results:\n",
    "    print(data)\n",
    "    log =math.log(data['perc_presence_lex_res'],10)\n",
    "    istogramAnticipation_lexRes.append([data['_id'], data['perc_presence_lex_res']])\n",
    "    log =math.log(data['perc_presence_twitter'],10)\n",
    "    istogramAnticipation_twitter.append([data['_id'], data['perc_presence_twitter']])\n",
    " \n",
    "# Create the pandas DataFrame\n",
    "dfAnticipation_lexRes = pd.DataFrame(istogramAnticipation_lexRes, columns = ['Risorsa', 'Percentuale'])\n",
    "dfAnticipation_twitter = pd.DataFrame(istogramAnticipation_twitter,columns = ['Risorsa', 'Percentuale'])      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Creazione istogramma per presenza parole nei twitter\n",
    "\n",
    "dfAnticipation_lexRes.plot.bar(x='Risorsa')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Creazione istogramma per presenza parole nei twitter\n",
    "\n",
    "dfAnticipation_twitter.plot.bar(x='Risorsa')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connessione, creazione cursore e database\n",
    "conn = psycopg2.connect(database = \"maadb\", user = \"postgres\", password = \"admin\", host = \"localhost\", port = \"5432\")\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione tabella risorse lessicali anticipation e inserimento\n",
    "cur.execute('''CREATE TABLE RISORSEANTICIPATION\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    NOME             TEXT       NOT NULL,\n",
    "                    PAROLA           TEXT       NOT NULL\n",
    "                );\n",
    "            '''\n",
    ")\n",
    "nome = \"NRC_anticipation\"\n",
    "for word in resourceNRC_anticipation:\n",
    "    cur.execute(\"INSERT INTO RISORSEANTICIPATION (NOME, PAROLA) VALUES(%s, %s)\",(nome,word))\n",
    "\n",
    "nome = \"sentisense_anticipation\"\n",
    "for word in resourceSentisense_anticipation:\n",
    "    cur.execute(\"INSERT INTO RISORSEANTICIPATION (NOME, PAROLA) VALUES(%s, %s)\",(nome,word))\n",
    "\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione tabelle per le parole, emoji, emoticon e hashtag\n",
    "\n",
    "cur.execute('''CREATE TABLE PAROLEANTICIPATION\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    PAROLA           TEXT       NOT NULL, \n",
    "                    NRC_anticipation      BOOL       NOT NULL DEFAULT FALSE,\n",
    "                    sentisense_anticipation      BOOL       NOT NULL DEFAULT FALSE,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ")   \n",
    "\n",
    "cur.execute('''CREATE TABLE EMOJIANTICIPATION\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    EMOJI       TEXT       NOT NULL,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ") \n",
    "\n",
    "cur.execute('''CREATE TABLE EMOTICONSANTICIPATION\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    EMOTICONS       TEXT       NOT NULL,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ") \n",
    "\n",
    "cur.execute('''CREATE TABLE HASHTAGANTICIPATION\n",
    "                (   ID SERIAL NOT NULL PRIMARY KEY,\n",
    "                    HASHTAG     TEXT       NOT NULL,\n",
    "                    FREQUENCY       INT     NOT NULL DEFAULT 0\n",
    "                );\n",
    "            '''\n",
    ") \n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inserimento hashtag, emoticon ed emoji\n",
    "\n",
    "for hashTag in hashtagAnticipation:\n",
    "    cur.execute(\"INSERT INTO HASHTAGANTICIPATION (HASHTAG,FREQUENCY) VALUES(%s,%s)\",(hashTag,0))\n",
    "\n",
    "for emoticons in emoticonAnticipation:\n",
    "    cur.execute(\"INSERT INTO EMOTICONSANTICIPATION (EMOTICONS,FREQUENCY) VALUES(%s,%s)\",(emoticons,0))\n",
    "\n",
    "for emojis in emojiAnticipation:\n",
    "    cur.execute(\"INSERT INTO EMOJIANTICIPATION (EMOJI,FREQUENCY) VALUES(%s,%s)\",(emojis,0))\n",
    "\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inserimento parole in db\n",
    "for word in anticipationListWord:\n",
    "    cur.execute(\"INSERT INTO PAROLEANTICIPATION (PAROLA,NRC_anticipation,sentisense_anticipation,FREQUENCY) VALUES(%s,%s,%s,%s)\",(word,False,False,0))\n",
    "\n",
    "conn.commit()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze hashtag\n",
    "cur.execute(\"update HASHTAGANTICIPATION as ha1 set frequency = (select count(*) from HASHTAGANTICIPATION ha2 where ha1.hashtag = ha2.hashtag);\")\n",
    "cur.execute(\"delete from HASHTAGANTICIPATION as ha1 where exists (Select 1 From HASHTAGANTICIPATION as ha2 where ha1.hashtag = ha2.hashtag and ha1.frequency = ha2.frequency and ha2.ctid > ha1.ctid);\")\n",
    "\n",
    "conn.commit()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze emoji\n",
    "cur.execute(\"update EMOJIANTICIPATION as ha1 set frequency = (select count(*) from EMOJIANTICIPATION ha2 where ha1.emoji = ha2.emoji);\")\n",
    "cur.execute(\"delete from EMOJIANTICIPATION as ha1 where exists (Select 1 From EMOJIANTICIPATION as ha2 where ha1.emoji = ha2.emoji and ha1.frequency = ha2.frequency and ha2.ctid > ha1.ctid);\")\n",
    "\n",
    "conn.commit()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze emoticons\n",
    "cur.execute(\"update EMOTICONSANTICIPATION as ha1 set frequency = (select count(*) from EMOTICONSANTICIPATION ha2 where ha1.emoticons = ha2.emoticons);\")\n",
    "cur.execute(\"delete from EMOTICONSANTICIPATION as ha1 where exists (Select 1 From EMOTICONSANTICIPATION as ha2 where ha1.emoticons = ha2.emoticons and ha1.frequency = ha2.frequency and ha2.ctid > ha1.ctid);\")\n",
    "\n",
    "conn.commit()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Frequenze parole\n",
    "cur.execute(\"select count(*),parola from PAROLEANTICIPATION group by parola\")\n",
    "result = []\n",
    "for word in cur.fetchall():\n",
    "    result.append(word) \n",
    "\n",
    "cur.execute(\"truncate table PAROLEANTICIPATION;\")\n",
    "#cur.execute(\"ALTER SEQUENCE PAROLEANTICIPATION RESTART WITH 1;\")\n",
    "\n",
    "for couple in result:\n",
    "    cur.execute(\"INSERT INTO PAROLEANTICIPATION (PAROLA,NRC_anticipation,sentisense_anticipation,FREQUENCY) VALUES(%s,%s,%s,%s)\",(couple[1],False,False,couple[0]))\n",
    "\n",
    "conn.commit()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggiornamento presenza parola nelle risorse lessicali\n",
    "cur.execute(\"UPDATE PAROLEANTICIPATION SET NRC_anticipation = TRUE WHERE parola IN (select p.parola from PAROLEANTICIPATION as p LEFT OUTER JOIN RISORSEANTICIPATION as r ON p.parola = r.parola WHERE r.nome = 'NRC_anticipation')\")\n",
    "cur.execute(\"UPDATE PAROLEANTICIPATION SET sentisense_anticipation = TRUE WHERE parola IN (select p.parola from PAROLEANTICIPATION as p LEFT OUTER JOIN RISORSEANTICIPATION as r ON p.parola = r.parola WHERE r.nome = 'sentisense_anticipation')\")\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvataggio in un txt delle parole non presenti in nessuna risorsa lessicale\n",
    "wordsMongo = anticipationCollections.find({\"LexResources\": []}, {\"_id\": 0, \"freq\": 0, \"LexResources\": 0})\n",
    "words = []\n",
    "\n",
    "for word in wordsMongo:\n",
    "    words.append(word['word'])\n",
    "\n",
    "with open('Nuove Parole/anticipation.txt', 'w',encoding=\"utf8\") as f:\n",
    "    for word in words:\n",
    "        f.write(\"%s\\n\" % word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trattamento dataset Disgust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lista di righe, ogni riga una tweet disgust\n",
    "file = open(\"Dataset/dataset_dt_disgust_60k.txt\", 'r',encoding=\"utf8\")\n",
    "disgust = []\n",
    "hashtagDisgust = []\n",
    "emoticonDisgust = []\n",
    "emojiDisgust = []\n",
    "\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "    riga = re.sub(r'URL', '',riga)\n",
    "    riga = re.sub(r'USERNAME', '',riga)\n",
    "\n",
    "    #array di hashtag singoli\n",
    "    hashTag = (re.findall('#[a-zA-Z]+', riga))\n",
    "    \n",
    "    #generiamo l'array di hashtag senza hashtag\n",
    "    for h in hashTag:\n",
    "        hashtagDisgust.append(h.replace(\"#\",\"\"))\n",
    "\n",
    "    #Rimuoviamo glin hashtag dalla riga\n",
    "    riga = riga.replace('#',\"\")\n",
    "\n",
    "    disgust.append(riga)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(disgust)):\n",
    "    #Controllo se l'emoticons c'è e in caso la rimuovo\n",
    "    for singleEmoticon in emoticons:\n",
    "        emoticonsFind = re.findall(re.escape(singleEmoticon),disgust[i])\n",
    "        if(len(emoticonsFind) != 0):\n",
    "            emoticonDisgust.extend(emoticonsFind)\n",
    "            disgust[i] = disgust[i].replace(singleEmoticon,\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(disgust)):\n",
    "    #preleviamo le emoji e le rimuoviamo dal testo\n",
    "    data = regex.findall(r'\\X', disgust[i])\n",
    "    for word in data:\n",
    "        if any(char in emoji.UNICODE_EMOJI['en'] for char in word):\n",
    "            emojiDisgust = [].append(word)\n",
    "            disgust[i] = disgust[i].replace(word, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processamento slang prima di punteggiatura e lowercase\n",
    "with open(\"Slangs/slangs_before.json\") as f:\n",
    "    slangsBefore = json.load(f)\n",
    "    for i in range(len(disgust)):\n",
    "        for word in disgust[i].split():\n",
    "            indexOfWord = disgust[i].find(word)\n",
    "            if slangsBefore.get(word) is None:\n",
    "                if slangsBefore.get(word.lower()) is None:\n",
    "                    if slangsBefore.get(word.upper()) is None:\n",
    "                        continue\n",
    "                    else:\n",
    "                        disgust[i] = disgust[i][:indexOfWord] + slangsBefore.get(word.upper()) + disgust[i][indexOfWord+len(word):]\n",
    "                else:\n",
    "                    disgust[i] = disgust[i][:indexOfWord] + slangsBefore.get(word.lower()) + disgust[i][indexOfWord+len(word):]\n",
    "            else:\n",
    "                disgust[i] = disgust[i][:indexOfWord] + slangsBefore.get(word) + disgust[i][indexOfWord+len(word):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(disgust)):\n",
    "    #Trattamento punteggiatura\n",
    "    line = re.compile(\"[\" + re.escape(\"\".join(puntuaction)) + \"]\")\n",
    "    disgust[i] = re.sub(line, \"\", disgust[i])\n",
    "\n",
    "    #Rendiamo la farse in minuscolo\n",
    "    disgust[i] = disgust[i].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizzazione di tutte le frasi\n",
    "disgustTokenized = []\n",
    "for linea in disgust:\n",
    "    result = word_tokenize(linea)\n",
    "    disgustTokenized.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Esecuzione della POS tagging per ogni riga della matrice\n",
    "for i in range(len(disgustTokenized)):\n",
    "    disgustTokenized[i] = nltk.pos_tag(disgustTokenized[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming\n",
    "#ps = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for line in disgustTokenized:\n",
    "    for i in range(len(line)):\n",
    "        tupla = [lemmatizer.lemmatize(line[i][0]),line[i][1]]\n",
    "        line[i] = tupla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trattamnento Stop word dopo Stemming\n",
    "stop_words = set(stopwords.words('english'))\n",
    "disgustTokStoped = []\n",
    "for line in disgustTokStoped:\n",
    "    filtred_word = []\n",
    "    for row in line:\n",
    "        if  row[0] not in stop_words:\n",
    "            filtred_word.append(row) \n",
    "    disgustTokStoped.append(filtred_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lavorazione risorse lessicali\n",
    "resourceDisgust = []\n",
    "#Lista di righe, ogni riga una tweet anger\n",
    "file = open(\"Risorse lessicali\\Disgust-Hate\\sentisense_disgust.txt.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        if(riga not in resourceDisgust):\n",
    "            resourceDisgust.append(riga)\n",
    "\n",
    "file.close()\n",
    "\n",
    "file = open(\"C:Risorse lessicali\\\\Disgust-Hate\\\\NRC_disgust.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        if(riga not in resourceDisgust):\n",
    "            resourceDisgust.append(line)\n",
    "\n",
    "file.close()\n",
    "\n",
    "file = open(\"Risorse lessicali\\Disgust-Hate\\sentisense_hate.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        if(riga not in resourceDisgust):\n",
    "            resourceDisgust.append(line)\n",
    "\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disgustListWord = []\n",
    "disgustListTok = []\n",
    "for line in disgustTokStoped:\n",
    "    for row in line:\n",
    "        disgustListWord.append(row[0])\n",
    "        disgustListTok.append(row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Slangs vengono modificati con la loro definizione\n",
    "# with open(\"Slangs/slangs.json\", \"r\", encoding='utf-8') as f:\n",
    "#     slangs = json.load(f)\n",
    "#     for row in range(len(disgustTokenized)):\n",
    "#         for column in range(len(disgustTokenized[row])):\n",
    "#             if disgustTokenized[row][column] in slangs:\n",
    "#                 disgustTokenized[row][column] = slangs.get(disgustTokenized[row][column])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trattamento dataset Fear "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lista di righe, ogni riga una tweet fear\n",
    "file = open(\"Dataset/dataset_dt_fear_60k.txt\", 'r',encoding=\"utf8\")\n",
    "fear = []\n",
    "hashtagFear = []\n",
    "emoticonFear = []\n",
    "emojiFear = []\n",
    "\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "    riga = re.sub(r'URL', '',riga)\n",
    "    riga = re.sub(r'USERNAME', '',riga)\n",
    "\n",
    "     #array di hashtag singoli\n",
    "    hashTag = (re.findall('#[a-zA-Z]+', riga))\n",
    "\n",
    "    #generiamo l'array di hashtag senza hashtag\n",
    "    for h in hashTag:\n",
    "        hashtagFear.append(h.replace(\"#\",\"\"))\n",
    "\n",
    "    #Rimuoviamo glin hashtag dalla riga\n",
    "    riga = riga.replace('#',\"\")\n",
    "\n",
    "    fear.append(riga)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(fear)):\n",
    "    #Controllo se l'emoticons c'è e in caso la rimuovo\n",
    "    for singleEmoticon in emoticons:\n",
    "        emoticonsFind = re.findall(re.escape(singleEmoticon),fear[i])\n",
    "        if(len(emoticonsFind) != 0):\n",
    "            emoticonFear.extend(emoticonsFind)\n",
    "            fear[i] = fear[i].replace(singleEmoticon,\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(fear)):\n",
    "    #preleviamo le emoji e le rimuoviamo dal testo\n",
    "    data = regex.findall(r'\\X', fear[i])\n",
    "    for word in data:\n",
    "        if any(char in emoji.UNICODE_EMOJI['en'] for char in word):\n",
    "            emojiFear = [].append(word)\n",
    "            fear[i] = fear[i].replace(word, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processamento slang prima di punteggiatura e lowercase\n",
    "with open(\"Slangs/slangs_before.json\") as f:\n",
    "    slangsBefore = json.load(f)\n",
    "    for i in range(len(fear)):\n",
    "        for word in fear[i].split():\n",
    "            indexOfWord = fear[i].find(word)\n",
    "            if slangsBefore.get(word) is None:\n",
    "                if slangsBefore.get(word.lower()) is None:\n",
    "                    if slangsBefore.get(word.upper()) is None:\n",
    "                        continue\n",
    "                    else:\n",
    "                        fear[i] = fear[i][:indexOfWord] + slangsBefore.get(word.upper()) + fear[i][indexOfWord+len(word):]\n",
    "                else:\n",
    "                    fear[i] = fear[i][:indexOfWord] + slangsBefore.get(word.lower()) + fear[i][indexOfWord+len(word):]\n",
    "            else:\n",
    "                fear[i] = fear[i][:indexOfWord] + slangsBefore.get(word) + fear[i][indexOfWord+len(word):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(fear)):\n",
    "    #Trattamento punteggiatura\n",
    "    line = re.compile(\"[\" + re.escape(\"\".join(puntuaction)) + \"]\")\n",
    "    fear[i] = re.sub(line, \"\", fear[i])\n",
    "\n",
    "    #Rendiamo la farse in minuscolo\n",
    "    fear[i] = fear[i].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizzazione di tutte le frasi\n",
    "fearTokenized = []\n",
    "for linea in fear:\n",
    "    result = word_tokenize(linea)\n",
    "    fearTokenized.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Esecuzione della POS tagging per ogni riga della matrice\n",
    "for i in range(len(fearTokenized)):\n",
    "    fearTokenized[i] = nltk.pos_tag(fearTokenized[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming\n",
    "#ps = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for line in fearTokenized:\n",
    "    for i in range(len(line)):\n",
    "        tupla = [lemmatizer.lemmatize(line[i][0]),line[i][1]]\n",
    "        line[i] = tupla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trattamnento Stop word dopo Stemming\n",
    "stop_words = set(stopwords.words('english'))\n",
    "fearTokStoped = []\n",
    "for line in fearTokStoped:\n",
    "    filtred_word = []\n",
    "    for row in line:\n",
    "        if  row[0] not in stop_words:\n",
    "            filtred_word.append(row) \n",
    "    fearTokStoped.append(filtred_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lavorazione risorse lessicali\n",
    "resourceFear = []\n",
    "#Lista di righe, ogni riga una tweet anger\n",
    "file = open(\"Risorse lessicali\\\\Fear\\\\NRC_fear.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        if(riga not in resourceFear):\n",
    "            resourceFear.append(line)\n",
    "\n",
    "file.close()\n",
    "\n",
    "file = open(\"Risorse lessicali\\Fear\\sentisense_fear.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        if(riga not in resourceFear):\n",
    "            resourceFear.append(line)\n",
    "\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fearListWord = []\n",
    "fearListToken = []\n",
    "for line in fearTokStoped:\n",
    "    for row in line:\n",
    "        fearListWord.append(row[0])\n",
    "        fearListToken.append(row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Slangs vengono modificati con la loro definizione\n",
    "# with open(\"Slangs/slangs.json\", \"r\", encoding='utf-8') as f:\n",
    "#     slangs = json.load(f)\n",
    "#     for row in range(len(fearTokenized)):\n",
    "#         for column in range(len(fearTokenized[row])):\n",
    "#             if fearTokenized[row][column] in slangs:\n",
    "#                 fearTokenized[row][column] = slangs.get(fearTokenized[row][column])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trattamento dataset Joy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lista di righe, ogni riga una tweet joy\n",
    "file = open(\"Dataset/dataset_dt_joy_60k.txt\", 'r',encoding=\"utf8\")\n",
    "joy = []\n",
    "hashtagJoy = []\n",
    "emoticonJoy = []\n",
    "emojiJoy = []\n",
    "\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "    riga = re.sub(r'URL', '',riga)\n",
    "    riga = re.sub(r'USERNAME', '',riga)\n",
    "    \n",
    "    #array di hashtag singoli\n",
    "    hashTag = (re.findall('#[a-zA-Z]+', riga))\n",
    "\n",
    "    #generiamo l'array di hashtag senza hashtag\n",
    "    for h in hashTag:\n",
    "        hashtagJoy.append(h.replace(\"#\",\"\"))\n",
    "\n",
    "    #Rimuoviamo glin hashtag dalla riga\n",
    "    riga = riga.replace('#',\"\")\n",
    "\n",
    "    joy.append(riga)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(joy)):\n",
    "    #Controllo se l'emoticons c'è e in caso la rimuovo\n",
    "    for singleEmoticon in emoticons:\n",
    "        emoticonJoy = re.findall(re.escape(singleEmoticon),joy[i])\n",
    "        if(len(emoticonsFind) != 0):\n",
    "            emoticonFear.extend(emoticonsFind)\n",
    "            joy[i] = joy[i].replace(singleEmoticon,\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(joy)):\n",
    "    #preleviamo le emoji e le rimuoviamo dal testo\n",
    "    data = regex.findall(r'\\X', joy[i])\n",
    "    for word in data:\n",
    "        if any(char in emoji.UNICODE_EMOJI['en'] for char in word):\n",
    "            emojiJoy = [].append(word)\n",
    "            joy[i] = joy[i].replace(word, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processamento slang prima di punteggiatura e lowercase\n",
    "with open(\"Slangs/slangs_before.json\") as f:\n",
    "    slangsBefore = json.load(f)\n",
    "    for i in range(len(joy)):\n",
    "        for word in joy[i].split():\n",
    "            indexOfWord = joy[i].find(word)\n",
    "            if slangsBefore.get(word) is None:\n",
    "                if slangsBefore.get(word.lower()) is None:\n",
    "                    if slangsBefore.get(word.upper()) is None:\n",
    "                        continue\n",
    "                    else:\n",
    "                        joy[i] = joy[i][:indexOfWord] + slangsBefore.get(word.upper()) + joy[i][indexOfWord+len(word):]\n",
    "                else:\n",
    "                    joy[i] = joy[i][:indexOfWord] + slangsBefore.get(word.lower()) + joy[i][indexOfWord+len(word):]\n",
    "            else:\n",
    "                joy[i] = joy[i][:indexOfWord] + slangsBefore.get(word) + joy[i][indexOfWord+len(word):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(joy)):\n",
    "    #Trattamento punteggiatura\n",
    "    line = re.compile(\"[\" + re.escape(\"\".join(puntuaction)) + \"]\")\n",
    "    joy[i] = re.sub(line, \"\", joy[i])\n",
    "\n",
    "    #Rendiamo la farse in minuscolo\n",
    "    joy[i] = joy[i].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizzazione di tutte le frasi\n",
    "joyTokenized = []\n",
    "for linea in joy:\n",
    "    result = word_tokenize(linea)\n",
    "    joyTokenized.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Esecuzione della POS tagging per ogni riga della matrice\n",
    "for i in range(len(joyTokenized)):\n",
    "    joyTokenized[i] = nltk.pos_tag(joyTokenized[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Slangs vengono modificati con la loro definizione\n",
    "# with open(\"Slangs/slangs.json\", \"r\", encoding='utf-8') as f:\n",
    "#     slangs = json.load(f)\n",
    "#     for row in range(len(joyTokenized)):\n",
    "#         for column in range(len(joyTokenized[row])):\n",
    "#             if joyTokenized[row][column] in slangs:\n",
    "#                 joyTokenized[row][column] = slangs.get(joyTokenized[row][column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming\n",
    "#ps = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for line in joyTokenized:\n",
    "    for i in range(len(line)):\n",
    "        tupla = [lemmatizer.lemmatize(line[i][0]),line[i][1]]\n",
    "        line[i] = tupla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trattamnento Stop word dopo Stemming\n",
    "stop_words = set(stopwords.words('english'))\n",
    "joyTokStoped = []\n",
    "for line in joyTokenized:\n",
    "    filtred_word = []\n",
    "    for row in line:\n",
    "        if  row[0] not in stop_words:\n",
    "            filtred_word.append(row) \n",
    "    joyTokStoped.append(filtred_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joyListWord = []\n",
    "joyListTok = []\n",
    "for line in joyTokStoped:\n",
    "    for row in line:\n",
    "        joyListWord.append(row[0])\n",
    "        joyListTok.append(row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lavorazione risorse lessicali\n",
    "resourceJoy = []\n",
    "#Lista di righe, ogni riga una tweet anger\n",
    "file = open(\"Risorse lessicali\\Joy\\EmoSN_joy.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        if(riga not in resourceJoy):\n",
    "            resourceJoy.append(riga)\n",
    "\n",
    "file.close()\n",
    "\n",
    "file = open(\"Risorse lessicali\\\\Joy\\\\NRC_joy.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        if(riga not in resourceJoy):\n",
    "            resourceJoy.append(line)\n",
    "\n",
    "file.close()\n",
    "\n",
    "file = open(\"Risorse lessicali\\Joy\\sentisense_joy.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        if(riga not in resourceJoy):\n",
    "            resourceJoy.append(line)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trattamento dataset Sadness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lista di righe, ogni riga una tweet sadness\n",
    "file = open(\"Dataset/dataset_dt_sadness_60k.txt\", 'r',encoding=\"utf8\")\n",
    "sadness = []\n",
    "hashtagSadness = []\n",
    "emoticonSadness = []\n",
    "emojiSadness = []\n",
    "\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "    riga = re.sub(r'URL', '',riga)\n",
    "    riga = re.sub(r'USERNAME', '',riga)\n",
    "\n",
    "    #array di hashtag singoli\n",
    "    hashTag = (re.findall('#[a-zA-Z]+', riga))\n",
    "\n",
    "    #Rimuoviamo glin hashtag dalla riga\n",
    "    riga = riga.replace('#',\"\")\n",
    "\n",
    "    sadness.append(riga)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sadness)):\n",
    "    #Controllo se l'emoticons c'è e in caso la rimuovo\n",
    "    for singleEmoticon in emoticons:\n",
    "        emoticonJoy = re.findall(re.escape(singleEmoticon),sadness[i])\n",
    "        if(len(emoticonsFind) != 0):\n",
    "            emoticonSadness.extend(emoticonsFind)\n",
    "            sadness[i] = sadness[i].replace(singleEmoticon,\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sadness)):\n",
    "    #preleviamo le emoji e le rimuoviamo dal testo\n",
    "    data = regex.findall(r'\\X', sadness[i])\n",
    "    for word in data:\n",
    "        if any(char in emoji.UNICODE_EMOJI['en'] for char in word):\n",
    "            emojiSadness = [].append(word)\n",
    "            sadness[i] = sadness[i].replace(word, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processamento slang prima di punteggiatura e lowercase\n",
    "with open(\"Slangs/slangs_before.json\") as f:\n",
    "    slangsBefore = json.load(f)\n",
    "    for i in range(len(sadness)):\n",
    "        for word in sadness[i].split():\n",
    "            indexOfWord = sadness[i].find(word)\n",
    "            if slangsBefore.get(word) is None:\n",
    "                if slangsBefore.get(word.lower()) is None:\n",
    "                    if slangsBefore.get(word.upper()) is None:\n",
    "                        continue\n",
    "                    else:\n",
    "                        sadness[i] = sadness[i][:indexOfWord] + slangsBefore.get(word.upper()) + sadness[i][indexOfWord+len(word):]\n",
    "                else:\n",
    "                    sadness[i] = sadness[i][:indexOfWord] + slangsBefore.get(word.lower()) + sadness[i][indexOfWord+len(word):]\n",
    "            else:\n",
    "                sadness[i] = sadness[i][:indexOfWord] + slangsBefore.get(word) + sadness[i][indexOfWord+len(word):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sadness)):\n",
    "    #Trattamento punteggiatura\n",
    "    line = re.compile(\"[\" + re.escape(\"\".join(puntuaction)) + \"]\")\n",
    "    sadness[i] = re.sub(line, \"\", sadness[i])\n",
    "\n",
    "    #Rendiamo la farse in minuscolo\n",
    "    sadness[i] = sadness[i].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizzazione di tutte le frasi\n",
    "sadnessTokenized = []\n",
    "for linea in sadness:\n",
    "    result = word_tokenize(linea)\n",
    "    sadnessTokenized.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Esecuzione della POS tagging per ogni riga della matrice\n",
    "for i in range(len(sadnessTokenized)):\n",
    "    sadnessTokenized[i] = nltk.pos_tag(sadnessTokenized[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Slangs vengono modificati con la loro definizione\n",
    "# with open(\"Slangs/slangs.json\", \"r\", encoding='utf-8') as f:\n",
    "#     slangs = json.load(f)\n",
    "#     for row in range(len(sadnessTokenized)):\n",
    "#         for column in range(len(sadnessTokenized[row])):\n",
    "#             if sadnessTokenized[row][column] in slangs:\n",
    "#                 sadnessTokenized[row][column] = slangs.get(sadnessTokenized[row][column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming\n",
    "#ps = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for line in sadnessTokenized:\n",
    "    for i in range(len(line)):\n",
    "        tupla = [lemmatizer.lemmatize(line[i][0]),line[i][1]]\n",
    "        line[i] = tupla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trattamnento Stop word dopo Stemming\n",
    "stop_words = set(stopwords.words('english'))\n",
    "sadnessTokStoped = []\n",
    "for line in sadnessTokenized:\n",
    "    filtred_word = []\n",
    "    for row in line:\n",
    "        if  row[0] not in stop_words:\n",
    "            filtred_word.append(row) \n",
    "    sadnessTokStoped.append(filtred_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sadnessListWord = []\n",
    "sadnessListTok = []\n",
    "for line in sadnessTokStoped:\n",
    "    for row in line:\n",
    "        sadnessListWord.append(row[0])\n",
    "        sadnessListTok.append(row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lavorazione risorse lessicali\n",
    "resourceSadness = []\n",
    "file = open(\"Risorse lessicali\\\\Sadness\\\\NRC_sadness.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        if(riga not in resourceSadness):\n",
    "            resourceSadness.append(line)\n",
    "\n",
    "file.close()\n",
    "\n",
    "file = open(\"Risorse lessicali\\Sadness\\sentisense_joy.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        if(riga not in resourceSadness):\n",
    "            resourceSadness.append(line)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trattamento dataset Surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lista di righe, ogni riga una tweet surprise\n",
    "file = open(\"Dataset/dataset_dt_surprise_60k.txt\", 'r',encoding=\"utf8\")\n",
    "surprise = []\n",
    "hashtagSurprise = []\n",
    "emoticonSurprise = []\n",
    "emojiSurprise = []\n",
    "\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "    riga = re.sub(r'URL', '',riga)\n",
    "    riga = re.sub(r'USERNAME', '',riga)\n",
    "\n",
    "     #array di hashtag singoli\n",
    "    hashTag = (re.findall('#[a-zA-Z]+', riga))\n",
    "\n",
    "    #generiamo l'array di hashtag senza hashtag\n",
    "    for h in hashTag:\n",
    "        hashtagSurprise.append(h.replace(\"#\",\"\"))\n",
    "\n",
    "    #Rimuoviamo glin hashtag dalla riga\n",
    "    riga = riga.replace('#',\"\")\n",
    "\n",
    "    surprise.append(riga)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(surprise)):\n",
    "    #Controllo se l'emoticons c'è e in caso la rimuovo\n",
    "    for singleEmoticon in emoticons:\n",
    "        emoticonJoy = re.findall(re.escape(singleEmoticon),surprise[i])\n",
    "        if(len(emoticonsFind) != 0):\n",
    "            emoticonSurprise.extend(emoticonsFind)\n",
    "            surprise[i] = surprise[i].replace(singleEmoticon,\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(surprise)):\n",
    "    #preleviamo le emoji e le rimuoviamo dal testo\n",
    "    data = regex.findall(r'\\X', surprise[i])\n",
    "    for word in data:\n",
    "        if any(char in emoji.UNICODE_EMOJI['en'] for char in word):\n",
    "            emojiSurprise = [].append(word)\n",
    "            surprise[i] = surprise[i].replace(word, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processamento slang prima di punteggiatura e lowercase\n",
    "with open(\"Slangs/slangs_before.json\") as f:\n",
    "    slangsBefore = json.load(f)\n",
    "    for i in range(len(surprise)):\n",
    "        for word in surprise[i].split():\n",
    "            indexOfWord = surprise[i].find(word)\n",
    "            if slangsBefore.get(word) is None:\n",
    "                if slangsBefore.get(word.lower()) is None:\n",
    "                    if slangsBefore.get(word.upper()) is None:\n",
    "                        continue\n",
    "                    else:\n",
    "                        surprise[i] = surprise[i][:indexOfWord] + slangsBefore.get(word.upper()) + surprise[i][indexOfWord+len(word):]\n",
    "                else:\n",
    "                    surprise[i] = surprise[i][:indexOfWord] + slangsBefore.get(word.lower()) + surprise[i][indexOfWord+len(word):]\n",
    "            else:\n",
    "                surprise[i] = surprise[i][:indexOfWord] + slangsBefore.get(word) + surprise[i][indexOfWord+len(word):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(surprise)):\n",
    "    #Trattamento punteggiatura\n",
    "    line = re.compile(\"[\" + re.escape(\"\".join(puntuaction)) + \"]\")\n",
    "    surprise[i] = re.sub(line, \"\", surprise[i])\n",
    "\n",
    "    #Rendiamo la farse in minuscolo\n",
    "    surprise[i] = surprise[i].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizzazione di tutte le frasi\n",
    "surpriseTokenized = []\n",
    "for linea in surprise:\n",
    "    result = word_tokenize(linea)\n",
    "    surpriseTokenized.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Esecuzione della POS tagging per ogni riga della matrice\n",
    "for i in range(len(surpriseTokenized)):\n",
    "    surpriseTokenized[i] = nltk.pos_tag(surpriseTokenized[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Slangs vengono modificati con la loro definizione\n",
    "# with open(\"Slangs/slangs.json\", \"r\", encoding='utf-8') as f:\n",
    "#     slangs = json.load(f)\n",
    "#     for row in range(len(surpriseTokenized)):\n",
    "#         for column in range(len(surpriseTokenized[row])):\n",
    "#             if surpriseTokenized[row][column] in slangs:\n",
    "#                 surpriseTokenized[row][column] = slangs.get(surpriseTokenized[row][column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming\n",
    "#ps = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for line in surpriseTokenized:\n",
    "    for i in range(len(line)):\n",
    "        tupla = [lemmatizer.lemmatize(line[i][0]),line[i][1]]\n",
    "        line[i] = tupla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trattamnento Stop word dopo Stemming\n",
    "stop_words = set(stopwords.words('english'))\n",
    "surpriseTokStoped = []\n",
    "for line in surpriseTokenized:\n",
    "    filtred_word = []\n",
    "    for row in line:\n",
    "        if  row[0] not in stop_words:\n",
    "            filtred_word.append(row) \n",
    "    surpriseTokStoped.append(filtred_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surpriseListWord = []\n",
    "surpriseListTok = []\n",
    "for line in surpriseTokStoped:\n",
    "    for row in line:\n",
    "        surpriseListWord.append(row[0])\n",
    "        surpriseListTok.append(row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lavorazione risorse lessicali\n",
    "resourceSurprise = []\n",
    "file = open(\"Risorse lessicali\\\\Surprise\\\\NRC_surprise.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        if(riga not in resourceSurprise):\n",
    "            resourceSurprise.append(line)\n",
    "\n",
    "file.close()\n",
    "\n",
    "file = open(\"Risorse lessicali\\Surprise\\sentisense_surprise.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        if(riga not in resourceSurprise):\n",
    "            resourceSurprise.append(line)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trattamento dataset Trust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lista di righe, ogni riga una tweet trust\n",
    "file = open(\"Dataset/dataset_dt_trust_60k.txt\", 'r',encoding=\"utf8\")\n",
    "trust = []\n",
    "hashtagTrust = []\n",
    "emoticonTrust = []\n",
    "emojiTrust = []\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "    riga = re.sub(r'URL', '',riga)\n",
    "    riga = re.sub(r'USERNAME', '',riga)\n",
    "    \n",
    "    #array di hashtag singoli\n",
    "    hashTag = (re.findall('#[a-zA-Z]+', riga))\n",
    "    \n",
    "    #generiamo l'array di hashtag senza hashtag\n",
    "    for h in hashTag:\n",
    "        hashtagTrust.append(h.replace(\"#\",\"\"))\n",
    "\n",
    "    #Rimuoviamo glin hashtag dalla riga\n",
    "    riga = riga.replace('#',\"\")\n",
    "\n",
    "    trust.append(riga)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(trust)):\n",
    "    #Controllo se l'emoticons c'è e in caso la rimuovo\n",
    "    for singleEmoticon in emoticons:\n",
    "        emoticonJoy = re.findall(re.escape(singleEmoticon),trust[i])\n",
    "        if(len(emoticonsFind) != 0):\n",
    "            emoticonTrust.extend(emoticonsFind)\n",
    "            trust[i] = trust[i].replace(singleEmoticon,\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(trust)):\n",
    "    #preleviamo le emoji e le rimuoviamo dal testo\n",
    "    data = regex.findall(r'\\X', trust[i])\n",
    "    for word in data:\n",
    "        if any(char in emoji.UNICODE_EMOJI['en'] for char in word):\n",
    "            emojiTrust = [].append(word)\n",
    "            trust[i] = trust[i].replace(word, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processamento slang prima di punteggiatura e lowercase\n",
    "with open(\"Slangs/slangs_before.json\") as f:\n",
    "    slangsBefore = json.load(f)\n",
    "    for i in range(len(trust)):\n",
    "        for word in trust[i].split():\n",
    "            indexOfWord = trust[i].find(word)\n",
    "            if slangsBefore.get(word) is None:\n",
    "                if slangsBefore.get(word.lower()) is None:\n",
    "                    if slangsBefore.get(word.upper()) is None:\n",
    "                        continue\n",
    "                    else:\n",
    "                        trust[i] = trust[i][:indexOfWord] + slangsBefore.get(word.upper()) + trust[i][indexOfWord+len(word):]\n",
    "                else:\n",
    "                    trust[i] = trust[i][:indexOfWord] + slangsBefore.get(word.lower()) + trust[i][indexOfWord+len(word):]\n",
    "            else:\n",
    "                trust[i] = trust[i][:indexOfWord] + slangsBefore.get(word) + trust[i][indexOfWord+len(word):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(trust)):\n",
    "    #Trattamento punteggiatura\n",
    "    line = re.compile(\"[\" + re.escape(\"\".join(puntuaction)) + \"]\")\n",
    "    trust[i] = re.sub(line, \"\", trust[i])\n",
    "\n",
    "    #Rendiamo la farse in minuscolo\n",
    "    trust[i] = trust[i].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizzazione di tutte le frasi\n",
    "trustTokenized = []\n",
    "for linea in trust:\n",
    "    result = word_tokenize(linea)\n",
    "    trustTokenized.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Esecuzione della POS tagging per ogni riga della matrice\n",
    "for i in range(len(trustTokenized)):\n",
    "    trustTokenized[i] = nltk.pos_tag(trustTokenized[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming\n",
    "#ps = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for line in trustTokenized:\n",
    "    for i in range(len(line)):\n",
    "        tupla = [lemmatizer.lemmatize(line[i][0]),line[i][1]]\n",
    "        line[i] = tupla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trattamnento Stop word dopo Stemming\n",
    "stop_words = set(stopwords.words('english'))\n",
    "trustTokStoped = []\n",
    "for line in trustTokenized:\n",
    "    filtred_word = []\n",
    "    for row in line:\n",
    "        if  row[0] not in stop_words:\n",
    "            filtred_word.append(row) \n",
    "    trustTokStoped.append(filtred_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trustListWord = []\n",
    "trustListTok = []\n",
    "for line in trustTokStoped:\n",
    "    for row in line:\n",
    "        trustListWord.append(row[0])\n",
    "        trustListTok.append(row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lavorazione risorse lessicali\n",
    "resourceTrust = []\n",
    "file = open(\"Risorse lessicali\\\\Trust\\\\NRC_trust.txt\",'r',encoding=\"utf8\")\n",
    "\n",
    "for line in file:\n",
    "    riga = line.strip()\n",
    "\n",
    "    if not \"_\" in riga:\n",
    "        if(riga not in resourceTrust):\n",
    "            resourceTrust.append(line)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Slangs vengono modificati con la loro definizione\n",
    "# with open(\"Slangs/slangs.json\", \"r\", encoding='utf-8') as f:\n",
    "#     slangs = json.load(f)\n",
    "#     for row in range(len(trustTokenized)):\n",
    "#         for column in range(len(trustTokenized[row])):\n",
    "#             if trustTokenized[row][column] in slangs:\n",
    "#                 trustTokenized[row][column] = slangs.get(trustTokenized[row][column])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "727cd1b8d9e0a8fe67ccb60934071f3fe824c12f3f18648f9575d2adb7fe198a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
